{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CifYJoNnBxKt"
      },
      "outputs": [],
      "source": [
        "from minbpe import RegexTokenizer\n",
        "\n",
        "tokenizer = RegexTokenizer()\n",
        "tokenizer_path = \"../output/tokenizer/darija_tokenizer.model\"\n",
        "tokenizer.load(model_file=tokenizer_path)\n",
        "\n",
        "\n",
        "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
        "    vocab = tokenizer.vocab\n",
        "    special_tokens = tokenizer.special_tokens\n",
        "\n",
        "    return len(vocab) + len(special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjnbL6csCk2y",
        "outputId": "2e4fa17b-a144-4901-d639-394cd5b17d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: b'\\x00',\n",
              " 1: b'\\x01',\n",
              " 2: b'\\x02',\n",
              " 3: b'\\x03',\n",
              " 4: b'\\x04',\n",
              " 5: b'\\x05',\n",
              " 6: b'\\x06',\n",
              " 7: b'\\x07',\n",
              " 8: b'\\x08',\n",
              " 9: b'\\t',\n",
              " 10: b'\\n',\n",
              " 11: b'\\x0b',\n",
              " 12: b'\\x0c',\n",
              " 13: b'\\r',\n",
              " 14: b'\\x0e',\n",
              " 15: b'\\x0f',\n",
              " 16: b'\\x10',\n",
              " 17: b'\\x11',\n",
              " 18: b'\\x12',\n",
              " 19: b'\\x13',\n",
              " 20: b'\\x14',\n",
              " 21: b'\\x15',\n",
              " 22: b'\\x16',\n",
              " 23: b'\\x17',\n",
              " 24: b'\\x18',\n",
              " 25: b'\\x19',\n",
              " 26: b'\\x1a',\n",
              " 27: b'\\x1b',\n",
              " 28: b'\\x1c',\n",
              " 29: b'\\x1d',\n",
              " 30: b'\\x1e',\n",
              " 31: b'\\x1f',\n",
              " 32: b' ',\n",
              " 33: b'!',\n",
              " 34: b'\"',\n",
              " 35: b'#',\n",
              " 36: b'$',\n",
              " 37: b'%',\n",
              " 38: b'&',\n",
              " 39: b\"'\",\n",
              " 40: b'(',\n",
              " 41: b')',\n",
              " 42: b'*',\n",
              " 43: b'+',\n",
              " 44: b',',\n",
              " 45: b'-',\n",
              " 46: b'.',\n",
              " 47: b'/',\n",
              " 48: b'0',\n",
              " 49: b'1',\n",
              " 50: b'2',\n",
              " 51: b'3',\n",
              " 52: b'4',\n",
              " 53: b'5',\n",
              " 54: b'6',\n",
              " 55: b'7',\n",
              " 56: b'8',\n",
              " 57: b'9',\n",
              " 58: b':',\n",
              " 59: b';',\n",
              " 60: b'<',\n",
              " 61: b'=',\n",
              " 62: b'>',\n",
              " 63: b'?',\n",
              " 64: b'@',\n",
              " 65: b'A',\n",
              " 66: b'B',\n",
              " 67: b'C',\n",
              " 68: b'D',\n",
              " 69: b'E',\n",
              " 70: b'F',\n",
              " 71: b'G',\n",
              " 72: b'H',\n",
              " 73: b'I',\n",
              " 74: b'J',\n",
              " 75: b'K',\n",
              " 76: b'L',\n",
              " 77: b'M',\n",
              " 78: b'N',\n",
              " 79: b'O',\n",
              " 80: b'P',\n",
              " 81: b'Q',\n",
              " 82: b'R',\n",
              " 83: b'S',\n",
              " 84: b'T',\n",
              " 85: b'U',\n",
              " 86: b'V',\n",
              " 87: b'W',\n",
              " 88: b'X',\n",
              " 89: b'Y',\n",
              " 90: b'Z',\n",
              " 91: b'[',\n",
              " 92: b'\\\\',\n",
              " 93: b']',\n",
              " 94: b'^',\n",
              " 95: b'_',\n",
              " 96: b'`',\n",
              " 97: b'a',\n",
              " 98: b'b',\n",
              " 99: b'c',\n",
              " 100: b'd',\n",
              " 101: b'e',\n",
              " 102: b'f',\n",
              " 103: b'g',\n",
              " 104: b'h',\n",
              " 105: b'i',\n",
              " 106: b'j',\n",
              " 107: b'k',\n",
              " 108: b'l',\n",
              " 109: b'm',\n",
              " 110: b'n',\n",
              " 111: b'o',\n",
              " 112: b'p',\n",
              " 113: b'q',\n",
              " 114: b'r',\n",
              " 115: b's',\n",
              " 116: b't',\n",
              " 117: b'u',\n",
              " 118: b'v',\n",
              " 119: b'w',\n",
              " 120: b'x',\n",
              " 121: b'y',\n",
              " 122: b'z',\n",
              " 123: b'{',\n",
              " 124: b'|',\n",
              " 125: b'}',\n",
              " 126: b'~',\n",
              " 127: b'\\x7f',\n",
              " 128: b'\\x80',\n",
              " 129: b'\\x81',\n",
              " 130: b'\\x82',\n",
              " 131: b'\\x83',\n",
              " 132: b'\\x84',\n",
              " 133: b'\\x85',\n",
              " 134: b'\\x86',\n",
              " 135: b'\\x87',\n",
              " 136: b'\\x88',\n",
              " 137: b'\\x89',\n",
              " 138: b'\\x8a',\n",
              " 139: b'\\x8b',\n",
              " 140: b'\\x8c',\n",
              " 141: b'\\x8d',\n",
              " 142: b'\\x8e',\n",
              " 143: b'\\x8f',\n",
              " 144: b'\\x90',\n",
              " 145: b'\\x91',\n",
              " 146: b'\\x92',\n",
              " 147: b'\\x93',\n",
              " 148: b'\\x94',\n",
              " 149: b'\\x95',\n",
              " 150: b'\\x96',\n",
              " 151: b'\\x97',\n",
              " 152: b'\\x98',\n",
              " 153: b'\\x99',\n",
              " 154: b'\\x9a',\n",
              " 155: b'\\x9b',\n",
              " 156: b'\\x9c',\n",
              " 157: b'\\x9d',\n",
              " 158: b'\\x9e',\n",
              " 159: b'\\x9f',\n",
              " 160: b'\\xa0',\n",
              " 161: b'\\xa1',\n",
              " 162: b'\\xa2',\n",
              " 163: b'\\xa3',\n",
              " 164: b'\\xa4',\n",
              " 165: b'\\xa5',\n",
              " 166: b'\\xa6',\n",
              " 167: b'\\xa7',\n",
              " 168: b'\\xa8',\n",
              " 169: b'\\xa9',\n",
              " 170: b'\\xaa',\n",
              " 171: b'\\xab',\n",
              " 172: b'\\xac',\n",
              " 173: b'\\xad',\n",
              " 174: b'\\xae',\n",
              " 175: b'\\xaf',\n",
              " 176: b'\\xb0',\n",
              " 177: b'\\xb1',\n",
              " 178: b'\\xb2',\n",
              " 179: b'\\xb3',\n",
              " 180: b'\\xb4',\n",
              " 181: b'\\xb5',\n",
              " 182: b'\\xb6',\n",
              " 183: b'\\xb7',\n",
              " 184: b'\\xb8',\n",
              " 185: b'\\xb9',\n",
              " 186: b'\\xba',\n",
              " 187: b'\\xbb',\n",
              " 188: b'\\xbc',\n",
              " 189: b'\\xbd',\n",
              " 190: b'\\xbe',\n",
              " 191: b'\\xbf',\n",
              " 192: b'\\xc0',\n",
              " 193: b'\\xc1',\n",
              " 194: b'\\xc2',\n",
              " 195: b'\\xc3',\n",
              " 196: b'\\xc4',\n",
              " 197: b'\\xc5',\n",
              " 198: b'\\xc6',\n",
              " 199: b'\\xc7',\n",
              " 200: b'\\xc8',\n",
              " 201: b'\\xc9',\n",
              " 202: b'\\xca',\n",
              " 203: b'\\xcb',\n",
              " 204: b'\\xcc',\n",
              " 205: b'\\xcd',\n",
              " 206: b'\\xce',\n",
              " 207: b'\\xcf',\n",
              " 208: b'\\xd0',\n",
              " 209: b'\\xd1',\n",
              " 210: b'\\xd2',\n",
              " 211: b'\\xd3',\n",
              " 212: b'\\xd4',\n",
              " 213: b'\\xd5',\n",
              " 214: b'\\xd6',\n",
              " 215: b'\\xd7',\n",
              " 216: b'\\xd8',\n",
              " 217: b'\\xd9',\n",
              " 218: b'\\xda',\n",
              " 219: b'\\xdb',\n",
              " 220: b'\\xdc',\n",
              " 221: b'\\xdd',\n",
              " 222: b'\\xde',\n",
              " 223: b'\\xdf',\n",
              " 224: b'\\xe0',\n",
              " 225: b'\\xe1',\n",
              " 226: b'\\xe2',\n",
              " 227: b'\\xe3',\n",
              " 228: b'\\xe4',\n",
              " 229: b'\\xe5',\n",
              " 230: b'\\xe6',\n",
              " 231: b'\\xe7',\n",
              " 232: b'\\xe8',\n",
              " 233: b'\\xe9',\n",
              " 234: b'\\xea',\n",
              " 235: b'\\xeb',\n",
              " 236: b'\\xec',\n",
              " 237: b'\\xed',\n",
              " 238: b'\\xee',\n",
              " 239: b'\\xef',\n",
              " 240: b'\\xf0',\n",
              " 241: b'\\xf1',\n",
              " 242: b'\\xf2',\n",
              " 243: b'\\xf3',\n",
              " 244: b'\\xf4',\n",
              " 245: b'\\xf5',\n",
              " 246: b'\\xf6',\n",
              " 247: b'\\xf7',\n",
              " 248: b'\\xf8',\n",
              " 249: b'\\xf9',\n",
              " 250: b'\\xfa',\n",
              " 251: b'\\xfb',\n",
              " 252: b'\\xfc',\n",
              " 253: b'\\xfd',\n",
              " 254: b'\\xfe',\n",
              " 255: b'\\xff',\n",
              " 256: b'de',\n",
              " 257: b' e',\n",
              " 258: b' de',\n",
              " 259: b' l',\n",
              " 260: b' p',\n",
              " 261: b'os',\n",
              " 262: b' a',\n",
              " 263: b'ue',\n",
              " 264: b' c',\n",
              " 265: b'en',\n",
              " 266: b'ar',\n",
              " 267: b' s',\n",
              " 268: b'er',\n",
              " 269: b'es',\n",
              " 270: b'as',\n",
              " 271: b'ci',\n",
              " 272: b'on',\n",
              " 273: b'or',\n",
              " 274: b' m',\n",
              " 275: b'st',\n",
              " 276: b' q',\n",
              " 277: b' la',\n",
              " 278: b'an',\n",
              " 279: b' que',\n",
              " 280: b' t',\n",
              " 281: b'\\xc3\\xb3',\n",
              " 282: b're',\n",
              " 283: b'ad',\n",
              " 284: b'un',\n",
              " 285: b' en',\n",
              " 286: b'in',\n",
              " 287: b'ee',\n",
              " 288: b'\\xc3\\xb3n',\n",
              " 289: b'.\\n',\n",
              " 290: b'al',\n",
              " 291: b' el',\n",
              " 292: b' y',\n",
              " 293: b' n',\n",
              " 294: b'ro',\n",
              " 295: b'ent',\n",
              " 296: b' h',\n",
              " 297: b'am',\n",
              " 298: b'\\xc3\\xad',\n",
              " 299: b'ti',\n",
              " 300: b'do',\n",
              " 301: b' un',\n",
              " 302: b'om',\n",
              " 303: b' con',\n",
              " 304: b' E',\n",
              " 305: b'di',\n",
              " 306: b'aci',\n",
              " 307: b' est',\n",
              " 308: b' se',\n",
              " 309: b' es',\n",
              " 310: b'\\xc3\\xa1',\n",
              " 311: b' los',\n",
              " 312: b' v',\n",
              " 313: b'ra',\n",
              " 314: b' P',\n",
              " 315: b' S',\n",
              " 316: b'te',\n",
              " 317: b' f',\n",
              " 318: b'res',\n",
              " 319: b'eeee',\n",
              " 320: b' in',\n",
              " 321: b'to',\n",
              " 322: b' C',\n",
              " 323: b' re',\n",
              " 324: b'em',\n",
              " 325: b'aj',\n",
              " 326: b'is',\n",
              " 327: b' par',\n",
              " 328: b'ic',\n",
              " 329: b' no',\n",
              " 330: b'ec',\n",
              " 331: b' las',\n",
              " 332: b'aci\\xc3\\xb3n',\n",
              " 333: b' ha',\n",
              " 334: b' o',\n",
              " 335: b'ado',\n",
              " 336: b'bi',\n",
              " 337: b'ol',\n",
              " 338: b' com',\n",
              " 339: b'im',\n",
              " 340: b'ente',\n",
              " 341: b'\\xc3\\xa9',\n",
              " 342: b'id',\n",
              " 343: b' por',\n",
              " 344: b' me',\n",
              " 345: b' del',\n",
              " 346: b'\\xc3\\xb1',\n",
              " 347: b' al',\n",
              " 348: b'i\\xc3\\xb3n',\n",
              " 349: b'ta',\n",
              " 350: b' pro',\n",
              " 351: b' g',\n",
              " 352: b' di',\n",
              " 353: b' A',\n",
              " 354: b' una',\n",
              " 355: b' su',\n",
              " 356: b'bl',\n",
              " 357: b'pe',\n",
              " 358: b' lo',\n",
              " 359: b' si',\n",
              " 360: b' para',\n",
              " 361: b'ones',\n",
              " 362: b'tu',\n",
              " 363: b'go',\n",
              " 364: b'uest',\n",
              " 365: b'da',\n",
              " 366: b'ajaj',\n",
              " 367: b' mi',\n",
              " 368: b'ui',\n",
              " 369: b'lo',\n",
              " 370: b' L',\n",
              " 371: b'ero',\n",
              " 372: b'eeeeeeee',\n",
              " 373: b'\\xc3\\xada',\n",
              " 374: b'uro',\n",
              " 375: b'ter',\n",
              " 376: b' ac',\n",
              " 377: b'ri',\n",
              " 378: b'ch',\n",
              " 379: b'ci\\xc3\\xb3n',\n",
              " 380: b' d',\n",
              " 381: b' M',\n",
              " 382: b'ari',\n",
              " 383: b' b',\n",
              " 384: b' N',\n",
              " 385: b'se',\n",
              " 386: b'ento',\n",
              " 387: b'orm',\n",
              " 388: b'tiv',\n",
              " 389: b'ide',\n",
              " 390: b'bre',\n",
              " 391: b'ur',\n",
              " 392: b'idad',\n",
              " 393: b'ul',\n",
              " 394: b'oy',\n",
              " 395: b'\\xc3\\xa1s',\n",
              " 396: b' x',\n",
              " 397: b'le',\n",
              " 398: b' to',\n",
              " 399: b'ca',\n",
              " 400: b' Com',\n",
              " 401: b' xD',\n",
              " 402: b'cia',\n",
              " 403: b'ist',\n",
              " 404: b'ados',\n",
              " 405: b'ien',\n",
              " 406: b' Y',\n",
              " 407: b' ex',\n",
              " 408: b' so',\n",
              " 409: b'ab',\n",
              " 410: b'\\xc3\\xba',\n",
              " 411: b' Est',\n",
              " 412: b' D',\n",
              " 413: b' T',\n",
              " 414: b'ales',\n",
              " 415: b' res',\n",
              " 416: b' Se',\n",
              " 417: b'tr',\n",
              " 418: b' U',\n",
              " 419: b'ce',\n",
              " 420: b' des',\n",
              " 421: b'ando',\n",
              " 422: b'que',\n",
              " 423: b'ada',\n",
              " 424: b' esta',\n",
              " 425: b'urope',\n",
              " 426: b'tos',\n",
              " 427: b'amos',\n",
              " 428: b' cu',\n",
              " 429: b' deb',\n",
              " 430: b' ten',\n",
              " 431: b'la',\n",
              " 432: b'li',\n",
              " 433: b' pr',\n",
              " 434: b' como',\n",
              " 435: b' este',\n",
              " 436: b'amente',\n",
              " 437: b' Comis',\n",
              " 438: b'mo',\n",
              " 439: b'ambi',\n",
              " 440: b' pa',\n",
              " 441: b' ap',\n",
              " 442: b' mu',\n",
              " 443: b' m\\xc3\\xa1s',\n",
              " 444: b'emos',\n",
              " 445: b' per',\n",
              " 446: b'co',\n",
              " 447: b'ir',\n",
              " 448: b' pue',\n",
              " 449: b'us',\n",
              " 450: b' ser',\n",
              " 451: b' No',\n",
              " 452: b' po',\n",
              " 453: b'tar',\n",
              " 454: b'ten',\n",
              " 455: b' j',\n",
              " 456: b'por',\n",
              " 457: b' Un',\n",
              " 458: b'\\xc3\\xa9n',\n",
              " 459: b'iden',\n",
              " 460: b' im',\n",
              " 461: b'uen',\n",
              " 462: b' sobre',\n",
              " 463: b'ran',\n",
              " 464: b'pa',\n",
              " 465: b' te',\n",
              " 466: b'lam',\n",
              " 467: b'ante',\n",
              " 468: b'form',\n",
              " 469: b'vi',\n",
              " 470: b' an',\n",
              " 471: b'mi',\n",
              " 472: b' Si',\n",
              " 473: b'uer',\n",
              " 474: b'eeeeeeeeeeeeeeee',\n",
              " 475: b'ora',\n",
              " 476: b' Comisi\\xc3\\xb3n',\n",
              " 477: b'tes',\n",
              " 478: b' le',\n",
              " 479: b' pre',\n",
              " 480: b' tra',\n",
              " 481: b' Q',\n",
              " 482: b' Con',\n",
              " 483: b' as',\n",
              " 484: b' Por',\n",
              " 485: b'um',\n",
              " 486: b' O',\n",
              " 487: b' En',\n",
              " 488: b'puest',\n",
              " 489: b'residen',\n",
              " 490: b'dos',\n",
              " 491: b' ma',\n",
              " 492: b' Es',\n",
              " 493: b'po',\n",
              " 494: b'per',\n",
              " 495: b'ble',\n",
              " 496: b'gun',\n",
              " 497: b'das',\n",
              " 498: b'des',\n",
              " 499: b' La',\n",
              " 500: b'ambi\\xc3\\xa9n',\n",
              " 501: b'ia',\n",
              " 502: b'cer',\n",
              " 503: b'der',\n",
              " 504: b'iz',\n",
              " 505: b'ica',\n",
              " 506: b'aciones',\n",
              " 507: b' cont',\n",
              " 508: b'\\xc3\\xa1n',\n",
              " 509: b'dad',\n",
              " 510: b' Presiden',\n",
              " 511: b' Europe',\n",
              " 512: b'it',\n",
              " 513: b'ust',\n",
              " 514: b'\\xc3\\xb1or',\n",
              " 515: b'ig',\n",
              " 516: b' El',\n",
              " 517: b'ru',\n",
              " 518: b' em',\n",
              " 519: b'\\xc3\\xadti',\n",
              " 520: b'ores',\n",
              " 521: b'gr',\n",
              " 522: b'pon',\n",
              " 523: b'jo',\n",
              " 524: b' B',\n",
              " 525: b' Par',\n",
              " 526: b' ya',\n",
              " 527: b' cons',\n",
              " 528: b' tambi\\xc3\\xa9n',\n",
              " 529: b' pol',\n",
              " 530: b' ver',\n",
              " 531: b'mos',\n",
              " 532: b'ici',\n",
              " 533: b' H',\n",
              " 534: b' inform',\n",
              " 535: b'br',\n",
              " 536: b'ier',\n",
              " 537: b'\\xc3\\xads',\n",
              " 538: b' man',\n",
              " 539: b' qui',\n",
              " 540: b'\\xc3\\xban',\n",
              " 541: b'con',\n",
              " 542: b' cre',\n",
              " 543: b'gar',\n",
              " 544: b' medi',\n",
              " 545: b'ida',\n",
              " 546: b' sin',\n",
              " 547: b'ga',\n",
              " 548: b' hay',\n",
              " 549: b' inter',\n",
              " 550: b'lar',\n",
              " 551: b'lamento',\n",
              " 552: b' pol\\xc3\\xadti',\n",
              " 553: b'00',\n",
              " 554: b'port',\n",
              " 555: b'cu',\n",
              " 556: b' Uni\\xc3\\xb3n',\n",
              " 557: b' comp',\n",
              " 558: b' pe',\n",
              " 559: b' \\xc2',\n",
              " 560: b'eces',\n",
              " 561: b' R',\n",
              " 562: b' tien',\n",
              " 563: b'mente',\n",
              " 564: b' pres',\n",
              " 565: b' pos',\n",
              " 566: b' pero',\n",
              " 567: b' G',\n",
              " 568: b' Me',\n",
              " 569: b'ac',\n",
              " 570: b'ario',\n",
              " 571: b'io',\n",
              " 572: b' mo',\n",
              " 573: b'ces',\n",
              " 574: b' co',\n",
              " 575: b' pa\\xc3\\xads',\n",
              " 576: b'ech',\n",
              " 577: b' dis',\n",
              " 578: b'den',\n",
              " 579: b' sol',\n",
              " 580: b'iones',\n",
              " 581: b' deci',\n",
              " 582: b' Se\\xc3\\xb1or',\n",
              " 583: b' nuest',\n",
              " 584: b' tan',\n",
              " 585: b' Presidente',\n",
              " 586: b'fic',\n",
              " 587: b'ras',\n",
              " 588: b'ajajajaj',\n",
              " 589: b' nos',\n",
              " 590: b'du',\n",
              " 591: b' V',\n",
              " 592: b' ent',\n",
              " 593: b'aa',\n",
              " 594: b' mis',\n",
              " 595: b'ver',\n",
              " 596: b'cial',\n",
              " 597: b' ob',\n",
              " 598: b'ido',\n",
              " 599: b' ve',\n",
              " 600: b'tas',\n",
              " 601: b'cho',\n",
              " 602: b'tiva',\n",
              " 603: b'pec',\n",
              " 604: b' ca',\n",
              " 605: b' han',\n",
              " 606: b'encia',\n",
              " 607: b' pas',\n",
              " 608: b'tor',\n",
              " 609: b'me',\n",
              " 610: b' seg',\n",
              " 611: b' fin',\n",
              " 612: b' Parlamento',\n",
              " 613: b'el',\n",
              " 614: b'ciones',\n",
              " 615: b' r',\n",
              " 616: b' Europea',\n",
              " 617: b'vo',\n",
              " 618: b' son',\n",
              " 619: b'ens',\n",
              " 620: b'pres',\n",
              " 621: b' I',\n",
              " 622: b'cion',\n",
              " 623: b'hora',\n",
              " 624: b'ajj',\n",
              " 625: b'qui',\n",
              " 626: b'fer',\n",
              " 627: b' hacer',\n",
              " 628: b'adas',\n",
              " 629: b' cas',\n",
              " 630: b'rol',\n",
              " 631: b'sejo',\n",
              " 632: b' ti',\n",
              " 633: b' Estados',\n",
              " 634: b' sus',\n",
              " 635: b' Consejo',\n",
              " 636: b'era',\n",
              " 637: b' neces',\n",
              " 638: b' posi',\n",
              " 639: b'acion',\n",
              " 640: b' am',\n",
              " 641: b'baj',\n",
              " 642: b' J',\n",
              " 643: b' lu',\n",
              " 644: b' import',\n",
              " 645: b'ber',\n",
              " 646: b' ni',\n",
              " 647: b'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee',\n",
              " 648: b'dr',\n",
              " 649: b'rec',\n",
              " 650: b'endo',\n",
              " 651: b' yo',\n",
              " 652: b' eso',\n",
              " 653: b'tado',\n",
              " 654: b' a\\xc3\\xb1',\n",
              " 655: b' (',\n",
              " 656: b'tro',\n",
              " 657: b' todo',\n",
              " 658: b' Pero',\n",
              " 659: b' Que',\n",
              " 660: b' miem',\n",
              " 661: b' propuest',\n",
              " 662: b'zar',\n",
              " 663: b' pa\\xc3\\xadses',\n",
              " 664: b'ico',\n",
              " 665: b'ven',\n",
              " 666: b' :',\n",
              " 667: b' pon',\n",
              " 668: b'bil',\n",
              " 669: b'u\\xc3\\xa9',\n",
              " 670: b' or',\n",
              " 671: b' -',\n",
              " 672: b'cl',\n",
              " 673: b' trabaj',\n",
              " 674: b' nue',\n",
              " 675: b' europe',\n",
              " 676: b'tic',\n",
              " 677: b'anos',\n",
              " 678: b'je',\n",
              " 679: b'almente',\n",
              " 680: b'ord',\n",
              " 681: b' est\\xc3\\xa1',\n",
              " 682: b' informe',\n",
              " 683: b' pl',\n",
              " 684: b'cip',\n",
              " 685: b' form',\n",
              " 686: b' lle',\n",
              " 687: b'lic',\n",
              " 688: b' muy',\n",
              " 689: b'za',\n",
              " 690: b'aria',\n",
              " 691: b' ar',\n",
              " 692: b'no',\n",
              " 693: b'ajjajajajaj',\n",
              " 694: b' entre',\n",
              " 695: b' De',\n",
              " 696: b' F',\n",
              " 697: b' vi',\n",
              " 698: b' todos',\n",
              " 699: b' ci',\n",
              " 700: b'bros',\n",
              " 701: b' pol\\xc3\\xadtica',\n",
              " 702: b' sal',\n",
              " 703: b' Sr',\n",
              " 704: b' bi',\n",
              " 705: b' pers',\n",
              " 706: b'entos',\n",
              " 707: b'\\xc3\\xadas',\n",
              " 708: b' prim',\n",
              " 709: b' reg',\n",
              " 710: b'entes',\n",
              " 711: b' As',\n",
              " 712: b' miembros',\n",
              " 713: b'ema',\n",
              " 714: b'eg',\n",
              " 715: b'tivo',\n",
              " 716: b'itu',\n",
              " 717: b'min',\n",
              " 718: b'tros',\n",
              " 719: b'unto',\n",
              " 720: b'ana',\n",
              " 721: b' Euro',\n",
              " 722: b' rec',\n",
              " 723: b' mej',\n",
              " 724: b' car',\n",
              " 725: b' cos',\n",
              " 726: b' tiene',\n",
              " 727: b'tra',\n",
              " 728: b'chos',\n",
              " 729: b' ter',\n",
              " 730: b' ra',\n",
              " 731: b' cuando',\n",
              " 732: b' fun',\n",
              " 733: b'ma',\n",
              " 734: b'aliz',\n",
              " 735: b'fec',\n",
              " 736: b'bar',\n",
              " 737: b' dere',\n",
              " 738: b'200',\n",
              " 739: b'echo',\n",
              " 740: b' tu',\n",
              " 741: b' tran',\n",
              " 742: b' porque',\n",
              " 743: b' ese',\n",
              " 744: b' li',\n",
              " 745: b' voy',\n",
              " 746: b' Europa',\n",
              " 747: b' mar',\n",
              " 748: b'il',\n",
              " 749: b' cuest',\n",
              " 750: b' tanto',\n",
              " 751: b' parte',\n",
              " 752: b'stitu',\n",
              " 753: b'uerdo',\n",
              " 754: b'uir',\n",
              " 755: b' esto',\n",
              " 756: b' puede',\n",
              " 757: b' ahora',\n",
              " 758: b'gan',\n",
              " 759: b' contra',\n",
              " 760: b'duc',\n",
              " 761: b'par',\n",
              " 762: b'emp',\n",
              " 763: b' X',\n",
              " 764: b'fici',\n",
              " 765: b'bilidad',\n",
              " 766: b'qu',\n",
              " 767: b' comun',\n",
              " 768: b'ami',\n",
              " 769: b' men',\n",
              " 770: b'arrol',\n",
              " 771: b' sab',\n",
              " 772: b' ab',\n",
              " 773: b'uci\\xc3\\xb3n',\n",
              " 774: b' \\xc2\\xbf',\n",
              " 775: b' may',\n",
              " 776: b'idades',\n",
              " 777: b' bien',\n",
              " 778: b' lugar',\n",
              " 779: b' person',\n",
              " 780: b' prin',\n",
              " 781: b'va',\n",
              " 782: b' tengo',\n",
              " 783: b' trans',\n",
              " 784: b' gran',\n",
              " 785: b'tir',\n",
              " 786: b' ej',\n",
              " 787: b' estoy',\n",
              " 788: b' cor',\n",
              " 789: b' algun',\n",
              " 790: b'los',\n",
              " 791: b' ag',\n",
              " 792: b'arios',\n",
              " 793: b' deba',\n",
              " 794: b'aba',\n",
              " 795: b' hecho',\n",
              " 796: b'cas',\n",
              " 797: b'car',\n",
              " 798: b' desarrol',\n",
              " 799: b' pu',\n",
              " 800: b'raci',\n",
              " 801: b'cias',\n",
              " 802: b' Ya',\n",
              " 803: b' mejor',\n",
              " 804: b'tivos',\n",
              " 805: b'rib',\n",
              " 806: b' mun',\n",
              " 807: b' vo',\n",
              " 808: b' ref',\n",
              " 809: b'cha',\n",
              " 810: b' Lo',\n",
              " 811: b' decir',\n",
              " 812: b'tura',\n",
              " 813: b'teri',\n",
              " 814: b'pli',\n",
              " 815: b'cio',\n",
              " 816: b'av',\n",
              " 817: b' esa',\n",
              " 818: b' Yo',\n",
              " 819: b' mer',\n",
              " 820: b' vez',\n",
              " 821: b' sig',\n",
              " 822: b' w',\n",
              " 823: b'ut',\n",
              " 824: b' leg',\n",
              " 825: b'miento',\n",
              " 826: b'mien',\n",
              " 827: b'tal',\n",
              " 828: b'pos',\n",
              " 829: b'sa',\n",
              " 830: b'las',\n",
              " 831: b' debe',\n",
              " 832: b'ros',\n",
              " 833: b've',\n",
              " 834: b' obje',\n",
              " 835: b' deber',\n",
              " 836: b' acuerdo',\n",
              " 837: b' princip',\n",
              " 838: b'ba',\n",
              " 839: b' col',\n",
              " 840: b'uego',\n",
              " 841: b'emas',\n",
              " 842: b'?\\n',\n",
              " 843: b' direc',\n",
              " 844: b' propuesta',\n",
              " 845: b' enmien',\n",
              " 846: b' probl',\n",
              " 847: b' ay',\n",
              " 848: b' va',\n",
              " 849: b' mismo',\n",
              " 850: b' pes',\n",
              " 851: b'bles',\n",
              " 852: b'pecto',\n",
              " 853: b' incl',\n",
              " 854: b'can',\n",
              " 855: b'if',\n",
              " 856: b' XD',\n",
              " 857: b' actu',\n",
              " 858: b' buen',\n",
              " 859: b' .',\n",
              " 860: b' creo',\n",
              " 861: b'zo',\n",
              " 862: b' solo',\n",
              " 863: b' cuen',\n",
              " 864: b'les',\n",
              " 865: b'ep',\n",
              " 866: b' importante',\n",
              " 867: b' segur',\n",
              " 868: b' In',\n",
              " 869: b'ja',\n",
              " 870: b'antes',\n",
              " 871: b' econ',\n",
              " 872: b' disc',\n",
              " 873: b'ular',\n",
              " 874: b' dem',\n",
              " 875: b' \\xc3\\xba',\n",
              " 876: b' do',\n",
              " 877: b' Europeo',\n",
              " 878: b'\\xc3\\xb1o',\n",
              " 879: b'ie',\n",
              " 880: b'empo',\n",
              " 881: b'\\xc3\\xa1m',\n",
              " 882: b'emente',\n",
              " 883: b' rel',\n",
              " 884: b'\\xc3\\xb3m',\n",
              " 885: b'ece',\n",
              " 886: b' hace',\n",
              " 887: b' mayor',\n",
              " 888: b' tiempo',\n",
              " 889: b' op',\n",
              " 890: b' Ha',\n",
              " 891: b'ista',\n",
              " 892: b'alidad',\n",
              " 893: b'ano',\n",
              " 894: b'udad',\n",
              " 895: b' toda',\n",
              " 896: b' \\xc3\\xa9',\n",
              " 897: b' as\\xc3\\xad',\n",
              " 898: b'bier',\n",
              " 899: b' bas',\n",
              " 900: b'cado',\n",
              " 901: b' clar',\n",
              " 902: b' embar',\n",
              " 903: b' conside',\n",
              " 904: b' dos',\n",
              " 905: b' respon',\n",
              " 906: b'ios',\n",
              " 907: b'uda',\n",
              " 908: b'reo',\n",
              " 909: b'OO',\n",
              " 910: b' apro',\n",
              " 911: b'adores',\n",
              " 912: b'uso',\n",
              " 913: b'eres',\n",
              " 914: b'nos',\n",
              " 915: b' ch',\n",
              " 916: b'ito',\n",
              " 917: b'lti',\n",
              " 918: b'esti',\n",
              " 919: b'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee',\n",
              " 920: b'entar',\n",
              " 921: b'ren',\n",
              " 922: b' sec',\n",
              " 923: b'untos',\n",
              " 924: b' apoy',\n",
              " 925: b' cual',\n",
              " 926: b' ade',\n",
              " 927: b' hemos',\n",
              " 928: b' espe',\n",
              " 929: b' cier',\n",
              " 930: b' gen',\n",
              " 931: b'ceso',\n",
              " 932: b' fu',\n",
              " 933: b' ne',\n",
              " 934: b' quiero',\n",
              " 935: b' ju',\n",
              " 936: b'cos',\n",
              " 937: b'ion',\n",
              " 938: b' sino',\n",
              " 939: b' desde',\n",
              " 940: b' cuesti\\xc3\\xb3n',\n",
              " 941: b' anim',\n",
              " 942: b' resul',\n",
              " 943: b' parti',\n",
              " 944: b' exist',\n",
              " 945: b' pregun',\n",
              " 946: b'asta',\n",
              " 947: b'oder',\n",
              " 948: b' ciudad',\n",
              " 949: b' embargo',\n",
              " 950: b' trabajo',\n",
              " 951: b'tamente',\n",
              " 952: b' prop',\n",
              " 953: b' situ',\n",
              " 954: b' desarrollo',\n",
              " 955: b'ino',\n",
              " 956: b' \\xc3\\xbalti',\n",
              " 957: b'uar',\n",
              " 958: b'aaaa',\n",
              " 959: b' hum',\n",
              " 960: b'cios',\n",
              " 961: b'\\xc3\\xb3lo',\n",
              " 962: b' ad',\n",
              " 963: b' forma',\n",
              " 964: b' mundo',\n",
              " 965: b'ual',\n",
              " 966: b'\\xc3\\xa9s',\n",
              " 967: b'ismo',\n",
              " 968: b' qu\\xc3\\xa9',\n",
              " 969: b'ye',\n",
              " 970: b'idos',\n",
              " 971: b' debate',\n",
              " 972: b'rupo',\n",
              " 973: b' cuenta',\n",
              " 974: b' ampli',\n",
              " 975: b'gen',\n",
              " 976: b' est\\xc3\\xa1n',\n",
              " 977: b'\\xc3\\xbabl',\n",
              " 978: b'aran',\n",
              " 979: b' estas',\n",
              " 980: b' fav',\n",
              " 981: b'ancia',\n",
              " 982: b' a\\xc3\\xb1os',\n",
              " 983: b' cap',\n",
              " 984: b' mom',\n",
              " 985: b' seguridad',\n",
              " 986: b' def',\n",
              " 987: b'anci',\n",
              " 988: b' da',\n",
              " 989: b'cci\\xc3\\xb3n',\n",
              " 990: b'pl',\n",
              " 991: b'mm',\n",
              " 992: b'tur',\n",
              " 993: b'dop',\n",
              " 994: b'arse',\n",
              " 995: b'ale',\n",
              " 996: b'\\xc3\\xadan',\n",
              " 997: b'ombre',\n",
              " 998: b' Los',\n",
              " 999: b' cab',\n",
              " ...}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQlfiiw-Cocb"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUw5QzzxCrNS",
        "outputId": "47b947af-27ee-4abc-8fce-3260950171e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bfabe5417d0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(3647)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB0-gZkLCs3S",
        "outputId": "d5c467b5-04ee-4605-add9-b9f2551e381f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'vocab_size': 50257, 'context_length': 1024, 'dropout': 0.2, 'qkv_bias': True, 'emb_dim': 1024, 'n_layers': 24, 'n_heads': 16}\n",
            "336.885774 M parameters\n"
          ]
        }
      ],
      "source": [
        "from transformer.pedro_model import GPTLanguageModel\n",
        "from transformer import BASE_CONFIG, selConfig\n",
        "\n",
        "selConfig('gpt2-medium (355M)')\n",
        "\n",
        "block_size = BASE_CONFIG['context_length']\n",
        "n_embd = BASE_CONFIG['emb_dim']\n",
        "n_head = BASE_CONFIG['n_heads']\n",
        "n_layer = BASE_CONFIG['n_layers']\n",
        "dropout = BASE_CONFIG['dropout']\n",
        "batch_size = 2\n",
        "vocab_size = get_vocab_size(tokenizer)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = GPTLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=block_size,\n",
        "    n_embd=n_embd,\n",
        "    n_head=n_head,\n",
        "    n_layer=n_layer,\n",
        "    dropout=dropout,\n",
        "    device=device\n",
        ").to(device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(16398, 1024)\n",
              "  (position_embedding_table): Embedding(1024, 1024)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (6): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (7): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (8): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (9): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (10): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (11): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (12): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (13): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (14): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (15): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (16): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (17): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (18): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (19): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (20): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (21): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (22): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (23): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-15): 16 x Head(\n",
              "            (key): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=1024, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (layer_norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (final_linear_layer): Linear(in_features=1024, out_features=16398, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMMpc93zC8XB"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lJK2Z_7C7ib",
        "outputId": "ff3e1af3-c1b5-4155-f730-7479f7cf1430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (452560,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "data_path = \"../output/encoded_data/encoded_atlaset.npy\"\n",
        "data = np.load(data_path, mmap_mode='r')\n",
        "print('Data shape:', data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k32YO0QHDHAr",
        "outputId": "f20dd606-d55a-4e96-ebf1-245cf9045f85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "407304"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_index = int(0.9*len(data))\n",
        "split_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkhB6LoNDO5G"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3JWcS0DTDJtc"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    if split == 'train':\n",
        "        start_index = 0\n",
        "        end_index = split_index\n",
        "    else:\n",
        "        start_index = split_index\n",
        "        end_index = len(data)\n",
        "\n",
        "    index = torch.randint(start_index, end_index - block_size, (batch_size,))\n",
        "    x_batch, y_batch = [], []\n",
        "    for i in index:\n",
        "        x_batch.append(data[i:i+block_size])\n",
        "        y_batch.append(data[i+1:i+block_size+1])\n",
        "\n",
        "    x_batch = np.array(x_batch)\n",
        "    y_batch = np.array(y_batch)\n",
        "\n",
        "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
        "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
        "\n",
        "    return x_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x8nEUq8BDSKc"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss() -> Dict:\n",
        "    output = {}\n",
        "    eval_iters = 1000\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "        output[split] = losses.mean()\n",
        "    model.train()\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SHggvNLnDTln"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(\n",
        "    model: GPTLanguageModel,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epoch: int,\n",
        "    loss: float,\n",
        "    file_path: str = \"checkpoint.pth\"\n",
        ") -> None:\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMA_WyDDDflx"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLVYqR17ER6j"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
            "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
            "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
            "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "eZhjb9yoDU3v",
        "outputId": "0dab38f6-ea25-4fc9-93c7-6acf0063d330"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 1002/203140 [02:46<559:35:04,  9.97s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1000: train loss 8.8364, val loss 8.9887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   1%|          | 2002/203140 [05:34<552:26:26,  9.89s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 2000: train loss 9.2697, val loss 9.4190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   1%|▏         | 3002/203140 [08:20<550:37:53,  9.90s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 3000: train loss 9.7622, val loss 9.9576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   2%|▏         | 4002/203140 [11:08<551:04:53,  9.96s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 4000: train loss 9.9918, val loss 10.2262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   2%|▏         | 5002/203140 [13:55<544:31:08,  9.89s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5000: train loss 9.9077, val loss 10.1444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   3%|▎         | 6002/203140 [16:42<546:07:57,  9.97s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 6000: train loss 10.1379, val loss 10.3038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   3%|▎         | 7002/203140 [19:30<541:14:58,  9.93s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 7000: train loss 10.1546, val loss 10.3824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   4%|▍         | 8002/203140 [22:16<533:35:21,  9.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 8000: train loss 10.3148, val loss 10.5464\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   4%|▍         | 9002/203140 [25:04<537:58:17,  9.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 9000: train loss 10.4091, val loss 10.6450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   5%|▍         | 9999/203140 [26:51<5:41:34,  9.42it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10000: train loss 10.1710, val loss 10.3744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   5%|▌         | 11002/203140 [30:39<524:33:09,  9.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 11000: train loss 10.8984, val loss 11.1518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   6%|▌         | 12002/203140 [33:24<521:12:24,  9.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 12000: train loss 10.5420, val loss 10.7526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   6%|▋         | 13002/203140 [36:11<524:19:14,  9.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 13000: train loss 10.7335, val loss 11.0229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   7%|▋         | 14002/203140 [38:58<524:23:29,  9.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 14000: train loss 10.7016, val loss 11.0130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   7%|▋         | 15002/203140 [41:45<515:46:33,  9.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15000: train loss 10.3583, val loss 10.6376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   8%|▊         | 16002/203140 [44:33<511:16:14,  9.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 16000: train loss 10.5111, val loss 10.7844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   8%|▊         | 17002/203140 [47:19<515:35:54,  9.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 17000: train loss 10.9179, val loss 11.2288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   9%|▉         | 18002/203140 [50:07<513:14:54,  9.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 18000: train loss 10.6125, val loss 10.9614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   9%|▉         | 19002/203140 [52:54<502:27:36,  9.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 19000: train loss 11.1777, val loss 11.6232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  10%|▉         | 19999/203140 [54:39<5:19:53,  9.54it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20000: train loss 10.7890, val loss 11.0736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  10%|█         | 21002/203140 [58:27<503:08:17,  9.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 21000: train loss 11.2303, val loss 11.6890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  11%|█         | 22002/203140 [1:01:14<502:53:27,  9.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 22000: train loss 11.2662, val loss 11.6514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  11%|█▏        | 23002/203140 [1:04:02<501:05:27, 10.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 23000: train loss 11.1808, val loss 11.5100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  12%|█▏        | 24002/203140 [1:06:50<492:46:51,  9.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 24000: train loss 11.2231, val loss 11.6648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  12%|█▏        | 25002/203140 [1:09:38<492:32:14,  9.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25000: train loss 11.0690, val loss 11.3552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  13%|█▎        | 26002/203140 [1:12:25<487:12:06,  9.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 26000: train loss 11.0955, val loss 11.4315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  13%|█▎        | 27002/203140 [1:15:12<487:39:40,  9.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 27000: train loss 11.2047, val loss 11.6925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  14%|█▍        | 28002/203140 [1:17:59<477:11:34,  9.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 28000: train loss 11.1704, val loss 11.4702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  14%|█▍        | 29002/203140 [1:20:45<479:34:31,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 29000: train loss 11.2261, val loss 11.6258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  15%|█▍        | 29999/203140 [1:22:30<5:07:02,  9.40it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30000: train loss 10.8345, val loss 11.2438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  15%|█▌        | 31002/203140 [1:26:20<479:28:25, 10.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 31000: train loss 10.8668, val loss 11.2571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  16%|█▌        | 32002/203140 [1:29:08<468:42:53,  9.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 32000: train loss 11.2964, val loss 11.6316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  16%|█▌        | 33002/203140 [1:31:54<462:23:06,  9.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 33000: train loss 11.2474, val loss 11.6119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  17%|█▋        | 34002/203140 [1:34:40<464:21:47,  9.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 34000: train loss 11.4634, val loss 11.7738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  17%|█▋        | 35002/203140 [1:37:25<457:22:38,  9.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35000: train loss 11.6599, val loss 12.1790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  18%|█▊        | 36002/203140 [1:40:11<461:56:34,  9.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 36000: train loss 11.8006, val loss 12.3035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  18%|█▊        | 37002/203140 [1:42:59<460:07:14,  9.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 37000: train loss 11.7089, val loss 12.0829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  19%|█▊        | 38002/203140 [1:45:47<459:46:28, 10.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 38000: train loss 11.5856, val loss 12.0529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  19%|█▉        | 39002/203140 [1:48:35<457:08:04, 10.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 39000: train loss 12.1000, val loss 12.5853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  20%|█▉        | 39999/203140 [1:50:22<4:47:49,  9.45it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40000: train loss 11.2447, val loss 11.7151\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  20%|██        | 41002/203140 [1:54:11<451:40:24, 10.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 41000: train loss 12.0029, val loss 12.5644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  21%|██        | 42002/203140 [1:56:58<442:12:27,  9.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 42000: train loss 12.0068, val loss 12.5688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  21%|██        | 43002/203140 [1:59:46<437:49:41,  9.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 43000: train loss 12.0169, val loss 12.5965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  22%|██▏       | 44002/203140 [2:02:32<437:59:24,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 44000: train loss 12.0426, val loss 12.5831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  22%|██▏       | 45002/203140 [2:05:17<430:31:26,  9.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45000: train loss 11.7593, val loss 12.3518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  23%|██▎       | 46002/203140 [2:08:03<427:41:47,  9.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 46000: train loss 11.9505, val loss 12.4214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  23%|██▎       | 47002/203140 [2:10:48<425:22:29,  9.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 47000: train loss 12.4336, val loss 13.0183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  24%|██▎       | 48002/203140 [2:13:34<423:04:08,  9.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 48000: train loss 12.5088, val loss 13.1105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  24%|██▍       | 49002/203140 [2:16:22<428:26:36, 10.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 49000: train loss 12.0178, val loss 12.5876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  25%|██▍       | 49999/203140 [2:18:08<4:30:11,  9.45it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50000: train loss 11.9929, val loss 12.5964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  25%|██▌       | 51002/203140 [2:21:56<422:44:33, 10.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 51000: train loss 12.5582, val loss 13.2869\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  26%|██▌       | 52002/203140 [2:24:43<414:21:52,  9.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 52000: train loss 12.0332, val loss 12.5240\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  26%|██▌       | 53002/203140 [2:27:29<408:00:19,  9.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 53000: train loss 12.2188, val loss 12.8130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  27%|██▋       | 54002/203140 [2:30:16<413:57:57,  9.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 54000: train loss 12.8692, val loss 13.5555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  27%|██▋       | 55002/203140 [2:33:03<407:50:55,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55000: train loss 12.2992, val loss 12.9438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  28%|██▊       | 56002/203140 [2:35:51<410:19:39, 10.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 56000: train loss 12.5071, val loss 13.2518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  28%|██▊       | 57002/203140 [2:38:39<403:52:14,  9.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 57000: train loss 11.7964, val loss 12.5267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  29%|██▊       | 58002/203140 [2:41:24<393:57:36,  9.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 58000: train loss 12.4159, val loss 12.9738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  29%|██▉       | 59002/203140 [2:44:11<396:42:21,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 59000: train loss 12.3507, val loss 12.9438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  30%|██▉       | 59999/203140 [2:45:57<4:12:39,  9.44it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60000: train loss 12.0350, val loss 12.6536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  30%|███       | 61002/203140 [2:49:46<395:48:31, 10.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 61000: train loss 12.4404, val loss 12.3951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  31%|███       | 62002/203140 [2:52:34<386:47:05,  9.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 62000: train loss 12.1945, val loss 12.2200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  31%|███       | 63002/203140 [2:55:21<388:26:28,  9.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 63000: train loss 12.3456, val loss 12.2884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  32%|███▏      | 64002/203140 [2:58:08<381:24:01,  9.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 64000: train loss 12.0875, val loss 11.9454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  32%|███▏      | 65002/203140 [3:00:54<379:38:55,  9.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65000: train loss 12.5839, val loss 12.4020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  32%|███▏      | 66002/203140 [3:03:41<379:10:54,  9.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 66000: train loss 12.4237, val loss 12.1079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  33%|███▎      | 67002/203140 [3:06:29<379:06:36, 10.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 67000: train loss 12.1588, val loss 11.8565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  33%|███▎      | 68002/203140 [3:09:18<378:02:34, 10.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 68000: train loss 12.6163, val loss 12.3701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  34%|███▍      | 69002/203140 [3:12:08<373:30:03, 10.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 69000: train loss 12.5054, val loss 12.1844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  34%|███▍      | 69999/203140 [3:13:54<3:55:14,  9.43it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70000: train loss 11.8696, val loss 11.5072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  35%|███▍      | 71002/203140 [3:17:42<361:37:23,  9.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 71000: train loss 12.1058, val loss 11.7372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  35%|███▌      | 72002/203140 [3:20:27<357:27:18,  9.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 72000: train loss 11.9178, val loss 11.4090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  36%|███▌      | 73002/203140 [3:23:14<356:44:04,  9.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 73000: train loss 12.2435, val loss 11.9496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:  36%|███▋      | 73999/203140 [3:25:17<5:58:16,  6.01it/s]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batches_processed % eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     losses = \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     51\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatches_processed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m     train_losses.append(losses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m         x, y = get_batch(split)\n\u001b[32m     13\u001b[39m         _, loss = model(x, y)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         losses[k] = \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     output[split] = losses.mean()\n\u001b[32m     16\u001b[39m model.train()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "gradient_accumulation_steps = 8\n",
        "eval_interval = 1000\n",
        "save_interval = 10000\n",
        "\n",
        "# equivalent to len(data) - block_size\n",
        "total_data_to_process = split_index - block_size\n",
        "total_data_to_process_in_batches = total_data_to_process // batch_size\n",
        "\n",
        "learning_rate = 3e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "batches_processed = 0\n",
        "train_losses, val_losses = [], []\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "for i in tqdm(\n",
        "    iterable=range(0, total_data_to_process, batch_size),\n",
        "    desc=\"Processing\",\n",
        "    total=total_data_to_process_in_batches\n",
        "):\n",
        "    # Load a batch of data\n",
        "    x_batch, y_batch = [], []\n",
        "    for j in range(i, i+batch_size):\n",
        "        x_batch.append(data[j:j+block_size])\n",
        "        y_batch.append(data[j+1:j+block_size+1])\n",
        "\n",
        "    x_batch = np.array(x_batch)\n",
        "    y_batch = np.array(y_batch)\n",
        "\n",
        "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
        "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "    loss /= gradient_accumulation_steps\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient accumulation\n",
        "    batches_processed += 1\n",
        "    if batches_processed % gradient_accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # Evaluate the model\n",
        "    if batches_processed % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(\n",
        "            f\"Batch {batches_processed}: \"\n",
        "            f\"train loss {losses['train']:.4f}, \"\n",
        "            f\"val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        train_losses.append(losses['train'])\n",
        "        val_losses.append(losses['val'])\n",
        "\n",
        "    # Save the model\n",
        "    if batches_processed % save_interval == 0:\n",
        "        save_checkpoint(\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            epoch=batches_processed,\n",
        "            loss=loss.item(),\n",
        "            file_path=f\"../output/pre_training/run_11/checkpoint_{batches_processed}.pth\"\n",
        "        )\n",
        "\n",
        "if batches_processed % gradient_accumulation_steps != 0:\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad(set_to_none=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRHz4iyrRChZ",
        "outputId": "78693910-5cc1-4208-d299-beafdf3c3348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, como 14, debemos votar a otros productos agrícola?\n",
            " Si haber tomado la vez más parte avanzada y año 69, presentada por haber tomado la Sra. Aparicio Sánchez por la iniciativa de él.\n",
            "    –    – Aparicio S\n"
          ]
        }
      ],
      "source": [
        "input_tokens = tokenizer.encode(\"Hola, como \")\n",
        "input_tokens = torch.tensor(\n",
        "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_tokens=input_tokens, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(output[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_checkpoint(\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            epoch=batches_processed,\n",
        "            loss=loss.item(),\n",
        "            file_path=f\"../output/pre_training/run_11/checkpoint_{batches_processed}.pth\"\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOFf3H/f2uqlO7MsAzzMlhc",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
