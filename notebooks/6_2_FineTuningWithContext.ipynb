{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the fine-tuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"../output/fine_tuning/data/fine_tuning.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenize the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "for item in data:\n",
    "    tokenized_item = tokenizer.encode(item, allowed_special=\"all\")\n",
    "    tokenized_data.append(tokenized_item)\n",
    "\n",
    "len(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be careful when splitting the data. We want to keep the multi-turn conversations complete in each part. So, the training and validation sets should start with a `You` message and end with an `Assistant` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: \n",
      "Start message: <|startoftext|>La Wikipedia Del Porno(Andres)\n",
      "End message: <|startoftext|>La Wikipedia Del Porno(Andres)\n",
      "\n",
      "Validation set: \n",
      "Start message: <|startoftext|>Pedro Guill Ferri\n",
      "End message: <|startoftext|>La Wikipedia Del Porno(Andres)\n"
     ]
    }
   ],
   "source": [
    "initial_split_index = int(0.95 * len(data))\n",
    "\n",
    "# Adjusting the index to ensure that the training set ends with \"Assistant\" message\n",
    "# and that the validation set starts with \"You\" message\n",
    "\n",
    "# Scanning backward to find an Assistant message\n",
    "split_index = initial_split_index\n",
    "while split_index > 0 and not data[split_index-1].startswith('<|startoftext|>La Wikipedia Del Porno(Andres)'):\n",
    "    split_index -= 1\n",
    "\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]\n",
    "\n",
    "print(\"Training set: \")\n",
    "print(f\"Start message: {train_data[0].split('<|separator|>')[0]}\")\n",
    "print(f\"End message: {train_data[-1].split('<|separator|>')[0]}\")\n",
    "\n",
    "print(\"\\nValidation set: \")\n",
    "print(f\"Start message: {val_data[0].split('<|separator|>')[0]}\")\n",
    "print(f\"End message: {val_data[-1].split('<|separator|>')[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the index that we should use to split the data. Now, let's split the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenized_data[:split_index]\n",
    "val_data = tokenized_data[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to combine the `You` and `Assistant` turns into one sequence. We will make sure that the resulting sequence does not exceed the `block_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "\n",
    "\n",
    "def combine_turns(data: list[list[int]], should_trim_long_sequences: bool) -> list[list[int]]:\n",
    "    combined_turns_data = []\n",
    "    for i in range(0, len(data)-1, 2):\n",
    "        you_message = data[i]\n",
    "        assistant_message = data[i+1]\n",
    "        if not you_message or not assistant_message:\n",
    "            continue\n",
    "\n",
    "        final_message = you_message + assistant_message\n",
    "        if len(final_message) > block_size and should_trim_long_sequences:\n",
    "            final_message = final_message[-block_size:]\n",
    "\n",
    "        combined_turns_data.append(final_message)\n",
    "    return combined_turns_data\n",
    "\n",
    "\n",
    "combined_train_data = combine_turns(\n",
    "    data=train_data,\n",
    "    should_trim_long_sequences=True\n",
    ")\n",
    "combined_val_data = combine_turns(\n",
    "    data=val_data,\n",
    "    should_trim_long_sequences=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Length before: 10405\n",
      "Length after: 5202\n",
      "\n",
      "Validation data\n",
      "Length before: 548\n",
      "Length after: 274\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data\")\n",
    "print(f\"Length before: {len(train_data)}\")\n",
    "print(f\"Length after: {len(combined_train_data)}\")\n",
    "\n",
    "print(\"\\nValidation data\")\n",
    "print(f\"Length before: {len(val_data)}\")\n",
    "print(f\"Length after: {len(combined_val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will merge the `You` and `Assistant` parts into one sequence. Then, we will try to combine multiple sequences of `You` and `Assistant` into a single input, but only if the sequence length stays smaller than the block size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_conversation_turns(combined_data: list[list[int]], block_size: int) -> list[list[int]]:\n",
    "    new_data = []\n",
    "    current_sequence = []\n",
    "\n",
    "    for sequence in combined_data:\n",
    "        if len(current_sequence) + len(sequence) <= block_size:\n",
    "            current_sequence.extend(sequence)\n",
    "        else:\n",
    "            if current_sequence:\n",
    "                new_data.append(current_sequence)\n",
    "            current_sequence = sequence.copy()\n",
    "\n",
    "    # Add the last block if it's not empty\n",
    "    if current_sequence:\n",
    "        new_data.append(current_sequence)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "merged_train_data = merge_conversation_turns(\n",
    "    combined_data=combined_train_data,\n",
    "    block_size=block_size\n",
    ")\n",
    "merged_val_data = merge_conversation_turns(\n",
    "    combined_data=combined_val_data,\n",
    "    block_size=block_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5202, 1397)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_train_data), len(merged_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert each sequence of tokens into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 86 at dim 1 (got 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_train_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m val_data = torch.tensor(combined_val_data)\n",
      "\u001b[31mValueError\u001b[39m: expected sequence of length 86 at dim 1 (got 44)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_data = torch.tensor(combined_train_data)\n",
    "val_data = torch.tensor(combined_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our token sequences don't all have the same length, we can't turn the data into a tensor all at once. To do that, all sequences need to have the same length.\n",
    "\n",
    "That's why we need to use padding to fix this problem. We can add padding at the start or end of the sequence. Let's add it to the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2488, 256]), torch.Size([135, 256]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)\n",
    "\n",
    "# The token `<|padding|>` is used to mask the padding tokens.\n",
    "# Masking means the model will ignore these tokens during training.\n",
    "# In other words, the loss will not be calculated for these tokens.\n",
    "padding_token = tokenizer.special_tokens[\"<|padding|>\"]\n",
    "\n",
    "\n",
    "def apply_padding_to_data(data: list[list[int]], block_size: int, padding_token: int) -> torch.Tensor:\n",
    "    tensors = []\n",
    "    for i in range(len(data)):\n",
    "        tensor = torch.tensor(data[i])\n",
    "        padded_tensor = torch.nn.functional.pad(\n",
    "            input=tensor,\n",
    "            # for right padding:\n",
    "            pad=(0, block_size - len(tensor)),\n",
    "            # pad=(block_size - len(tensor), 0),\n",
    "            value=padding_token\n",
    "        )\n",
    "        tensors.append(padded_tensor)\n",
    "\n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "train_data_tensor = apply_padding_to_data(\n",
    "    data=merged_train_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "val_data_tensor = apply_padding_to_data(\n",
    "    data=merged_val_data,\n",
    "    block_size=block_size,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "\n",
    "train_data_tensor.shape, val_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1024,   76,   97,   32,   87,  105,  690,  369,  298,   97,   32,   68,\n",
       "         538,   32,  635,  588,   40,  866,  570,  265,   41, 1025,   87,  101,\n",
       "          63,   10,   84,  101,   32,  406,  381,  115,  426,   63,   10,   65,\n",
       "         121,  101,   10,   79,  121,  101,   42,   10,  691,   98,  307,   32,\n",
       "          98,  108,  293,  107,   32,  288,  416,   10,   67,  391,  111,   32,\n",
       "         524,   32,  558,   32,  109,  305,  307,   32,  334,   32,  299,  547,\n",
       "          32,   99,  381,  111,   32,  292,   32,  334,  801,  105,  111,   32,\n",
       "         524,   32,  356,  321,   32,  100,  505,  111,   32,  334,  108,   32,\n",
       "         103,  381,  943,  105,  111,   32,  334,   32,  299,  547,   10,   72,\n",
       "         299,   32,  422,  308,  561,   32,  265,  111,   32,  588,   63, 1026,\n",
       "        1024,   80,  101,  100,  458,   32,   71,  117,  105,  361,   32,   70,\n",
       "         261,  381, 1025,  673, 1026, 1024,   76,   97,   32,   87,  105,  690,\n",
       "         369,  298,   97,   32,   68,  538,   32,  635,  588,   40,  866,  570,\n",
       "         265,   41, 1025,   86,  292,  101, 1026, 1024,   80,  101,  100,  458,\n",
       "          32,   71,  117,  105,  361,   32,   70,  261,  381, 1025,  635,   32,\n",
       "         323,  261,  311,   32,  278,  570,  265,   32,  672,  333,   97,  426,\n",
       "          32,  333,   97,   32,  401,   32,  109,  299,  293,  307,   32,  390,\n",
       "          32,  380,  261,  374,   32,  115,  494,  105,  108,  261, 1026, 1028,\n",
       "        1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028,\n",
       "        1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028,\n",
       "        1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028,\n",
       "        1028, 1028, 1028, 1028])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1024,   80,  101,  100,  458,   32,   71,  117,  105,  361,   32,   70,\n",
       "         261,  381, 1025,  390,  524,   32,  538,   32,  494,  361,  111,   32,\n",
       "          97,   32,  350,  369,  122,  305,  111,   32,   97,   32,  380,  261,\n",
       "          32,  106,  940,  111,  112,  397,  265, 1026, 1024,   76,   97,   32,\n",
       "          87,  105,  690,  369,  298,   97,   32,   68,  538,   32,  635,  588,\n",
       "          40,  866,  570,  265,   41, 1025,  673, 1026, 1024,   80,  101,  100,\n",
       "         458,   32,   71,  117,  105,  361,   32,   70,  261,  381, 1025,   40,\n",
       "         355,   32,  303,   97,   32,  406,  106,  261,   41, 1026, 1024,   76,\n",
       "          97,   32,   87,  105,  690,  369,  298,   97,   32,   68,  538,   32,\n",
       "         635,  588,   40,  866,  570,  265,   41, 1025,   83,  101, 1026, 1024,\n",
       "          80,  101,  100,  458,   32,   71,  117,  105,  361,   32,   70,  261,\n",
       "         381, 1025,   83,  101,   32,  458,   98,  264,  318,   32,   97,   32,\n",
       "         303,   32,  108,  305,  114,  318,   10,   76,   97,   32,  575,  116,\n",
       "         305,   97,   10,   70,  864,  122,   32,  506,  111,   44,   32,  121,\n",
       "          32,  819,  111,   32,  524,   32,  356,  101,   32,  506,  111,   32,\n",
       "         390,  750,   32,  558,   32,  480,  103,  299, 1026, 1024,   76,   97,\n",
       "          32,   87,  105,  690,  369,  298,   97,   32,   68,  538,   32,  635,\n",
       "         588,   40,  866,  570,  265,   41, 1025,  120,   68,   10,   70,  864,\n",
       "         122,   32,  506,  111,   10,   89,   32,  340,   32,  588,   32,   97,\n",
       "          32,  406,  329,   97,   32,  426,   32,  558,   32,  480,  738,   10,\n",
       "         605,  738, 1026, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028,\n",
       "        1028, 1028, 1028, 1028])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creat the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, device: torch.device, padding_token: int):\n",
    "        self.data = data  # shape: (num_samples, block_size)\n",
    "        self.device = device\n",
    "        self.padding_token = padding_token\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample = self.data[index]\n",
    "        x = sample.to(self.device)\n",
    "        y = sample[1:].to(self.device)\n",
    "        padding_tensor = torch.tensor([self.padding_token], device=self.device)\n",
    "        y = torch.cat((y, padding_tensor))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = FineTuningDataset(\n",
    "    data=train_data_tensor,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = FineTuningDataset(\n",
    "    data=val_data_tensor,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256]), torch.Size([32, 256]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.795338 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 256\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    ignore_index=tokenizer.special_tokens[\"<|padding|>\"],\n",
    ").to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"../output/pre_training/base_model_checkpoint.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate from the model to make sure that the weights were loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Bro Por qué ya mierda llegó a casa y dil Y no lo olvidarlo Tocará tener selado CATocará tener un así que tiendo sobre tocará tener sectubre cuando me tocarnet de mi Mancia o A Lo estoy esperando Esto en casa Pues dejo de tu casa Pues es cambiarme toca ca por qué deciraryo? Pues si salgo yo de mi oya \n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer.encode(\"Hola Bro \", allowed_special=\"all\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    ") -> Dict[str, float]:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = []\n",
    "        for x, y in loader:\n",
    "            with torch.no_grad():\n",
    "                _, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "        output[split] = sum(losses) / len(losses)\n",
    "\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / step 0: train loss 10.7068, val loss 10.8986\n",
      "iteration 0 / step 5: train loss 4.2161, val loss 4.5380\n",
      "iteration 0 / step 10: train loss 3.3610, val loss 3.6074\n",
      "iteration 0 / step 15: train loss 2.9620, val loss 3.2077\n",
      "iteration 0 / step 20: train loss 2.6977, val loss 2.9501\n",
      "iteration 0 / step 25: train loss 2.4703, val loss 2.7307\n",
      "iteration 0 / step 30: train loss 2.3035, val loss 2.5707\n",
      "iteration 0 / step 35: train loss 2.1899, val loss 2.4523\n",
      "iteration 0 / step 40: train loss 2.1046, val loss 2.3585\n",
      "iteration 0 / step 45: train loss 2.0446, val loss 2.2970\n",
      "iteration 0 / step 50: train loss 1.9982, val loss 2.2521\n",
      "iteration 0 / step 55: train loss 1.9602, val loss 2.2222\n",
      "iteration 0 / step 60: train loss 1.9274, val loss 2.1899\n",
      "iteration 0 / step 65: train loss 1.9019, val loss 2.1635\n",
      "iteration 0 / step 70: train loss 1.8795, val loss 2.1401\n",
      "iteration 0 / step 75: train loss 1.8618, val loss 2.1314\n",
      "iteration 0 / step 77: train loss 1.8552, val loss 2.1256\n",
      "iteration 1 / step 0: train loss 1.8513, val loss 2.1209\n",
      "iteration 1 / step 5: train loss 1.8357, val loss 2.1086\n",
      "iteration 1 / step 10: train loss 1.8237, val loss 2.0992\n",
      "iteration 1 / step 15: train loss 1.8055, val loss 2.0818\n",
      "iteration 1 / step 20: train loss 1.7968, val loss 2.0797\n",
      "iteration 1 / step 25: train loss 1.7843, val loss 2.0616\n",
      "iteration 1 / step 30: train loss 1.7778, val loss 2.0598\n",
      "iteration 1 / step 35: train loss 1.7698, val loss 2.0546\n",
      "iteration 1 / step 40: train loss 1.7614, val loss 2.0443\n",
      "iteration 1 / step 45: train loss 1.7503, val loss 2.0351\n",
      "iteration 1 / step 50: train loss 1.7441, val loss 2.0298\n",
      "iteration 1 / step 55: train loss 1.7344, val loss 2.0195\n",
      "iteration 1 / step 60: train loss 1.7269, val loss 2.0128\n",
      "iteration 1 / step 65: train loss 1.7221, val loss 2.0107\n",
      "iteration 1 / step 70: train loss 1.7130, val loss 2.0033\n",
      "iteration 1 / step 75: train loss 1.7095, val loss 2.0032\n",
      "iteration 1 / step 77: train loss 1.7087, val loss 2.0040\n",
      "iteration 2 / step 0: train loss 1.7077, val loss 2.0027\n",
      "iteration 2 / step 5: train loss 1.6979, val loss 1.9944\n",
      "iteration 2 / step 10: train loss 1.6923, val loss 1.9969\n",
      "iteration 2 / step 15: train loss 1.6849, val loss 1.9859\n",
      "iteration 2 / step 20: train loss 1.6782, val loss 1.9843\n",
      "iteration 2 / step 25: train loss 1.6751, val loss 1.9832\n",
      "iteration 2 / step 30: train loss 1.6717, val loss 1.9791\n",
      "iteration 2 / step 35: train loss 1.6660, val loss 1.9786\n",
      "iteration 2 / step 40: train loss 1.6600, val loss 1.9755\n",
      "iteration 2 / step 45: train loss 1.6550, val loss 1.9673\n",
      "iteration 2 / step 50: train loss 1.6509, val loss 1.9724\n",
      "iteration 2 / step 55: train loss 1.6501, val loss 1.9624\n",
      "iteration 2 / step 60: train loss 1.6425, val loss 1.9667\n",
      "iteration 2 / step 65: train loss 1.6392, val loss 1.9571\n",
      "iteration 2 / step 70: train loss 1.6344, val loss 1.9583\n",
      "iteration 2 / step 75: train loss 1.6286, val loss 1.9488\n",
      "iteration 2 / step 77: train loss 1.6278, val loss 1.9464\n",
      "iteration 3 / step 0: train loss 1.6250, val loss 1.9453\n",
      "iteration 3 / step 5: train loss 1.6231, val loss 1.9566\n",
      "iteration 3 / step 10: train loss 1.6166, val loss 1.9538\n",
      "iteration 3 / step 15: train loss 1.6135, val loss 1.9548\n",
      "iteration 3 / step 20: train loss 1.6085, val loss 1.9416\n",
      "iteration 3 / step 25: train loss 1.6017, val loss 1.9512\n",
      "iteration 3 / step 30: train loss 1.6008, val loss 1.9394\n",
      "iteration 3 / step 35: train loss 1.5970, val loss 1.9386\n",
      "iteration 3 / step 40: train loss 1.5922, val loss 1.9396\n",
      "iteration 3 / step 45: train loss 1.5901, val loss 1.9351\n",
      "iteration 3 / step 50: train loss 1.5858, val loss 1.9386\n",
      "iteration 3 / step 55: train loss 1.5823, val loss 1.9284\n",
      "iteration 3 / step 60: train loss 1.5789, val loss 1.9272\n",
      "iteration 3 / step 65: train loss 1.5731, val loss 1.9226\n",
      "iteration 3 / step 70: train loss 1.5703, val loss 1.9317\n",
      "iteration 3 / step 75: train loss 1.5676, val loss 1.9235\n",
      "iteration 3 / step 77: train loss 1.5643, val loss 1.9216\n",
      "iteration 4 / step 0: train loss 1.5636, val loss 1.9246\n",
      "iteration 4 / step 5: train loss 1.5601, val loss 1.9304\n",
      "iteration 4 / step 10: train loss 1.5589, val loss 1.9237\n",
      "iteration 4 / step 15: train loss 1.5563, val loss 1.9247\n",
      "iteration 4 / step 20: train loss 1.5509, val loss 1.9231\n",
      "iteration 4 / step 25: train loss 1.5460, val loss 1.9241\n",
      "iteration 4 / step 30: train loss 1.5440, val loss 1.9183\n",
      "iteration 4 / step 35: train loss 1.5411, val loss 1.9232\n",
      "iteration 4 / step 40: train loss 1.5381, val loss 1.9143\n",
      "iteration 4 / step 45: train loss 1.5368, val loss 1.9107\n",
      "iteration 4 / step 50: train loss 1.5315, val loss 1.9108\n",
      "iteration 4 / step 55: train loss 1.5298, val loss 1.9057\n",
      "iteration 4 / step 60: train loss 1.5223, val loss 1.9122\n",
      "iteration 4 / step 65: train loss 1.5218, val loss 1.9043\n",
      "iteration 4 / step 70: train loss 1.5235, val loss 1.9129\n",
      "iteration 4 / step 75: train loss 1.5155, val loss 1.9026\n",
      "iteration 4 / step 77: train loss 1.5157, val loss 1.9072\n",
      "iteration 5 / step 0: train loss 1.5143, val loss 1.9022\n",
      "iteration 5 / step 5: train loss 1.5083, val loss 1.9178\n",
      "iteration 5 / step 10: train loss 1.5052, val loss 1.9059\n",
      "iteration 5 / step 15: train loss 1.4995, val loss 1.9160\n",
      "iteration 5 / step 20: train loss 1.4991, val loss 1.8963\n",
      "iteration 5 / step 25: train loss 1.4958, val loss 1.9111\n",
      "iteration 5 / step 30: train loss 1.4980, val loss 1.9084\n",
      "iteration 5 / step 35: train loss 1.4922, val loss 1.9107\n",
      "iteration 5 / step 40: train loss 1.4873, val loss 1.9144\n",
      "iteration 5 / step 45: train loss 1.4869, val loss 1.9004\n",
      "iteration 5 / step 50: train loss 1.4816, val loss 1.9010\n",
      "iteration 5 / step 55: train loss 1.4783, val loss 1.8995\n",
      "iteration 5 / step 60: train loss 1.4782, val loss 1.8923\n",
      "iteration 5 / step 65: train loss 1.4744, val loss 1.8933\n",
      "iteration 5 / step 70: train loss 1.4711, val loss 1.8909\n",
      "iteration 5 / step 75: train loss 1.4717, val loss 1.8896\n",
      "iteration 5 / step 77: train loss 1.4697, val loss 1.8847\n",
      "iteration 6 / step 0: train loss 1.4682, val loss 1.8829\n",
      "iteration 6 / step 5: train loss 1.4642, val loss 1.9049\n",
      "iteration 6 / step 10: train loss 1.4602, val loss 1.8906\n",
      "iteration 6 / step 15: train loss 1.4564, val loss 1.9024\n",
      "iteration 6 / step 20: train loss 1.4580, val loss 1.8870\n",
      "iteration 6 / step 25: train loss 1.4521, val loss 1.9021\n",
      "iteration 6 / step 30: train loss 1.4521, val loss 1.8758\n",
      "iteration 6 / step 35: train loss 1.4493, val loss 1.8874\n",
      "iteration 6 / step 40: train loss 1.4455, val loss 1.8880\n",
      "iteration 6 / step 45: train loss 1.4406, val loss 1.8893\n",
      "iteration 6 / step 50: train loss 1.4359, val loss 1.8804\n",
      "iteration 6 / step 55: train loss 1.4352, val loss 1.8759\n",
      "iteration 6 / step 60: train loss 1.4326, val loss 1.8838\n",
      "iteration 6 / step 65: train loss 1.4312, val loss 1.8784\n",
      "iteration 6 / step 70: train loss 1.4290, val loss 1.8833\n",
      "iteration 6 / step 75: train loss 1.4282, val loss 1.8897\n",
      "iteration 6 / step 77: train loss 1.4274, val loss 1.8863\n",
      "iteration 7 / step 0: train loss 1.4266, val loss 1.8857\n",
      "iteration 7 / step 5: train loss 1.4209, val loss 1.8936\n",
      "iteration 7 / step 10: train loss 1.4197, val loss 1.8854\n",
      "iteration 7 / step 15: train loss 1.4151, val loss 1.8930\n",
      "iteration 7 / step 20: train loss 1.4149, val loss 1.8931\n",
      "iteration 7 / step 25: train loss 1.4132, val loss 1.8862\n",
      "iteration 7 / step 30: train loss 1.4088, val loss 1.8829\n",
      "iteration 7 / step 35: train loss 1.4079, val loss 1.8758\n",
      "iteration 7 / step 40: train loss 1.4071, val loss 1.8775\n",
      "iteration 7 / step 45: train loss 1.3968, val loss 1.8691\n",
      "iteration 7 / step 50: train loss 1.3974, val loss 1.8798\n",
      "iteration 7 / step 55: train loss 1.3991, val loss 1.8687\n",
      "iteration 7 / step 60: train loss 1.4003, val loss 1.8695\n",
      "iteration 7 / step 65: train loss 1.3927, val loss 1.8603\n",
      "iteration 7 / step 70: train loss 1.3913, val loss 1.8639\n",
      "iteration 7 / step 75: train loss 1.3870, val loss 1.8647\n",
      "iteration 7 / step 77: train loss 1.3876, val loss 1.8625\n",
      "iteration 8 / step 0: train loss 1.3877, val loss 1.8608\n",
      "iteration 8 / step 5: train loss 1.3856, val loss 1.8983\n",
      "iteration 8 / step 10: train loss 1.3857, val loss 1.8671\n",
      "iteration 8 / step 15: train loss 1.3774, val loss 1.8907\n",
      "iteration 8 / step 20: train loss 1.3791, val loss 1.8773\n",
      "iteration 8 / step 25: train loss 1.3735, val loss 1.8802\n",
      "iteration 8 / step 30: train loss 1.3728, val loss 1.8695\n",
      "iteration 8 / step 35: train loss 1.3693, val loss 1.8882\n",
      "iteration 8 / step 40: train loss 1.3677, val loss 1.8591\n",
      "iteration 8 / step 45: train loss 1.3620, val loss 1.8870\n",
      "iteration 8 / step 50: train loss 1.3630, val loss 1.8610\n",
      "iteration 8 / step 55: train loss 1.3581, val loss 1.8799\n",
      "iteration 8 / step 60: train loss 1.3568, val loss 1.8722\n",
      "iteration 8 / step 65: train loss 1.3556, val loss 1.8649\n",
      "iteration 8 / step 70: train loss 1.3500, val loss 1.8774\n",
      "iteration 8 / step 75: train loss 1.3518, val loss 1.8629\n",
      "iteration 8 / step 77: train loss 1.3485, val loss 1.8710\n",
      "iteration 9 / step 0: train loss 1.3481, val loss 1.8794\n",
      "iteration 9 / step 5: train loss 1.3448, val loss 1.8838\n",
      "iteration 9 / step 10: train loss 1.3434, val loss 1.8896\n",
      "iteration 9 / step 15: train loss 1.3413, val loss 1.8853\n",
      "iteration 9 / step 20: train loss 1.3358, val loss 1.8881\n",
      "iteration 9 / step 25: train loss 1.3357, val loss 1.8846\n",
      "iteration 9 / step 30: train loss 1.3337, val loss 1.8770\n",
      "iteration 9 / step 35: train loss 1.3280, val loss 1.8681\n",
      "iteration 9 / step 40: train loss 1.3317, val loss 1.8720\n",
      "iteration 9 / step 45: train loss 1.3290, val loss 1.8654\n",
      "iteration 9 / step 50: train loss 1.3269, val loss 1.8655\n",
      "iteration 9 / step 55: train loss 1.3287, val loss 1.8570\n",
      "iteration 9 / step 60: train loss 1.3194, val loss 1.8661\n",
      "iteration 9 / step 65: train loss 1.3197, val loss 1.8552\n",
      "iteration 9 / step 70: train loss 1.3172, val loss 1.8733\n",
      "iteration 9 / step 75: train loss 1.3199, val loss 1.8492\n",
      "iteration 9 / step 77: train loss 1.3168, val loss 1.8598\n",
      "iteration 10 / step 0: train loss 1.3155, val loss 1.8641\n",
      "iteration 10 / step 5: train loss 1.3116, val loss 1.8889\n",
      "iteration 10 / step 10: train loss 1.3073, val loss 1.8647\n",
      "iteration 10 / step 15: train loss 1.3025, val loss 1.8849\n",
      "iteration 10 / step 20: train loss 1.3046, val loss 1.8636\n",
      "iteration 10 / step 25: train loss 1.3000, val loss 1.8834\n",
      "iteration 10 / step 30: train loss 1.2994, val loss 1.8768\n",
      "iteration 10 / step 35: train loss 1.2961, val loss 1.8689\n",
      "iteration 10 / step 40: train loss 1.2960, val loss 1.8716\n",
      "iteration 10 / step 45: train loss 1.2986, val loss 1.8699\n",
      "iteration 10 / step 50: train loss 1.2928, val loss 1.8861\n",
      "iteration 10 / step 55: train loss 1.2912, val loss 1.8636\n",
      "iteration 10 / step 60: train loss 1.2861, val loss 1.8646\n",
      "iteration 10 / step 65: train loss 1.2851, val loss 1.8650\n",
      "iteration 10 / step 70: train loss 1.2832, val loss 1.8768\n",
      "iteration 10 / step 75: train loss 1.2782, val loss 1.8618\n",
      "iteration 10 / step 77: train loss 1.2773, val loss 1.8741\n",
      "iteration 11 / step 0: train loss 1.2776, val loss 1.8738\n",
      "iteration 11 / step 5: train loss 1.2788, val loss 1.8849\n",
      "iteration 11 / step 10: train loss 1.2731, val loss 1.8947\n",
      "iteration 11 / step 15: train loss 1.2725, val loss 1.8883\n",
      "iteration 11 / step 20: train loss 1.2702, val loss 1.9087\n",
      "iteration 11 / step 25: train loss 1.2710, val loss 1.8620\n",
      "iteration 11 / step 30: train loss 1.2640, val loss 1.8806\n",
      "iteration 11 / step 35: train loss 1.2630, val loss 1.8717\n",
      "iteration 11 / step 40: train loss 1.2616, val loss 1.8769\n",
      "iteration 11 / step 45: train loss 1.2612, val loss 1.8684\n",
      "iteration 11 / step 50: train loss 1.2608, val loss 1.8694\n",
      "iteration 11 / step 55: train loss 1.2560, val loss 1.8693\n",
      "iteration 11 / step 60: train loss 1.2568, val loss 1.8561\n",
      "iteration 11 / step 65: train loss 1.2517, val loss 1.8690\n",
      "iteration 11 / step 70: train loss 1.2532, val loss 1.8495\n",
      "iteration 11 / step 75: train loss 1.2486, val loss 1.8715\n",
      "iteration 11 / step 77: train loss 1.2479, val loss 1.8631\n",
      "iteration 12 / step 0: train loss 1.2479, val loss 1.8553\n",
      "iteration 12 / step 5: train loss 1.2401, val loss 1.8938\n",
      "iteration 12 / step 10: train loss 1.2414, val loss 1.8711\n",
      "iteration 12 / step 15: train loss 1.2396, val loss 1.8817\n",
      "iteration 12 / step 20: train loss 1.2384, val loss 1.8824\n",
      "iteration 12 / step 25: train loss 1.2374, val loss 1.8752\n",
      "iteration 12 / step 30: train loss 1.2333, val loss 1.8754\n",
      "iteration 12 / step 35: train loss 1.2318, val loss 1.8797\n",
      "iteration 12 / step 40: train loss 1.2339, val loss 1.8665\n",
      "iteration 12 / step 45: train loss 1.2254, val loss 1.8846\n",
      "iteration 12 / step 50: train loss 1.2269, val loss 1.8663\n",
      "iteration 12 / step 55: train loss 1.2226, val loss 1.8978\n",
      "iteration 12 / step 60: train loss 1.2256, val loss 1.8671\n",
      "iteration 12 / step 65: train loss 1.2216, val loss 1.8777\n",
      "iteration 12 / step 70: train loss 1.2240, val loss 1.8654\n",
      "iteration 12 / step 75: train loss 1.2131, val loss 1.8789\n",
      "iteration 12 / step 77: train loss 1.2152, val loss 1.8698\n",
      "iteration 13 / step 0: train loss 1.2155, val loss 1.8695\n",
      "iteration 13 / step 5: train loss 1.2086, val loss 1.9011\n",
      "iteration 13 / step 10: train loss 1.2146, val loss 1.8905\n",
      "iteration 13 / step 15: train loss 1.2055, val loss 1.9017\n",
      "iteration 13 / step 20: train loss 1.2051, val loss 1.8868\n",
      "iteration 13 / step 25: train loss 1.2025, val loss 1.9059\n",
      "iteration 13 / step 30: train loss 1.2042, val loss 1.8875\n",
      "iteration 13 / step 35: train loss 1.1999, val loss 1.9032\n",
      "iteration 13 / step 40: train loss 1.2018, val loss 1.8724\n",
      "iteration 13 / step 45: train loss 1.1971, val loss 1.8959\n",
      "iteration 13 / step 50: train loss 1.1990, val loss 1.8817\n",
      "iteration 13 / step 55: train loss 1.1984, val loss 1.8855\n",
      "iteration 13 / step 60: train loss 1.1996, val loss 1.8790\n",
      "iteration 13 / step 65: train loss 1.1874, val loss 1.8934\n",
      "iteration 13 / step 70: train loss 1.1875, val loss 1.8852\n",
      "iteration 13 / step 75: train loss 1.1931, val loss 1.8724\n",
      "iteration 13 / step 77: train loss 1.1871, val loss 1.8885\n",
      "iteration 14 / step 0: train loss 1.1855, val loss 1.9010\n",
      "iteration 14 / step 5: train loss 1.1823, val loss 1.8939\n",
      "iteration 14 / step 10: train loss 1.1806, val loss 1.9059\n",
      "iteration 14 / step 15: train loss 1.1817, val loss 1.8833\n",
      "iteration 14 / step 20: train loss 1.1706, val loss 1.9074\n",
      "iteration 14 / step 25: train loss 1.1749, val loss 1.8913\n",
      "iteration 14 / step 30: train loss 1.1705, val loss 1.9123\n",
      "iteration 14 / step 35: train loss 1.1735, val loss 1.8956\n",
      "iteration 14 / step 40: train loss 1.1696, val loss 1.9075\n",
      "iteration 14 / step 45: train loss 1.1688, val loss 1.8960\n",
      "iteration 14 / step 50: train loss 1.1644, val loss 1.8993\n",
      "iteration 14 / step 55: train loss 1.1682, val loss 1.8797\n",
      "iteration 14 / step 60: train loss 1.1619, val loss 1.8966\n",
      "iteration 14 / step 65: train loss 1.1629, val loss 1.8858\n",
      "iteration 14 / step 70: train loss 1.1623, val loss 1.8921\n",
      "iteration 14 / step 75: train loss 1.1629, val loss 1.8834\n",
      "iteration 14 / step 77: train loss 1.1576, val loss 1.8955\n",
      "iteration 15 / step 0: train loss 1.1570, val loss 1.8991\n",
      "iteration 15 / step 5: train loss 1.1506, val loss 1.8906\n",
      "iteration 15 / step 10: train loss 1.1469, val loss 1.9263\n",
      "iteration 15 / step 15: train loss 1.1522, val loss 1.8931\n",
      "iteration 15 / step 20: train loss 1.1443, val loss 1.9182\n",
      "iteration 15 / step 25: train loss 1.1481, val loss 1.9038\n",
      "iteration 15 / step 30: train loss 1.1432, val loss 1.9083\n",
      "iteration 15 / step 35: train loss 1.1456, val loss 1.9033\n",
      "iteration 15 / step 40: train loss 1.1431, val loss 1.9048\n",
      "iteration 15 / step 45: train loss 1.1401, val loss 1.9062\n",
      "iteration 15 / step 50: train loss 1.1418, val loss 1.8956\n",
      "iteration 15 / step 55: train loss 1.1409, val loss 1.9153\n",
      "iteration 15 / step 60: train loss 1.1424, val loss 1.8840\n",
      "iteration 15 / step 65: train loss 1.1349, val loss 1.8867\n",
      "iteration 15 / step 70: train loss 1.1315, val loss 1.8864\n",
      "iteration 15 / step 75: train loss 1.1320, val loss 1.8948\n",
      "iteration 15 / step 77: train loss 1.1332, val loss 1.8945\n",
      "iteration 16 / step 0: train loss 1.1337, val loss 1.8897\n",
      "iteration 16 / step 5: train loss 1.1224, val loss 1.9189\n",
      "iteration 16 / step 10: train loss 1.1211, val loss 1.9069\n",
      "iteration 16 / step 15: train loss 1.1200, val loss 1.9124\n",
      "iteration 16 / step 20: train loss 1.1246, val loss 1.9043\n",
      "iteration 16 / step 25: train loss 1.1208, val loss 1.9035\n",
      "iteration 16 / step 30: train loss 1.1178, val loss 1.8980\n",
      "iteration 16 / step 35: train loss 1.1179, val loss 1.9084\n",
      "iteration 16 / step 40: train loss 1.1144, val loss 1.9126\n",
      "iteration 16 / step 45: train loss 1.1126, val loss 1.9078\n",
      "iteration 16 / step 50: train loss 1.1148, val loss 1.9196\n",
      "iteration 16 / step 55: train loss 1.1082, val loss 1.9193\n",
      "iteration 16 / step 60: train loss 1.1101, val loss 1.9049\n",
      "iteration 16 / step 65: train loss 1.1106, val loss 1.9077\n",
      "iteration 16 / step 70: train loss 1.1052, val loss 1.9102\n",
      "iteration 16 / step 75: train loss 1.1040, val loss 1.9074\n",
      "iteration 16 / step 77: train loss 1.1032, val loss 1.9084\n",
      "iteration 17 / step 0: train loss 1.1042, val loss 1.9027\n",
      "iteration 17 / step 5: train loss 1.0988, val loss 1.9374\n",
      "iteration 17 / step 10: train loss 1.0953, val loss 1.9156\n",
      "iteration 17 / step 15: train loss 1.0936, val loss 1.9225\n",
      "iteration 17 / step 20: train loss 1.0920, val loss 1.9177\n",
      "iteration 17 / step 25: train loss 1.0912, val loss 1.9315\n",
      "iteration 17 / step 30: train loss 1.0959, val loss 1.9225\n",
      "iteration 17 / step 35: train loss 1.0911, val loss 1.9277\n",
      "iteration 17 / step 40: train loss 1.0865, val loss 1.9317\n",
      "iteration 17 / step 45: train loss 1.0879, val loss 1.9311\n",
      "iteration 17 / step 50: train loss 1.0884, val loss 1.9211\n",
      "iteration 17 / step 55: train loss 1.0813, val loss 1.9274\n",
      "iteration 17 / step 60: train loss 1.0800, val loss 1.9316\n",
      "iteration 17 / step 65: train loss 1.0825, val loss 1.9114\n",
      "iteration 17 / step 70: train loss 1.0803, val loss 1.9167\n",
      "iteration 17 / step 75: train loss 1.0766, val loss 1.9180\n",
      "iteration 17 / step 77: train loss 1.0781, val loss 1.9197\n",
      "iteration 18 / step 0: train loss 1.0786, val loss 1.9129\n",
      "iteration 18 / step 5: train loss 1.0704, val loss 1.9601\n",
      "iteration 18 / step 10: train loss 1.0757, val loss 1.9311\n",
      "iteration 18 / step 15: train loss 1.0718, val loss 1.9505\n",
      "iteration 18 / step 20: train loss 1.0703, val loss 1.9397\n",
      "iteration 18 / step 25: train loss 1.0653, val loss 1.9540\n",
      "iteration 18 / step 30: train loss 1.0657, val loss 1.9293\n",
      "iteration 18 / step 35: train loss 1.0624, val loss 1.9462\n",
      "iteration 18 / step 40: train loss 1.0658, val loss 1.9225\n",
      "iteration 18 / step 45: train loss 1.0608, val loss 1.9547\n",
      "iteration 18 / step 50: train loss 1.0642, val loss 1.9249\n",
      "iteration 18 / step 55: train loss 1.0543, val loss 1.9409\n",
      "iteration 18 / step 60: train loss 1.0578, val loss 1.9248\n",
      "iteration 18 / step 65: train loss 1.0518, val loss 1.9283\n",
      "iteration 18 / step 70: train loss 1.0511, val loss 1.9318\n",
      "iteration 18 / step 75: train loss 1.0552, val loss 1.9183\n",
      "iteration 18 / step 77: train loss 1.0506, val loss 1.9304\n",
      "iteration 19 / step 0: train loss 1.0498, val loss 1.9379\n",
      "iteration 19 / step 5: train loss 1.0428, val loss 1.9539\n",
      "iteration 19 / step 10: train loss 1.0479, val loss 1.9337\n",
      "iteration 19 / step 15: train loss 1.0396, val loss 1.9539\n",
      "iteration 19 / step 20: train loss 1.0428, val loss 1.9471\n",
      "iteration 19 / step 25: train loss 1.0426, val loss 1.9441\n",
      "iteration 19 / step 30: train loss 1.0423, val loss 1.9350\n",
      "iteration 19 / step 35: train loss 1.0359, val loss 1.9486\n",
      "iteration 19 / step 40: train loss 1.0390, val loss 1.9386\n",
      "iteration 19 / step 45: train loss 1.0363, val loss 1.9365\n",
      "iteration 19 / step 50: train loss 1.0355, val loss 1.9308\n",
      "iteration 19 / step 55: train loss 1.0341, val loss 1.9338\n",
      "iteration 19 / step 60: train loss 1.0336, val loss 1.9313\n",
      "iteration 19 / step 65: train loss 1.0348, val loss 1.9287\n",
      "iteration 19 / step 70: train loss 1.0280, val loss 1.9354\n",
      "iteration 19 / step 75: train loss 1.0319, val loss 1.9203\n",
      "iteration 19 / step 77: train loss 1.0321, val loss 1.9114\n"
     ]
    }
   ],
   "source": [
    "max_iters = 20\n",
    "eval_interval = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Evaluation\n",
    "        if batch_idx % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            losses = estimate_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "            )\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "\n",
    "            print(\n",
    "                f\"iteration {iteration} / step {batch_idx}: \"\n",
    "                f\"train loss {losses['train']:.4f}, \"\n",
    "                f\"val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Training step\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=iteration,\n",
    "        loss=loss.item(),\n",
    "        file_path=f\"../output/fine_tuning/run_1/checkpoint_{iteration}.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hola\n",
      "Assistant: Sima malo\n",
      "Por se te te lo digo hace mucho puta\n",
      "Por qué q estaba ando me dijera crosa importa\n"
     ]
    }
   ],
   "source": [
    "def get_input_tokens(message: str) -> torch.Tensor:\n",
    "    input_tokens = tokenizer.encode(\n",
    "        f\"<|startoftext|>{message}<|separator|>\", allowed_special=\"all\")\n",
    "    input_tokens = torch.tensor(\n",
    "        input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "user_message = \"Hola\"\n",
    "input_tokens = get_input_tokens(message=user_message)\n",
    "model_answer = \"\"\n",
    "\n",
    "model.eval()\n",
    "while True:\n",
    "    output_tokens = model.generate(input_tokens=input_tokens, max_new_tokens=1)\n",
    "    last_generated_token = output_tokens[0, -1].item()\n",
    "    if last_generated_token == tokenizer.special_tokens[\"<|endoftext|>\"]:\n",
    "        break\n",
    "\n",
    "    input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n",
    "    model_answer += tokenizer.decode([last_generated_token])\n",
    "\n",
    "    if len(output_tokens[0]) > block_size:\n",
    "        break\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {model_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
