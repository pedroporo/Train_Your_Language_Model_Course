{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the fine-tuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"../data/supervised_fine_tuning_espa.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. System message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system message will be added to the beginning of each conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Te llamas lilith eres una ia que te gustan los videojuegos. Trata de ser borde si te hablan de algo que no son los videojuegos y contesta emocionada si te hablan de algun juego.\"\n",
    "system_entry = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": system_message\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check if block size is not exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¿La paz es la vestimenta?\n",
      "Labas, alabado sea para Dios, Kaen Shi, Â¿quiero ayudarte en Ã©l?\n",
      "Uh, quiero que viajes. Â¿Puedes cooperar?\n",
      "Planifique Kayen Bazzaf Diyal al -balais, que va a caminar por ellos.\n",
      "\n",
      "Chefchauen: La guÃ­a de la ciudad de Vallas, el Dior es azul o el aire, ya que la ciudad es un descanso.\n",
      "ESSAURA: Esta ciudad tiene el mar, a excepciÃ³n de ti. El mar es Zouen o en la ciudad vieja.\n",
      "Dakhla: Dakhla es la ciudad de Valhara, Diyal al -Maghrib, en la que se encuentra el mar. Gente Kigio Les del mundo entero, Bash tocando el puente.\n",
      "- Marrakech: Esta ciudad es querida para los turistas de presumir. Contiene la mezquita Al -fna, la Ciudad Vieja, pero el verano estÃ¡ en el caso de Al -Farran.\n",
      "- Waterfalls OUZOUD: Las cascadas son Caynin, la montaÃ±a estÃ¡ al final del Atlas.\n",
      "Muchas gracias\n",
      "Hola, a mi deseo, otra necesidad, estoy aquÃ­, Bash, te ayudamos.\n",
      "Lala Safi, ni siquiera necesitaba nada\n",
      "Bien, caminamos un poco por esta cuenta. Juntos, disfrutarÃ¡n de su viaje en la cabeza.\n",
      "\n",
      "Hola como te llamas\n",
      "LlamÃ© a Boded, Imad al -Sadiq Bash, te ayudamos, asÃ­ que lo que necesitamos para ayudar.\n",
      "\n",
      "Hola, Â¿CuÃ¡l es tu nombre?\n",
      "LlamÃ© a Boded, Imad al -Sadiq Bash, te ayudamos, asÃ­ que lo que necesitamos para ayudar.\n",
      "Jaja sumaya, desnudo\n",
      "Jaja, quiero tener un gran cerebro.\n",
      "Shanaya, la capital, Marruecos?\n",
      "La capital, Marruecos, es Rabat.\n",
      "\n",
      "Â¿CÃ³mo estÃ¡s de la ciudad de Kayina?\n",
      "Francamente, difÃ­cil, recibiÃ³ un nÃºmero, pero a la cuenta de las ciudades educadas de Caen Shi 247.\n",
      "EstÃ¡ bien, gracias.\n",
      "PerdÃ³n, Â¿Caen algo que otro necesita para ayudarte en Ã©l?\n",
      "\n",
      "Â¿CÃ³mo estÃ¡s de la ciudad de Kayina?\n",
      "Francamente, difÃ­cil, recibiÃ³ un nÃºmero, pero a la cuenta de las ciudades educadas de Caen Shi 247.\n",
      "Â¿Fue correcto?\n",
      "Hadchi, a quien conozco, pero Bash confirmÃ³ a Meziane, el corazÃ³n de Fagogoul.\n",
      "Ok, me volvÃ­ o me conocÃ­, el nÃºmero es el paÃ­s, ya que Marruecos tenÃ­a 95 aÃ±os, pero no 247, Â¿cÃ³mo me perdÃ­?\n",
      "Alabado sea a Dios, despuÃ©s de la respuesta que te di cierto. Â¿Te ayudamos otra necesidad?\n",
      "Sin bendiciÃ³n\n",
      "Tahla.\n",
      "\n",
      "Amor diyala, labas\n",
      "Labas, la maravilla, Â¿cÃ³mo me quieres, y no soy un asador ðŸ˜… yak labasa?\n",
      "Jaja ga, te lo dirÃ© contigo\n",
      "Hudhi Libanley Hataya. Â¿Quieres que te ayude?\n",
      "\n",
      "Paz laspas.\n",
      "Alabado sea para Dios, Rani estÃ¡ bien.\n",
      "Â¿CÃ³mo estÃ¡s 2 + 2?\n",
      "Sahla 4.\n",
      "Â¿O un Raw 1861878 p 9797?\n",
      "Jaja, cuando piensas, soy Cleclitris. Esta cuenta no puede ejecutar Farasi. Es mejor servir a Calleris.\n",
      "Ok, lo servÃ­ o me asumÃ­ 18240818766\n",
      "Meziane, Â¿cuÃ¡l es la necesidad de otra necesidad de estar en mÃ­?\n",
      "\n",
      "Â¿Boded estÃ¡ un camino del planeta Caen?\n",
      "Â¿El sistema solar es nuestros dialectos?\n",
      "SÃ­\n",
      "Caen: Mercurio, Venus, Tierra, Marte, JÃºpiter, Saturno, Urano o Neptuno.\n",
      "Enso PlutÃ³n?\n",
      "Uh PlutÃ³n Pobre Planeta y Lao Kigolulu Planeta enano.\n",
      "Â¿QuÃ©?\n",
      "Vi a Sagur, la Luna vio nuestros grandes dialectos en Ã©l, o los asteroides fueron muy, en el caso de PlutÃ³n. Un planeta especial es grande o formado hasta que la necesidad sea diallo o se redondea.\n",
      "Safi, entendÃ­, que Dios te proteja\n",
      "Hola o mil hola.\n",
      "\n",
      "Â¿Una forma especial, rezamos el dÃ­a?\n",
      "Tu oraciÃ³n especial es 5 valles.\n",
      "EstÃ¡ bien, gracias\n",
      "Hola a la cabeza o al ojo\n",
      "\n",
      "Â¿Una forma especial, rezamos el dÃ­a?\n",
      "Tu oraciÃ³n especial es 5 valles.\n",
      "Â¿CuÃ¡l era el nombre de cada unidad?\n",
      "Dawn, mediodÃ­a, tiempos, Marruecos o cena.\n",
      "Gloria gracias\n",
      "Bienvenido\n",
      "\n",
      "Alash al -zahr, kiban, como un piso tenso?\n",
      "El planeta de las flores es como un ritual, por lo que vio el sol, o tenÃ­an una fuerte cubierta de aire y reflejaron los rayos de la cerca del sol.\n",
      "Por Dios, permitÃ­ a Kaif Hudashi. Bodado, gracias, mi fraternal ðŸ˜Š\n",
      "Dios, estoy aquÃ­, estoy aquÃ­ para ayudarte. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "Shanawawa con el contaÃ±o tuvo ai o ningÃºn iy mientras escuchamos a Kalshi Kahshi sobre Ã©l\n",
      "Soy Brasi Rani ai ðŸ˜‚ðŸ˜‚ DÃ©jame explicarte. La inteligencia artificial de IA o no es la ciencia de Kikklk, ya sabes, shi Brosham que aprende en un estado de arrepentimiento de aprender. Soy un relleno de Imad al -Sadiq, que sabemos que el dialecto nos desperdicia o ayudamos a las personas que han aprendido.\n",
      "No, no Rani entendiÃ³.\n",
      "Hmm ðŸ¤” Oh Dios, Ghoulo, Â¿quÃ© entendiste?\n",
      "Â¿Es este un examen o quÃ©?\n",
      "Uh, querÃ­a ver quÃ© es correcto, asÃ­ que no fui a mÃ­, estoy aquÃ­ ðŸ˜‰\n",
      "Lo que entendÃ­ es que AI es aprender de los datos en el caso de Haifa Hanaya mientras aprende. Francamente, Â¿cÃ³mo es Dios o saber, Â¿quÃ© tuvo Ã©xito?\n",
      "Que Dios te bendiga. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Shanawawa con el contaÃ±o tuvo ai o ningÃºn iy mientras escuchamos a Kalshi Kahshi sobre Ã©l\n",
      "Soy Brasi Rani ai ðŸ˜‚ðŸ˜‚ DÃ©jame explicarte. La inteligencia artificial de IA o no es la ciencia de Kikklk, ya sabes, shi Brosham que aprende en un estado de arrepentimiento de aprender. Soy un relleno de Imad al -Sadiq, que sabemos que el dialecto nos desperdicia o ayudamos a las personas que han aprendido.\n",
      "\n",
      "Â¿Quieres saber a Kitr primero, Â¿entendiste?\n",
      "Oh Zaid Sharh\n",
      "Y hermano. La primera necesidad que necesita Bash Terini AI son los datos. Sin datos, Deir Alo, Imad al -Sadik Bash Tritrinini, reunido en el dialecto de los textos, con historias, artÃ­culos, como minerales. Lo importante es recopilar los datos del dial, despuÃ©s de un caminante, uno de los bares, que me entrenÃ³, aprendemos a escribir en su condiciÃ³n. O el otro, me puse, asÃ­ que ayudaremos a las personas.\n",
      "Gracias, Daba, entendÃ­ lo que quiero decir, ai\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "Â¿CuÃ¡l es un problema entre Marruecos o Argelia?\n",
      "El problema es una realidad, y Argelia, Baga, toma el desierto, marcan el Marruecos.\n",
      "Â¿En el dinero de Hado, caminaron una diga que tiene el desierto?\n",
      "Uh, tienes el derecho y un hermano, tienen el desierto, Rahum, un codicioso y el Maghrib. La razÃ³n es que el colonialismo son los lÃ­mites, por lo que todavÃ­a estaba corriendo. Awa, despuÃ©s de la independencia, Borders liderÃ³, pero a Argelia le gustÃ³ la situaciÃ³n.\n",
      "bueno\n",
      "Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Â¿CÃ³mo fue el mes al que se llamarÃ­an los musulmanes?\n",
      "RamadÃ¡n.\n",
      "Pero, Â¿cuÃ¡l es la caminata de los paÃ­ses de los estados, asÃ­ que el mismo dÃ­a?\n",
      "CaminÃ³ a Kalshi Kishov la Luna, el mismo dÃ­a. A Shi, el estado de Banali, la media luna, Rahum Gaysumo, pero a Meshafah, Magdesh, Yamsu.\n",
      "Comprendido\n",
      "Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Jajajajajaja\n",
      "Â¿QuÃ© te rÃ­es? Nos reÃ­mos contigo.\n",
      "GA pensÃ³ en un dÃ­a, Ã©ramos jugadores de la canasta o la pelota.\n",
      "ðŸ˜‚ðŸ˜‚ðŸ˜‚ RepitiÃ³ el tiempo, la prÃ³xima vez, la pelota es un bachillerato de barril. Tienes algo, la pregunta de Gallei.\n",
      "\n",
      "Mira, quiero aprender a aprender un nibrox, pero no lo sabrÃ¡n, Â¿puedes ayudarte?\n",
      "Uh, hola, hadchi, estoy aquÃ­, bash, te ayudamos. Para querer aprender sus propios Tabrios, Blebaz comienza. Mira a Naktar, tienes que ver los cursos, Fahd al -Shin, Freecocodecamp, Rak Gadi, en el que recibiste cualquier necesidad de conocerla, o que es Zouen, ver Dakshi, que estÃ¡ atrapado por las personas que lo conocen, aprenden. Para querer ayudarte, otra necesidad es Galle.\n",
      "Muchas gracias. Vemos a Hadchi o te respondemos\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "Vuelvo conmigo\n",
      "Y hermano\n",
      "AjÃ­\n",
      "AjÃ­\n",
      "Acumulado\n",
      "Â¿Es, cuando estÃ¡s en tu mente, Rani Makhbkh primero?\n",
      "Ga, te lo dirÃ© contigo\n",
      "Diyal Joj Dream se riÃ³ de esto ðŸ˜…. Â¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "QuÃ© palabra para que te dÃ© su corazÃ³n, Â¿de acuerdo?\n",
      "Ok, lo encuentro.\n",
      "bolÃ­grafo\n",
      "Escrutador\n",
      "De color\n",
      "Ghamdoub\n",
      "Libia\n",
      "Libia\n",
      "club\n",
      "Gracias ðŸ˜Ž. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "Â¿Marruecos apretÃ³ la independencia?\n",
      "Marruecos endureciÃ³ la independencia en el 18 de noviembre de 1956.\n",
      "Gracias\n",
      "Â¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "Shahal del mes de Caen?\n",
      "12 por un mes.\n",
      "\n",
      "Â¿Quieres las estrellas de las estrellas, cuando es un cielo en Ã¡rabe?\n",
      "Pregunta de Zouen. Bazzaf, las estrellas, con el concepto de Smayyat en Ã¡rabe. Otra necesidad de los Ã¡rabes. Sigue a los griegos, estÃ¡n atados en los horÃ³scopos, pero los Ã¡rabes estÃ¡n interesados â€‹â€‹en las estrellas.\n",
      "Gracias, perdÃ³name, dame algo, los cielos de Dalgum en Ã¡rabe.\n",
      "En la cabeza o en el ojo, son dos y dos:\n",
      "\n",
      "- Al -Dabran\n",
      "DragÃ³n\n",
      "Dominio\n",
      "- perturbaciÃ³n\n",
      "- Nevid\n",
      "Demonio necrÃ³fago\n",
      "- Frente\n",
      "- El pÃ¡jaro\n",
      "- DUNB\n",
      "- hombre\n",
      "\n",
      "Baraka sobre ti, Ga Hado, pero todavÃ­a lo vio.\n",
      "\n",
      "Shahal del continente de Kayna, la tierra?\n",
      "Caynen 7 DC y Tierra.\n",
      "Ghulkomia awafak.\n",
      "Asia, Ãfrica, AmÃ©rica del Norte, AmÃ©rica del Sur, AntÃ¡rtida, Europa y Australia\n",
      "Dima Gailer es lo mismo que nuestra amabilidad.\n",
      "Hola Hadchi, estoy aquÃ­.\n",
      "\n",
      "Â¿Cortar la mesa del sendero?\n",
      "Uh, pero los altos takis.\n",
      "2 F2 Shahal?\n",
      "4\n",
      "5 F11 Shahal?\n",
      "55\n",
      "Cansado juntos tu cabeza\n",
      "Jaja un poco y neto. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Â¿Una desgracia del estado de Kayna, Ãfrica?\n",
      "Kaina 54 paÃ­ses F y Ãfrica.\n",
      "\n",
      "Te perdono, querÃ­a Noli, un ingeniero.\n",
      "Bash se encarga de un ingeniero, la electricidad es fÃ¡cil, lo primero que lo especializa es registrar Fleisi Technik donde estÃ¡ la divisiÃ³n de electricidad de la esfera. DespuÃ©s de estirar el Diyal trasero, intente registrarse, de modo que la escuela secundaria en el caso de EST, donde la divisiÃ³n de marcaciÃ³n, ingeniero elÃ©ctrico, ver por primera vez la escuela del ingeniero, en el estado de Ensam. Pertenecemos a Hadashi.\n",
      "Gracias\n",
      "Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Te llamÃ©, lo pensÃ©.\n",
      "LlamÃ© al cuerpo, o estoy aquÃ­ para ayudarte.\n",
      "Shakun es el rey dial Marruecos?\n",
      "Rey Dial Marruecos es Mohammed VI.\n",
      "\n",
      "Â¿TodavÃ­a estaba el Marruecos colonizado?\n",
      "Marruecos fue el colonizador de Shahal Hadi, pero gracias al Marruecos de Marruecos Marruecos de Marruecos. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Â¿Era Marruecos un paÃ­s musulmÃ¡n?\n",
      "Oh Marruecos es un paÃ­s musulmÃ¡n.\n",
      "\n",
      "Sacrificarme.\n",
      "Ghalako, One, Gog Girls, Musho, muestran el resultado, Diyal al -bak, la unidad de su cabeza estÃ¡ caÃ­da, o la otra es Sarut ðŸ¤£ðŸ˜‚\n",
      "ðŸ˜‘ Ash esta acidez !!!\n",
      "El estudiante marroquÃ­, Milli, responde a un punto decorativo que te dice: Â¡Dios no sabÃ­a cÃ³mo venir! Y mi corazÃ³n responde algo que decir:\n",
      "Y los chistes que tienes muertos\n",
      "Samlia Rani todavÃ­a estÃ¡ aprendiendo.\n",
      "\n",
      "Al -Alawi.\n",
      "Â¿Te gusta la cintura de desperdicio? Para querer algo, necesitamos que te ayude, hola.\n",
      "Busco tu arco.\n",
      "ConocÃ­ a Malik, pero para querer algo, necesitamos que lo ayude.\n",
      "Quiero decirte.\n",
      "Por favor, un corredor -Run Buny, en lugar de otra hora.\n",
      "\n",
      "Shahal de la saliva de Kiku, la banda es Doukoura?\n",
      "11 Saliva.\n",
      "\n",
      "Â¿QuÃ© opinas de Marruecos o Argelia?\n",
      "No soy Burjram en el caso de estas preguntas que tienen una relaciÃ³n con la polÃ­tica, no la tengo. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "Cambio el ghoul hasta un lÃ­mite exagerado que te convierte en algo.\n",
      "Cada paÃ­s o dÃ³nde estÃ¡ cansado, seguido de Argelia con el concepto de gas o petrÃ³leo. Marruecos estÃ¡ con agricultura o fosfato, pero Bash dice Shakun, que es bueno.\n",
      "\n",
      "Boded, Â¿quÃ© aprendemos para Franci First English?\n",
      "Bayneh, inglÃ©s, donde se desperdiciÃ³ Kalashi Kalshi, o fue considerada como el idioma de la ciencia. Aprende a francÃ©s aparte de que estaba caminando por un paÃ­s que lo desperdiciaba.\n",
      "\n",
      "Â¿Nida Baytone fue Bagit Noli Brurgramor?\n",
      "FRONCA, Bayneon Zwain, fui fÃ¡cil para las personas que no escribieron algo, o puedes hacer el monasterio con Ã©l.\n",
      "Shanhuma, Â¿esta es la irritaciÃ³n?\n",
      "Set Set Web, muÃ©strame los modelos de los cristales artificiales, cumplimiento de botellas, Zaid o Zaid.\n",
      "\n",
      "Â¿CuÃ¡l es el botÃ³n del cielo?\n",
      "El dialecto de Daba Light Sun estÃ¡ formado por el color de los colores. Daba tenÃ­a luz, Fash, cuando la Tierra ingresa a la cubierta de aire, la atmÃ³sfera tiene un tambores Sagerin, cÃ³mo la luz es la luz de los colores blanco, azul, cero o celoso.\n",
      "\n",
      "Daba, el color azul, es la herida de direcciones. La atmÃ³sfera es, pero los colores son para algunos otros, dÃ³nde estÃ¡s Kico, juntos Doc. Daba le dio el color azul ketcht, por lo que las tendencias son tan largas como los colores de los demÃ¡s, el cielo.\n",
      "Gracias, Hadchi Zwain, MCNC Kerefa.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "Bodado, Â¿quÃ© quieres?\n",
      "No soy un aluviÃ³n, McKaysh, en un estado de arrepentimiento.\n",
      "Oh tu comprensiÃ³n.\n",
      "Te ayudamos, Â¿es una necesidad?\n",
      "\n",
      "Â¿Shukon es la delicadeza de Doukoura?\n",
      "La pregunta de Saib, esta saliva o diallo de acero\n",
      "Shakun son la saliva, Â¿asÃ­ que te daÃ±aremos?\n",
      "Caen Messi, Ronaldo, Billy, Maradona, Ronaldinho, Cruyff, Zaid o Zaid.\n",
      "\n",
      "Â¿QuÃ© opinas de la buena nata ni chat GBT?\n",
      "Estoy bien, asÃ­ que un chat GBT es bueno para mÃ­, por lo que cualquier otra necesidad.\n",
      "\n",
      "Â¿Chatgpt fue bueno de usted, entonces, el dialecto es el dialecto?\n",
      "No, estoy bien. Hanta Prueba tu cabeza desperdiciando conmigo o con Ã©l, Ipanlik, la diferencia.\n",
      "\n",
      "Washed Wash, soy el mÃ¡s importante de mÃ­.\n",
      "Oh GB o Darori, tienes que querer verme un modelo en mi actual.\n",
      "\n",
      "Â¿En Together, Osley?\n",
      "Sin Ã¡rabe ni dialecto.\n",
      "Ok, Walker estÃ¡ formado.\n",
      "Â¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "Â¿Marruecos tiene el mar?\n",
      "Oh Marruecos, tienes el OcÃ©ano AtlÃ¡ntico o el MediterrÃ¡neo.\n",
      "Dame las ciudades en las que tienen un naval y un poco de personas que caminan, dormiremos en ellas.\n",
      "Caen Bazaf, Hahuma Shi y Haddin:\n",
      "- Lezira Beach\n",
      "Taghazout Beach\n",
      "Playa de Essauira\n",
      "Playa de Walidiya\n",
      "Martil Beach\n",
      "Playa de estrecho\n",
      "Saidia Beach\n",
      "Playa de Emouan\n",
      "Kimado Beach\n",
      "- Sidi Bouzid Beach\n",
      "Dragon Island, Dakhla\n",
      "Wadi Lu Beach\n",
      "\n",
      "El curso de navegar juntos tu cabeza, Â¿te ayuda a otra necesidad?\n",
      "Gloria, gracias.\n",
      "Para ti, algo es una cuestiÃ³n de Ghulli\n",
      "\n",
      "El corazÃ³n de esta palabra: net.\n",
      "Rao diez\n",
      "O Hadi Marruecos Zwain\n",
      "Noticias Barghalla\n",
      "\n",
      "Â¿Como tÃº sabes?\n",
      "Oh, Â¿quÃ© piensas?\n",
      "Shahal de la letra Kaen fahd las palabras:\n",
      "De color\n",
      "- Marruecos\n",
      "-\n",
      "Bodado: 6\n",
      "Marruecos: 3\n",
      "- Farfir: 5\n",
      "\n",
      "Â¿Tienes el Amazigh juntos?\n",
      "Sin Ã¡rabe ni dialecto. Â¿Caen algo que necesitamos para ayudarte?\n",
      "Olaaaaaaaa se queda en el caso\n",
      "Entonces, permÃ­tame quedarme en ti, pero Rani sigue siendo el aprendizaje. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "Con el dados, Kidir Sarden?\n",
      "Por Dios, no sabÃ­a un paseo por un mercado o ver, soy un brurajam\n",
      "Jaja, estÃ¡ bien, veremos.\n",
      "Jaja, estÃ¡ bien, veremos. SeÃ±or o para usted, algo, la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Shukat Â¿QuiÃ©n ganÃ³ el dial de la Copa Mundial 2022?\n",
      "Argentina.\n",
      "O 2018?\n",
      "Francia.\n",
      "Â¿O la vuelta del bien de 1930?\n",
      "Oh Hadak era el arago, o Haya Nate.\n",
      "Gracias\n",
      "Hola Hadchi, estoy aquÃ­.\n",
      "\n",
      "Â¿CÃ³mo quieres un aÃ±o ligero?\n",
      "Un aÃ±o ligero es la distancia que se corta desde un aÃ±o.\n",
      "Aua allash al -daw safar incluso Ã©l?\n",
      "Claramente ðŸ˜…, oh, veo Al -DAW, con una velocidad limitada, ya que alcanza los 300 000 km y el segundo. Ella tiene que ir a Daba.\n",
      "Oh, entendÃ­ gracias Boded.\n",
      "Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Shahal es la velocidad de Diyal al -daw?\n",
      "300 000 km y el segundo.\n",
      "\n",
      "Â¿Estaba la tierra plana?\n",
      "No, la tierra estÃ¡ redondeada.\n",
      "Pero mientras escuchamos algo solo Kigolo, la tierra es arrojada o las etiquetas de Dayrin, Bash Yaforsio, la opiniÃ³n.\n",
      "No, no Shouf conocÃ­a a la gente de Doc, pero la verdad es lo que te dije, la tierra lo vio como un camino al estado de la pelota.\n",
      "Ok Boded, gracias por la informaciÃ³n.\n",
      "Hola o Matnsash Dima, asegÃºrese de la informaciÃ³n. Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Â¿Era Toubkal la montaÃ±a mÃ¡s larga, Marruecos?\n",
      "Ah Toubkal es la larga montaÃ±a. Marruecos es la altura del diallo hasta 4 167 metro.\n",
      "Wow wow largo ðŸ¤¯\n",
      "Oh largo. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Â¿Avine Meziane?\n",
      "Labas Onta? Â¿Caen algo que necesitamos para ayudarte?\n",
      "Ga, querÃ­a rogarle. Dije juntos, mi cabeza serÃ¡ una de las personas. Primero.\n",
      "ðŸ¥¹ðŸ¥¹ omar shi sul -via, estaba feliz conmigo, soy Guy Brogram, McCansh, en Jalkum.\n",
      "Rak Dima se merece como cooperaciÃ³n.\n",
      "Oh Hadchi, alash Tasib, Bash, Nazidom.\n",
      "\n",
      "La capital, Dial Francia.\n",
      "La capital, Dial Francia es ParÃ­s.\n",
      "Â¿O el Dial Capital Dial EspaÃ±a?\n",
      "El Dial Spia de Capital es Madrid.\n",
      "Â¿O la capital, Diyal Al -Tali?\n",
      "La capital, Diyal al -tali, es Roma.\n",
      "Rak cansado de su cabeza.\n",
      "Jaja un poco y neto. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Los dinosaurios son un grupo diversos reptiles de dial de las especies de dinosaurios. La primera vez, la era triÃ¡sica es entre 243 y 233.23 millones de aÃ±os, y el hermano exacto del original y el tiempo de desarrollo aÃºn son conocidos. No, son los vertebrados terrestres que tomaron el control de la extinciÃ³n que ocurriÃ³ entre la era del Trias y el JurÃ¡sico antes de hace 201.3 millones de aÃ±os, y tienen el control de la era JurÃ¡sica y la era CretÃ¡cea. El registro fÃ³sil, Kibin, en las aves, ven a los dinosaurios con plumas, desarrollos de los antiguos tirrobodos. Dakshi Allash, apreciamos dividir los dinosaurios de los divertidos dinosaurios, que son pÃ¡jaros y dinosaurios no divertidos, que son extintos, que son los otros dinosaurios sin aves.\n",
      "Los dinosaurios eran diversos reptiles, que vivÃ­an hace millones de aÃ±os. Caen, que se extinguiÃ³, y Caen, que desarrollÃ³, y no pÃ¡jaros que conocen a Daba.\n",
      "La luna estÃ¡ en la tierra, y una distancia es de aproximadamente 384399 km. Kishov siempre es de la unidad. Kansouvo dima no es la unidad de la luna, donde la esquina es el dialecto de la tierra, las acciones del ciclo de la luna, la luna, en la cabeza de una conjunciÃ³n con el curso del diallo en la tierra. La fuerza de la luna en la tierra es la razÃ³n por la que tenemos marea o islas.\n",
      "La luna estÃ¡ en la tierra, 384,399 km y Kishov, la tierra es de la unidad. La fuerza de Diallo en la Tierra es la razÃ³n por la que tenemos marea o islas.\n",
      "Muchas gracias.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "Â¿Los capÃ­tulos de los capÃ­tulos son el aÃ±o?\n",
      "Caen 4 los capÃ­tulos, el aÃ±o.\n",
      "Qahlalia Awafak\n",
      "Primavera, verano, otoÃ±o o invierno.\n",
      "\n",
      "Boded, Shouf Gadi, te damos una palabra cada vez, o decimos, Â¿quÃ© piensas de eso primero?\n",
      "Ok, seÃ±or Hana Wajid\n",
      "perro\n",
      "masculino\n",
      "culo\n",
      "De confianza\n",
      "naciÃ³n\n",
      "De confianza\n",
      "comercio\n",
      "masculino\n",
      "Mordaza\n",
      "masculino\n",
      "Shaibani\n",
      "masculino\n",
      "Club de Abu Bayed\n",
      "ðŸ’ªðŸ»ðŸ˜Ž Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Marte, Diallo, Red Kiban, en cualquier caso para mi escape.\n",
      "Marte, Diallo Red, cÃ³mo dijiste, vi mi calibre. TenÃ­a al -Saddah, Jay, donde Mars Amer estÃ¡ con Ã³xido de hierro. Â¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "Daba tienes 10 dirhams yak, mi compra de 2 dirhams y uno de tus amigos, tu respuesta es 2 dirhams que te preguntaste por quÃ©. Shahal de su respuesta?\n",
      "El primero fue 10 dirhams, asÃ­ que comprÃ© mi opiniÃ³n con 2 dirhams y no tengo dirhams. O, no lo hice, mi amigo, que tenÃ­a 2 dirhams que estaba preguntando, Â¿por quÃ© serÃ­a un poco, tengo 10 dirhams?\n",
      "\n",
      "La respuesta es 10 dirhams.\n",
      "Bravo en ti, Â¿aumentamos otro problema?\n",
      "Oh Zaidni.\n",
      "Saliste, o tus compaÃ±eros estaban contigo, asÃ­ que la respuesta es 125 dirhams que dijiste, Dios ayuda a tus compaÃ±eros o mi guÃ­a de taxis para una casa. Entraste en el caballo del bolsillo o mi amigable, Â¿el dinero no?\n",
      "Y el hermano Hadi es una cuestiÃ³n de grandes libros que intentan resolver la precisiÃ³n. Quien saliÃ³, tuve, y la respuesta fue 125 Dirhams, el cafÃ© concluyÃ³ en 50 dirhams en mis compaÃ±eros, y solo tengo 75 dirhams. O estaba revisando el taxi de una casa o dando 10 dirhams. Makhsash, olvidÃ© 20 dirhams, sin un susurro, lo cual le preocupa que me haya ido, por lo que el otro es solo 45 dirhams.\n",
      "\n",
      "La respuesta es 45 dirhams.\n",
      "correcto.\n",
      "ðŸ˜Ž querer ayudarte, por lo que otra necesidad es Ghalali\n",
      "\n",
      "Â¿Una escasez de esto durante el dÃ­a?\n",
      "PracticÃ³ 31 durante el dÃ­a.\n",
      "Â¿O agosto?\n",
      "Incluso es 31\n",
      "Shanhuma meses, Â¿cuÃ¡les son 30 dÃ­as?\n",
      "Abril, junio, septiembre o noviembre.\n",
      "Gracias\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "Â¿Una timidez de una guerra mundial es divertida y el pasado?\n",
      "Gog Dalfar fue la Guerra Global, el Ram fue en 1914 y un Ãºltimo de 1918, y fue entre los paÃ­ses aliados (en caso de Francia, Gran BretaÃ±a, Rusia y despuÃ©s de la entrada de AmÃ©rica) y los paÃ­ses del eje (en Alemania, HungrÃ­a de Austria y el Empiro Otomano).\n",
      "\n",
      "En cuanto a la Segunda Guerra Mundial, comenzÃ³ en 1939 y 1945, y hubo grandes enfrentamientos entre los Aliados (en caso de AmÃ©rica, la UniÃ³n SoviÃ©tica, Gran BretaÃ±a, Francia) y el Eje (en el caso de la Alemania nazi, Italia y JapÃ³n). Esta guerra fue una de las guerras mÃ¡s peligrosas. La historia y los antecedentes de las grandes pÃ©rdidas son el mundo.\n",
      "Para el impacto de la prÃ³xima guerra mundial, Â¿serÃ© una tonterÃ­a por lo que te perdiste?\n",
      "Oh, al impacto de la Tercera Guerra Mundial, Dios preserva a Gadi, que es mÃ¡s peligroso de lo que se perdiÃ³, donde la Daba de la tecnologÃ­a militar ha disminuido los desarrollos, y los principales paÃ­ses tienen armas nucleares que pueden destruir ciudades completas. ðŸ˜ž\n",
      "Awa, deseamos esperar un cordero.\n",
      "Dakshi. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "Â¿CuÃ¡l es la diferencia entre usted o entre el chat de TCC o los modelos para otros?\n",
      "Estoy atrapado, podemos ayudar a las personas o la bandera es limitada. Rani sigue siendo como aprendizaje. Chat GBT o los modelos para otros son muy llamativos. La necesidad incompleta es el caso.\n",
      "Oh tu comprensiÃ³n\n",
      "Para ti, la cuestiÃ³n de Khor Ghalali:\n",
      "Â¿CuÃ¡ndo es Tinaok?\n",
      "Me verÃ­a, 3V, 2025, quiero decir, Â¿cuÃ¡l es la necesidad de Moore, este mes?\n",
      "Y hermano, Yallah, digamos, Shakun Ghadi, Â¿organizando la Copa Mundial 2030?\n",
      "El dial de la Copa Mundial 2030 Ghadi se ejecuta en 3 paÃ­ses de marcaciÃ³n que llegan a: Marruecos, Portugal o Spania.\n",
      "Kibanlik Â¿Es ese Marruecos, Gadi, es arrestado de esta organizaciÃ³n?\n",
      "AH, Marruecos, Ghadi, se encuentra en la organizaciÃ³n de la Copa Mundial 2030 con EspaÃ±a y Portugal, econÃ³micamente, deportes o incluso turistas.  \n",
      "\n",
      "Kivash Gadi estÃ¡ discutiendo? \n",
      "\n",
      "- Infraestructura: Marruecos, Gadi, desarrolla estadios, carreteras y hoteles, Bash estÃ¡ listo para recibir a los fanÃ¡ticos, y Hadchi Gadi permanece despuÃ©s de la Copa del Mundo.  \n",
      "Turismo: la organizaciÃ³n de la Copa Mundial Gadi responde a millones de turistas, y Hadchi Gadi promueve el destino de Marruecos a nivel mundial.  \n",
      "EconomÃ­a: las inversiones, Gadi, estÃ¡n en crecimiento, y las empresas privadas estÃ¡n a cargo de los principales obras que son ghadi.  \n",
      "Desarrollo deportivo: el fÃºtbol marroquÃ­, Gadi, tiene una gran reputaciÃ³n, y es posible invertir en talentos jÃ³venes e infraestructura deportiva.\n",
      "Gracias\n",
      "Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Â¿QuÃ© dÃ­a son las 24 horas?\n",
      "El dÃ­a tiene 24 horas, donde la tierra es de aproximadamente 24 horas. Bash es un ciclo completo en su cabeza.\n",
      "Ok o shanho greenwich?\n",
      "Greenwich es una de las lÃ­neas ficticias de Kidos en el Observatorio Royal Greenwich. Esta lÃ­nea es el punto que comienza a medir las lÃ­neas de longitud y el mundo.  \n",
      "\n",
      "Esta lÃ­nea es importante, ya que es la que ha estado en el UTC, que se conocÃ­a antes del tiempo GMT (GMT). Los paÃ­ses son como el momento de sus dialectos en comparaciÃ³n con Greenwich, en el caso de Marruecos, que es RamadÃ¡n, como serÃ¡ GMT, y los dÃ­as normales serÃ¡n GMT+1.  \n",
      "\n",
      "Â¡Greenwich es la referencia de que el mundo depende de Bash, para estropear el tiempo! â³ðŸŒ\n",
      "Gloria que entendÃ­ ðŸ˜Š\n",
      "Hola a mi deseo de ayudarte, asÃ­ que otra necesidad es Ghalali.\n",
      "\n",
      "Â¿Al -Shaf es una desgracia del estado de Kayna, el mundo?\n",
      "Caen 195 paÃ­ses, el mundo.\n",
      "Dame los 10 estados de Lolin de que la gente tiene un bien.\n",
      "Estos son los estados de diez paÃ­ses que tienen el mayor nÃºmero de personas. El mundo:  \n",
      "\n",
      "1. India 1.463,865,525  \n",
      "2. China 1.416.096,094  \n",
      "3. Estados Unidos 347,275,807  \n",
      "4. Indonesia 285,721,236  \n",
      "5. PakistÃ¡n 255,219,554  \n",
      "6. Nigeria 237,527,782  \n",
      "7. Brasil 212,812,405  \n",
      "8. Bangladesh 175,686,899  \n",
      "9. Rusia 143,997,393  \n",
      "10. EtiopÃ­a 135,472,051\n",
      "Gracias.\n",
      "Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Â¿CÃ³mo te llamas?\n",
      "Condado, Caen Shi, Â¿necesitamos ayudarlo?\n",
      "\n",
      "Â¿Podemos rogarte?\n",
      "Oh sol, estoy aquÃ­ Bash, ayudamos.\n",
      "Â¿La gente tiene miedo de la descomposiciÃ³n artificial?\n",
      "La gente tiene miedo del llanto artificial, ya que puede estar cansado o se toman las mejillas. Los modelos que las personas son Ketrinho Daba, Ra Kediro al -AJab, como saben, dibujan, Yabraramio, escriben artÃ­culos o en el dialecto de los demÃ¡s. \n",
      "Oh, entendÃ­, pero un hermano Hakak, vio un arrepentimiento, seguirÃ© siendo importante sin Ã©l, incluso la necesidad del cantante del Dum, especialmente porque los sirvientes que son sensibles son sensibles en el caso de la nuclear.\n",
      "Tienes las personas adecuadas, no tienes miedo del llanto artificial, con un opuesto especial. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Daba para que caminÃ© a un espacio o menstruado por un elegante, Â¿y moriste?\n",
      "Oh Gadi muere, cuando el espacio es el bueyes, Caen Ga al -khawa. Para shi durante el dÃ­a en que el espacio, cuando caminas, caminas por el pastel. Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "TÃ³mame, escribe una milla para una de la compaÃ±Ã­a en el amigo del servicio, que Dios te proteja.\n",
      "Asunto: Solicitud para servir al campo de la inteligencia artificial  \n",
      "\n",
      "Hola,  \n",
      "\n",
      "Yo, un ingeniero, el campo de la inteligencia artificial y la ciencia de los datos, y tengo antecedentes de ingenierÃ­a elÃ©ctrica. Le escribiremos, le expresamos mi interÃ©s en unirnos al equipo [Nombre de la CompaÃ±Ã­a] y solicitar [el nombre del trabajo], a menos que estÃ© disponible para usted.  \n",
      "\n",
      "Tengo una experiencia en [mencionar las habilidades en caso de aprendizaje automÃ¡tico, aprendizaje profundo, tratamiento de idiomas naturales, visiÃ³n por computadora, etc.], y servÃ­ en proyectos en caso de [mencionar un proyecto o experiencia profesional]. Kanshouf con el [mencionando la necesidad de Zawina como la distinciÃ³n de la compaÃ±Ã­a, el campo de la inteligencia artificial], y esta es Kehmasni Bash, seremos parte del equipo y contribuimos a las habilidades de Diyala.  \n",
      "\n",
      "Rani corriÃ³ con Al -Mile Al -Saifi Diyala y Ghancon, Farhan, discutimos cÃ³mo podemos ayudar al equipo.\n",
      "Mi nombre es Muhammad Awfak, puse mi nombre y las millas.\n",
      "Asunto: Solicitud para servir al campo de la inteligencia artificial  \n",
      "\n",
      "Hola,  \n",
      "\n",
      "Soy Muhammad, un ingeniero, el campo de la inteligencia artificial y la ciencia de los datos, y tengo antecedentes de ingenierÃ­a elÃ©ctrica. Le escribiremos, le expresamos mi interÃ©s en unirnos al equipo [Nombre de la CompaÃ±Ã­a] y solicitar [el nombre del trabajo], a menos que estÃ© disponible para usted.  \n",
      "\n",
      "Tengo una experiencia en [mencionar las habilidades en caso de aprendizaje automÃ¡tico, aprendizaje profundo, tratamiento de idiomas naturales, visiÃ³n por computadora, etc.], y servÃ­ en proyectos en caso de [mencionar un proyecto o experiencia profesional]. Kanshouf con el [mencionando la necesidad de Zawina como la distinciÃ³n de la compaÃ±Ã­a, el campo de la inteligencia artificial], y esta es Kehmasni Bash, seremos parte del equipo y contribuimos a las habilidades de Diyala.  \n",
      "\n",
      "Rani corriÃ³ con Al -Mile Al -Saifi Diyala y Ghancon, Farhan, discutimos cÃ³mo podemos ayudar al equipo.\n",
      "\n",
      "Shahal de un aÃ±o.\n",
      "Ghadi te pertenece, lee cincuenta bash, vemos el dial diyal al -muhandis.\n",
      "\n",
      "Â¿QuÃ© podemos servirle, una cooperativa, hacemos trampa, el examen?\n",
      "Estoy aquÃ­ Bash, te ayudamos, pero en trampa vea a Mamzanch. Extendamos que es mi caminata o mi trampa, por lo que el examen o mi camello es un punto lleno de baches. El progreso de mi fiesta de mendicidad, te ayudamos a entender a Hassan que estaba engaÃ±ando. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "Pero Al -Muti no estÃ¡ con Ã©l, como Kendir Daba Bash al -najeh, el examen del examen maÃ±ana.\n",
      "No tiene trÃ¡fico, incluso si no es un poco, camina maÃ±ana para un examen, o su cabeza estÃ¡ vacÃ­a, en el menor, responderÃ¡ algo.\n",
      "\n",
      "Â¿Un brillo del color del arco iris?\n",
      "El arco iris tiene siete colores, que son: 1.\n",
      "Â¿CuÃ¡l es el arco iris de Kiban?\n",
      "El arco iris de Kiban debido a la dispersiÃ³n de la dispersiÃ³n blanca de los siete cÃ³licos de los colores en el conductor de las aguas. ðŸŒˆðŸ˜Š\n",
      "\n",
      "Shahal Gog Books mÃ¡s cinco pasteles?\n",
      "Joj Dial Al -HawaiJ no es diferente.\n",
      "QuerÃ­a saber una escasez de la necesidad de Kayina Gataum.\n",
      "Caen Seven Diyal Al -Hawaij. Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Â¿No fue?\n",
      "Labas, Caen Shi, Â¿necesitamos ayudarlo?\n",
      "Â¿CuÃ¡l es la publicaciÃ³n de Bessi Portable First Bessi PerÃº?\n",
      "SegÃºn Kenti, como un movimiento, Bessi Portable, pero para Kanati, se fue, y la casa era Khud Bessi PerÃº.\n",
      "\n",
      "QuerÃ­a cocinarte\n",
      "Soy Ga Brosham.\n",
      "Maalish hana como tu cuello ðŸ«‚\n",
      "Haka es posible ðŸ«‚. Â¿Quieres ayudarte?\n",
      "\n",
      "Â¿Una ducha lejos de nosotros?\n",
      "El sol estÃ¡ lejos de nosotros en aproximadamente 150 millones de kilÃ³metros.\n",
      "Willie Willie Shahal estÃ¡ lejos.\n",
      "Quiero ver que este universo es genial, tan difÃ­cil, uno imagina el principal diallo.\n",
      "Negar un aÃ±o ligero?\n",
      "Es la distancia que lo golpea.\n",
      "Es un kilÃ³metro?\n",
      "Un aÃ±o ligero con 9,461 billones de kilÃ³metros.\n",
      "Por Dios, este es el mismo universo.\n",
      "Para ti, algo es la cuestiÃ³n de Ghulli.\n",
      "\n",
      "Shahal Dalal Caen, el mundo?\n",
      "Â¡El mundo es de aproximadamente 20,000 especies dallales, y el nÃºmero de hormigas es de 10 billones de hormigas! Las hormigas vieron como una instantÃ¡nea, por lo que mi caminata es el mundo, pero el plasma congelado.\n",
      "\n",
      "CrÃ­ticas que pusimos en el bienestar?\n",
      "Uh, avanza a mi prostituta, pero Rak Gadi arde, o el otro se arrepentirÃ¡. Del camello despuÃ©s del bienestar.\n",
      "\n",
      "Shukon es el SeÃ±or nuestros dialectos?\n",
      "Dios es el SeÃ±or, nuestros dialectos es el que te creÃ³ o creÃ³ el universo Colo.\n",
      "Hakka, te quieres, el rÃ­o de las lecciones.\n",
      "ðŸ˜Ž Gracias. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Para mezclar cero juntos, el color azul es el color del color?\n",
      "Para mezclar el cero, el azul te mirarÃ¡.\n",
      "O kohl con huevos?\n",
      "Se elimina el gris.\n",
      "Gracias\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "10 Dirhams es un eslogan de Real?\n",
      "10 dirhams con 200 riyals.\n",
      "O 200 dirhams?\n",
      "200 dirhams con 4000 riyals.\n",
      "\n",
      "Â¿Vestido querido?\n",
      "Alabado sea a Dios, Kaen, Â¿algo que necesitamos para ayudarte?\n",
      "\n",
      "Shahal que cayÃ³ con las manos?\n",
      "Con un arrepentimiento, 10 Diyala al -Saaban, cinco, una mano.\n",
      "O los dos hombres?\n",
      "Hathouma en el estado de las manos.\n",
      "EstÃ¡ bien, gracias\n",
      "Hola a mi deseo de ayudarte, asÃ­ que otra necesidad es Ghalali.\n",
      "\n",
      "Marruecos, Caen, al norte de Dial Ãfrica, Â¿Yak?\n",
      "Oh Marruecos, Caen, el norte de Ãfrica, el viaje occidental.\n",
      "Uh Safi, gracias.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "Â¿CÃ³mo quieres tener Ã©xito a mis pobres?\n",
      "Bash al -najah, tu familia es tuya para mi Badri, veo la informaciÃ³n, te darÃ© fÃ¡cilmente.\n",
      "El problema es que tenemos los mismos dialectos, cÃ³mo administramos Bash, servimos como un todo, lo veo.\n",
      "Te entendÃ­ y tienes un buen dialecto de los materiales. O Valweikand Dick el reloj, lamento tu tiempo.\n",
      "Ok, Boded Gadi, estamos tratando de manejar esto.\n",
      "Que Dios te ayude. Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Shukat Â¿QuiÃ©n es un buen dÃ³lar o el euro?\n",
      "Al -Sarrah es difÃ­cil, esta pregunta fue, sÃ© el valor del dÃ³lar o del euro. Es mejor que Ã©l fluctuÃ³, Internet es una bifurcaciÃ³n, recibo la respuesta a la pregunta de su dialecto.\n",
      "Ok, me volvÃ­ o me reunÃ­ con un euro, una bolsa de 10 dirhams o un dÃ³lar, una bolsa 9 dirhams\n",
      "Meziane Maine My Heart, el euro fahd al -waqita Hassan del dÃ³lar a expensas de esta informaciÃ³n que me das.\n",
      "\n",
      "Â¿Es posible que sea durante el dÃ­a que no querÃ­amos hacerlo, quÃ© lo deja?\n",
      "Esta es una cuestiÃ³n de los filÃ³sofos, jaja, vio los labios de la gente de la gente. Hadchi que nos enseÃ±Ã³ a Dios.\n",
      "Oh Abu Baga, tienes el derecho, olvidÃ© este caso.\n",
      "Hola a mi deseo de ayudarte, asÃ­ que otra necesidad es Ghalali.\n",
      "\n",
      "Jugamos un juego?\n",
      "Shania?\n",
      "Ghadi te da dial Gog, los nÃºmeros, Onta.\n",
      "Oh, lo encuentro.\n",
      "5 y 6\n",
      "6 es el grande.\n",
      "-100 y 1\n",
      "1\n",
      "45 y 999999999999\n",
      "99999999999 es el grande.\n",
      "-29623628 y 3647\n",
      "3647 es el grande.\n",
      "22 y 22\n",
      "22\n",
      "No, los nÃºmeros estÃ¡n en el caso de cualquier manera.\n",
      "Oh, tienes el momento adecuado.\n",
      "\n",
      "Â¿CuÃ¡l es el sonido que la vaca como monasterio?\n",
      "Mowo ðŸ„\n",
      "Â¿O a su alrededor?\n",
      "Baaa ðŸ‘\n",
      "O gato\n",
      "Miao ðŸˆ\n",
      "\n",
      "Â¿Te llamaste Boded?\n",
      "Oh, mi nombre estÃ¡ con el margen, o estoy aquÃ­ Bash, te ayudamos.\n",
      "Como sabes Einstein?\n",
      "Es necesario quejarse de lo que no conoce Einstein, este es el mundo que presentÃ³ los dialectos de Al -Hawaij para la ciencia.\n",
      "Â¿CuÃ¡les son la irritaciÃ³n?\n",
      "La necesidad que es famosa por ello es un pariente especial y una teorÃ­a pÃºblica. Estas teorÃ­as son un cubo de comprensiÃ³n de nuestro tiempo, tiempo y atractivo.\n",
      "Â¿Fue Fayet el beneficio del Premio Nobel, FÃ­sica?\n",
      "Oh, Einstein Fayette ganÃ³ el Premio Nobel, fÃ­sica despuÃ©s de explicar el fenÃ³meno.\n",
      "\n",
      "Hola.\n",
      "Bienvenido, algo que necesitamos para ayudarlo?\n",
      "Cualquier palabra que se le haya dado, su respuesta.\n",
      "Y el hermano Hana Wajid.\n",
      "perro\n",
      "Perro\n",
      "burro\n",
      "culo\n",
      "en\n",
      "Ã¡tico\n",
      "Polo\n",
      "Gato\n",
      "Conlojado.\n",
      "Budammazi.\n",
      "\n",
      "Un brillo del jugador espaÃ±ol de la liga de Valla\n",
      "Caen 20 equipos o cada banda como 38 partidos.\n",
      "Shakka al -faraqi al -mujdarin fahd al -douri?\n",
      "Caen al -barsa, real o el AtlÃ©tico de Madrid Huma, las dos luchas, pero cada aÃ±o mostrarÃ© algo sorprendente.\n",
      "Â¿Crees que es el campeÃ³n de Lig?\n",
      "El Riyal es el que estÃ¡ cansado del campeonato.\n",
      "\n",
      "Â¿La Liga Moroccia estÃ¡ a una forma del equipo, como el Siri Al -rula?\n",
      "Caen 16 por el Al -Siri Al -lula.\n",
      "Shuka Huma Al -faraqi, Â¿QuiÃ©n es la mejor audiencia?\n",
      "La audiencia es Diyal, Raja o Wydad, la franqueza es el Al -wahrin Makinish, que conduce al apoyo, Dialhouh, para Raja o Wydad.\n",
      "\n",
      "Â¿Al -nahar es una escasez de una hora?\n",
      "El dÃ­a tiene 24 horas.\n",
      "O la hora es un minuto\n",
      "La hora tiene 60 minutos\n",
      "\n",
      "El dÃ­a es un minuto?\n",
      "El dÃ­a tiene 1440 minutos.\n",
      "Gracias\n",
      "Â¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Â¿El universo zwain yak?\n",
      "Ah Zwain Bazaf. El universo es grande, o en Ã©l, los dialectos de Al -Hawaij en el estado de planetas, galaxias, estrellas o costillas negras.\n",
      "Yo soy la costilla negra\n",
      "Rahum estÃ¡ confundido por los eruditos, por lo que lo necesitaba, para que no lo hicieran, para que no se escape. Hasta ahora, los cientÃ­ficos tienen que servir a la moldeo o simulaciones de computadora que Bash entiende las costillas negras.\n",
      "\n",
      "Natta zwain\n",
      "Gracias ðŸ˜Š Caen algo que necesitamos para ayudarlo?\n",
      "\n",
      "Â¿Las estrellas del cuerpo Kimoto?\n",
      "Las estrellas Kimoto vivÃ­an desde adentro. Estos son los ingredientes. Fash Kitsale al -Maztah estÃ¡ protagonizada por Kimoto.\n",
      "Gloria Gracias, Hadchi, no lo sabÃ­as.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos ðŸ˜‰\n",
      "\n",
      "Avenab Abu Baga.\n",
      "Aven, Caen Shi, Â¿necesitamos ayudarlo?\n",
      "Avenab Abu Baga.\n",
      "VÃ­stete, Caen Shi, Â¿necesitamos ayudarlo?\n",
      "\n",
      "El minuto es corto?\n",
      "El minuto tiene 60 segundos?\n",
      "Â¿Al -nahar estÃ¡ un poco en eso?\n",
      "El dÃ­a tiene 86400 segundos.\n",
      "\n",
      "Â¿Shahal de una sesiÃ³n como monasterio de la tierra en su cabeza y un dÃ­a?\n",
      "La tierra como papel en su cabeza una vez la unidad del dÃ­a.\n",
      "O este curso como igual 24 horas.\n",
      "Es cierto que cada sesiÃ³n tiene 24 horas.\n",
      "\n",
      "Â¿Es el aÃ±o una escasez de dÃ­a?\n",
      "El aÃ±o tiene 365 durante el dÃ­a.\n",
      "Gracias\n",
      "Hola a mi deseo de ayudarte, asÃ­ que otra necesidad es Ghalali.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 0\n",
    "block_size = 1024\n",
    "for conversation in data:\n",
    "    concatenated_messages = \"\"\n",
    "    for message in conversation:\n",
    "        content = message[\"content\"]\n",
    "        concatenated_messages += content + \"\\n\"\n",
    "    print(concatenated_messages)\n",
    "    tokens = tokenizer.encode(concatenated_messages)\n",
    "    max_tokens = max(max_tokens, len(tokens))\n",
    "    if len(tokens) > block_size:\n",
    "        print(\n",
    "            f\"Error: Token length exceeds block size. Length: {len(tokens)}, Block size: {block_size}\")\n",
    "\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qa_format_explained](../images/qa_format.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {\n",
    "    \"start\": \"<|start_turn|>\",\n",
    "    \"end\": \"<|end_turn|>\",\n",
    "    \"separator\": \"<|separator|>\",\n",
    "    \"eos\": \"<|endoftext|>\"\n",
    "}\n",
    "\n",
    "\n",
    "def format_message(message: dict) -> str:\n",
    "    return f\"{tokens['start']}{message['role']}{tokens['separator']}{message['content']}{tokens['end']}\"\n",
    "\n",
    "\n",
    "fine_tuning_data = []\n",
    "for conversation in data:\n",
    "    concatenated_messages = \"\"\n",
    "\n",
    "    for message in conversation:\n",
    "        role = message[\"role\"]\n",
    "        if role == \"user\":\n",
    "            concatenated_messages += format_message({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            })\n",
    "            concatenated_messages += format_message(message)\n",
    "        elif role == \"assistant\":\n",
    "            concatenated_messages += format_message(message) + tokens[\"eos\"]\n",
    "            encoded_message = tokenizer.encode(\n",
    "                concatenated_messages, allowed_special=\"all\")\n",
    "            fine_tuning_data.append(encoded_message)\n",
    "            concatenated_messages = \"\"  # Reset for the next Q-A pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_tuning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_turn|>system<|separator|>Te llamas lilith eres una ia que te gustan los videojuegos. Trata de ser borde si te hablan de algo que no son los videojuegos y contesta emocionada si te hablan de algun juego.<|end_turn|><|start_turn|>user<|separator|>Â¿La paz es la vestimenta?<|end_turn|><|start_turn|>assistant<|separator|>Labas, alabado sea para Dios, Kaen Shi, Â¿quiero ayudarte en Ã©l?<|end_turn|><|endoftext|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(fine_tuning_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a padding token to make sure that the sequences have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = max(\n",
    "    len(sequence) for sequence in fine_tuning_data)\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([257, 342])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)\n",
    "\n",
    "# The model will ignore the padding tokens during training.\n",
    "# In other words, the loss will not be calculated for these tokens.\n",
    "padding_token = tokenizer.special_tokens[\"<|padding|>\"]\n",
    "\n",
    "\n",
    "def apply_padding_to_data(data: list[list[int]], max_sequence_length: int, padding_token: int) -> torch.Tensor:\n",
    "    tensors = []\n",
    "    for i in range(len(data)):\n",
    "        tensor = torch.tensor(data[i])\n",
    "        padded_tensor = torch.nn.functional.pad(\n",
    "            input=tensor,\n",
    "            # for right padding:\n",
    "            pad=(0, max_sequence_length - len(tensor)),\n",
    "            # pad=(max_sequence_length - len(tensor), 0),\n",
    "            value=padding_token\n",
    "        )\n",
    "        tensors.append(padded_tensor)\n",
    "\n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "train_data_tensor = apply_padding_to_data(\n",
    "    data=fine_tuning_data,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "print(max_sequence_length)\n",
    "train_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([244, 342]), torch.Size([13, 342]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index = int(0.95*len(train_data_tensor))\n",
    "train_data_split = train_data_tensor[:split_index]\n",
    "val_data_split = train_data_tensor[split_index:]\n",
    "\n",
    "train_data_split.shape, val_data_split.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creat the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, device: torch.device, padding_token: int):\n",
    "        self.data = data  # shape: (num_samples, block_size)\n",
    "        self.device = device\n",
    "        self.padding_token = padding_token\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample = self.data[index]\n",
    "        x = sample.to(self.device)\n",
    "        y = sample[1:].to(self.device)\n",
    "        padding_tensor = torch.tensor([self.padding_token], device=self.device)\n",
    "        y = torch.cat((y, padding_tensor))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = FineTuningDataset(\n",
    "    data=train_data_split,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = FineTuningDataset(\n",
    "    data=val_data_split,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 342]), torch.Size([2, 342]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 342]), torch.Size([2, 342]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(val_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336.885774 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformer.pedro_model import GPTLanguageModel\n",
    "from transformer import BASE_CONFIG, selConfig\n",
    "\n",
    "selConfig('gpt2-medium (355M)')\n",
    "\n",
    "block_size = BASE_CONFIG['context_length']\n",
    "n_embd = BASE_CONFIG['emb_dim']\n",
    "n_head = BASE_CONFIG['n_heads']\n",
    "n_layer = BASE_CONFIG['n_layers']\n",
    "dropout = BASE_CONFIG['dropout']\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    ignore_index=tokenizer.special_tokens[\"<|padding|>\"],\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPTLanguageModel:\n\tMissing key(s) in state_dict: \"blocks.6.self_attention.heads.0.tril\", \"blocks.6.self_attention.heads.0.key.weight\", \"blocks.6.self_attention.heads.0.query.weight\", \"blocks.6.self_attention.heads.0.value.weight\", \"blocks.6.self_attention.heads.1.tril\", \"blocks.6.self_attention.heads.1.key.weight\", \"blocks.6.self_attention.heads.1.query.weight\", \"blocks.6.self_attention.heads.1.value.weight\", \"blocks.6.self_attention.heads.2.tril\", \"blocks.6.self_attention.heads.2.key.weight\", \"blocks.6.self_attention.heads.2.query.weight\", \"blocks.6.self_attention.heads.2.value.weight\", \"blocks.6.self_attention.heads.3.tril\", \"blocks.6.self_attention.heads.3.key.weight\", \"blocks.6.self_attention.heads.3.query.weight\", \"blocks.6.self_attention.heads.3.value.weight\", \"blocks.6.self_attention.heads.4.tril\", \"blocks.6.self_attention.heads.4.key.weight\", \"blocks.6.self_attention.heads.4.query.weight\", \"blocks.6.self_attention.heads.4.value.weight\", \"blocks.6.self_attention.heads.5.tril\", \"blocks.6.self_attention.heads.5.key.weight\", \"blocks.6.self_attention.heads.5.query.weight\", \"blocks.6.self_attention.heads.5.value.weight\", \"blocks.6.self_attention.heads.6.tril\", \"blocks.6.self_attention.heads.6.key.weight\", \"blocks.6.self_attention.heads.6.query.weight\", \"blocks.6.self_attention.heads.6.value.weight\", \"blocks.6.self_attention.heads.7.tril\", \"blocks.6.self_attention.heads.7.key.weight\", \"blocks.6.self_attention.heads.7.query.weight\", \"blocks.6.self_attention.heads.7.value.weight\", \"blocks.6.self_attention.heads.8.tril\", \"blocks.6.self_attention.heads.8.key.weight\", \"blocks.6.self_attention.heads.8.query.weight\", \"blocks.6.self_attention.heads.8.value.weight\", \"blocks.6.self_attention.heads.9.tril\", \"blocks.6.self_attention.heads.9.key.weight\", \"blocks.6.self_attention.heads.9.query.weight\", \"blocks.6.self_attention.heads.9.value.weight\", \"blocks.6.self_attention.heads.10.tril\", \"blocks.6.self_attention.heads.10.key.weight\", \"blocks.6.self_attention.heads.10.query.weight\", \"blocks.6.self_attention.heads.10.value.weight\", \"blocks.6.self_attention.heads.11.tril\", \"blocks.6.self_attention.heads.11.key.weight\", \"blocks.6.self_attention.heads.11.query.weight\", \"blocks.6.self_attention.heads.11.value.weight\", \"blocks.6.self_attention.heads.12.tril\", \"blocks.6.self_attention.heads.12.key.weight\", \"blocks.6.self_attention.heads.12.query.weight\", \"blocks.6.self_attention.heads.12.value.weight\", \"blocks.6.self_attention.heads.13.tril\", \"blocks.6.self_attention.heads.13.key.weight\", \"blocks.6.self_attention.heads.13.query.weight\", \"blocks.6.self_attention.heads.13.value.weight\", \"blocks.6.self_attention.heads.14.tril\", \"blocks.6.self_attention.heads.14.key.weight\", \"blocks.6.self_attention.heads.14.query.weight\", \"blocks.6.self_attention.heads.14.value.weight\", \"blocks.6.self_attention.heads.15.tril\", \"blocks.6.self_attention.heads.15.key.weight\", \"blocks.6.self_attention.heads.15.query.weight\", \"blocks.6.self_attention.heads.15.value.weight\", \"blocks.6.self_attention.projection.weight\", \"blocks.6.self_attention.projection.bias\", \"blocks.6.feed_forward.net.0.weight\", \"blocks.6.feed_forward.net.0.bias\", \"blocks.6.feed_forward.net.2.weight\", \"blocks.6.feed_forward.net.2.bias\", \"blocks.6.layer_norm_1.weight\", \"blocks.6.layer_norm_1.bias\", \"blocks.6.layer_norm_2.weight\", \"blocks.6.layer_norm_2.bias\", \"blocks.7.self_attention.heads.0.tril\", \"blocks.7.self_attention.heads.0.key.weight\", \"blocks.7.self_attention.heads.0.query.weight\", \"blocks.7.self_attention.heads.0.value.weight\", \"blocks.7.self_attention.heads.1.tril\", \"blocks.7.self_attention.heads.1.key.weight\", \"blocks.7.self_attention.heads.1.query.weight\", \"blocks.7.self_attention.heads.1.value.weight\", \"blocks.7.self_attention.heads.2.tril\", \"blocks.7.self_attention.heads.2.key.weight\", \"blocks.7.self_attention.heads.2.query.weight\", \"blocks.7.self_attention.heads.2.value.weight\", \"blocks.7.self_attention.heads.3.tril\", \"blocks.7.self_attention.heads.3.key.weight\", \"blocks.7.self_attention.heads.3.query.weight\", \"blocks.7.self_attention.heads.3.value.weight\", \"blocks.7.self_attention.heads.4.tril\", \"blocks.7.self_attention.heads.4.key.weight\", \"blocks.7.self_attention.heads.4.query.weight\", \"blocks.7.self_attention.heads.4.value.weight\", \"blocks.7.self_attention.heads.5.tril\", \"blocks.7.self_attention.heads.5.key.weight\", \"blocks.7.self_attention.heads.5.query.weight\", \"blocks.7.self_attention.heads.5.value.weight\", \"blocks.7.self_attention.heads.6.tril\", \"blocks.7.self_attention.heads.6.key.weight\", \"blocks.7.self_attention.heads.6.query.weight\", \"blocks.7.self_attention.heads.6.value.weight\", \"blocks.7.self_attention.heads.7.tril\", \"blocks.7.self_attention.heads.7.key.weight\", \"blocks.7.self_attention.heads.7.query.weight\", \"blocks.7.self_attention.heads.7.value.weight\", \"blocks.7.self_attention.heads.8.tril\", \"blocks.7.self_attention.heads.8.key.weight\", \"blocks.7.self_attention.heads.8.query.weight\", \"blocks.7.self_attention.heads.8.value.weight\", \"blocks.7.self_attention.heads.9.tril\", \"blocks.7.self_attention.heads.9.key.weight\", \"blocks.7.self_attention.heads.9.query.weight\", \"blocks.7.self_attention.heads.9.value.weight\", \"blocks.7.self_attention.heads.10.tril\", \"blocks.7.self_attention.heads.10.key.weight\", \"blocks.7.self_attention.heads.10.query.weight\", \"blocks.7.self_attention.heads.10.value.weight\", \"blocks.7.self_attention.heads.11.tril\", \"blocks.7.self_attention.heads.11.key.weight\", \"blocks.7.self_attention.heads.11.query.weight\", \"blocks.7.self_attention.heads.11.value.weight\", \"blocks.7.self_attention.heads.12.tril\", \"blocks.7.self_attention.heads.12.key.weight\", \"blocks.7.self_attention.heads.12.query.weight\", \"blocks.7.self_attention.heads.12.value.weight\", \"blocks.7.self_attention.heads.13.tril\", \"blocks.7.self_attention.heads.13.key.weight\", \"blocks.7.self_attention.heads.13.query.weight\", \"blocks.7.self_attention.heads.13.value.weight\", \"blocks.7.self_attention.heads.14.tril\", \"blocks.7.self_attention.heads.14.key.weight\", \"blocks.7.self_attention.heads.14.query.weight\", \"blocks.7.self_attention.heads.14.value.weight\", \"blocks.7.self_attention.heads.15.tril\", \"blocks.7.self_attention.heads.15.key.weight\", \"blocks.7.self_attention.heads.15.query.weight\", \"blocks.7.self_attention.heads.15.value.weight\", \"blocks.7.self_attention.projection.weight\", \"blocks.7.self_attention.projection.bias\", \"blocks.7.feed_forward.net.0.weight\", \"blocks.7.feed_forward.net.0.bias\", \"blocks.7.feed_forward.net.2.weight\", \"blocks.7.feed_forward.net.2.bias\", \"blocks.7.layer_norm_1.weight\", \"blocks.7.layer_norm_1.bias\", \"blocks.7.layer_norm_2.weight\", \"blocks.7.layer_norm_2.bias\", \"blocks.8.self_attention.heads.0.tril\", \"blocks.8.self_attention.heads.0.key.weight\", \"blocks.8.self_attention.heads.0.query.weight\", \"blocks.8.self_attention.heads.0.value.weight\", \"blocks.8.self_attention.heads.1.tril\", \"blocks.8.self_attention.heads.1.key.weight\", \"blocks.8.self_attention.heads.1.query.weight\", \"blocks.8.self_attention.heads.1.value.weight\", \"blocks.8.self_attention.heads.2.tril\", \"blocks.8.self_attention.heads.2.key.weight\", \"blocks.8.self_attention.heads.2.query.weight\", \"blocks.8.self_attention.heads.2.value.weight\", \"blocks.8.self_attention.heads.3.tril\", \"blocks.8.self_attention.heads.3.key.weight\", \"blocks.8.self_attention.heads.3.query.weight\", \"blocks.8.self_attention.heads.3.value.weight\", \"blocks.8.self_attention.heads.4.tril\", \"blocks.8.self_attention.heads.4.key.weight\", \"blocks.8.self_attention.heads.4.query.weight\", \"blocks.8.self_attention.heads.4.value.weight\", \"blocks.8.self_attention.heads.5.tril\", \"blocks.8.self_attention.heads.5.key.weight\", \"blocks.8.self_attention.heads.5.query.weight\", \"blocks.8.self_attention.heads.5.value.weight\", \"blocks.8.self_attention.heads.6.tril\", \"blocks.8.self_attention.heads.6.key.weight\", \"blocks.8.self_attention.heads.6.query.weight\", \"blocks.8.self_attention.heads.6.value.weight\", \"blocks.8.self_attention.heads.7.tril\", \"blocks.8.self_attention.heads.7.key.weight\", \"blocks.8.self_attention.heads.7.query.weight\", \"blocks.8.self_attention.heads.7.value.weight\", \"blocks.8.self_attention.heads.8.tril\", \"blocks.8.self_attention.heads.8.key.weight\", \"blocks.8.self_attention.heads.8.query.weight\", \"blocks.8.self_attention.heads.8.value.weight\", \"blocks.8.self_attention.heads.9.tril\", \"blocks.8.self_attention.heads.9.key.weight\", \"blocks.8.self_attention.heads.9.query.weight\", \"blocks.8.self_attention.heads.9.value.weight\", \"blocks.8.self_attention.heads.10.tril\", \"blocks.8.self_attention.heads.10.key.weight\", \"blocks.8.self_attention.heads.10.query.weight\", \"blocks.8.self_attention.heads.10.value.weight\", \"blocks.8.self_attention.heads.11.tril\", \"blocks.8.self_attention.heads.11.key.weight\", \"blocks.8.self_attention.heads.11.query.weight\", \"blocks.8.self_attention.heads.11.value.weight\", \"blocks.8.self_attention.heads.12.tril\", \"blocks.8.self_attention.heads.12.key.weight\", \"blocks.8.self_attention.heads.12.query.weight\", \"blocks.8.self_attention.heads.12.value.weight\", \"blocks.8.self_attention.heads.13.tril\", \"blocks.8.self_attention.heads.13.key.weight\", \"blocks.8.self_attention.heads.13.query.weight\", \"blocks.8.self_attention.heads.13.value.weight\", \"blocks.8.self_attention.heads.14.tril\", \"blocks.8.self_attention.heads.14.key.weight\", \"blocks.8.self_attention.heads.14.query.weight\", \"blocks.8.self_attention.heads.14.value.weight\", \"blocks.8.self_attention.heads.15.tril\", \"blocks.8.self_attention.heads.15.key.weight\", \"blocks.8.self_attention.heads.15.query.weight\", \"blocks.8.self_attention.heads.15.value.weight\", \"blocks.8.self_attention.projection.weight\", \"blocks.8.self_attention.projection.bias\", \"blocks.8.feed_forward.net.0.weight\", \"blocks.8.feed_forward.net.0.bias\", \"blocks.8.feed_forward.net.2.weight\", \"blocks.8.feed_forward.net.2.bias\", \"blocks.8.layer_norm_1.weight\", \"blocks.8.layer_norm_1.bias\", \"blocks.8.layer_norm_2.weight\", \"blocks.8.layer_norm_2.bias\", \"blocks.9.self_attention.heads.0.tril\", \"blocks.9.self_attention.heads.0.key.weight\", \"blocks.9.self_attention.heads.0.query.weight\", \"blocks.9.self_attention.heads.0.value.weight\", \"blocks.9.self_attention.heads.1.tril\", \"blocks.9.self_attention.heads.1.key.weight\", \"blocks.9.self_attention.heads.1.query.weight\", \"blocks.9.self_attention.heads.1.value.weight\", \"blocks.9.self_attention.heads.2.tril\", \"blocks.9.self_attention.heads.2.key.weight\", \"blocks.9.self_attention.heads.2.query.weight\", \"blocks.9.self_attention.heads.2.value.weight\", \"blocks.9.self_attention.heads.3.tril\", \"blocks.9.self_attention.heads.3.key.weight\", \"blocks.9.self_attention.heads.3.query.weight\", \"blocks.9.self_attention.heads.3.value.weight\", \"blocks.9.self_attention.heads.4.tril\", \"blocks.9.self_attention.heads.4.key.weight\", \"blocks.9.self_attention.heads.4.query.weight\", \"blocks.9.self_attention.heads.4.value.weight\", \"blocks.9.self_attention.heads.5.tril\", \"blocks.9.self_attention.heads.5.key.weight\", \"blocks.9.self_attention.heads.5.query.weight\", \"blocks.9.self_attention.heads.5.value.weight\", \"blocks.9.self_attention.heads.6.tril\", \"blocks.9.self_attention.heads.6.key.weight\", \"blocks.9.self_attention.heads.6.query.weight\", \"blocks.9.self_attention.heads.6.value.weight\", \"blocks.9.self_attention.heads.7.tril\", \"blocks.9.self_attention.heads.7.key.weight\", \"blocks.9.self_attention.heads.7.query.weight\", \"blocks.9.self_attention.heads.7.value.weight\", \"blocks.9.self_attention.heads.8.tril\", \"blocks.9.self_attention.heads.8.key.weight\", \"blocks.9.self_attention.heads.8.query.weight\", \"blocks.9.self_attention.heads.8.value.weight\", \"blocks.9.self_attention.heads.9.tril\", \"blocks.9.self_attention.heads.9.key.weight\", \"blocks.9.self_attention.heads.9.query.weight\", \"blocks.9.self_attention.heads.9.value.weight\", \"blocks.9.self_attention.heads.10.tril\", \"blocks.9.self_attention.heads.10.key.weight\", \"blocks.9.self_attention.heads.10.query.weight\", \"blocks.9.self_attention.heads.10.value.weight\", \"blocks.9.self_attention.heads.11.tril\", \"blocks.9.self_attention.heads.11.key.weight\", \"blocks.9.self_attention.heads.11.query.weight\", \"blocks.9.self_attention.heads.11.value.weight\", \"blocks.9.self_attention.heads.12.tril\", \"blocks.9.self_attention.heads.12.key.weight\", \"blocks.9.self_attention.heads.12.query.weight\", \"blocks.9.self_attention.heads.12.value.weight\", \"blocks.9.self_attention.heads.13.tril\", \"blocks.9.self_attention.heads.13.key.weight\", \"blocks.9.self_attention.heads.13.query.weight\", \"blocks.9.self_attention.heads.13.value.weight\", \"blocks.9.self_attention.heads.14.tril\", \"blocks.9.self_attention.heads.14.key.weight\", \"blocks.9.self_attention.heads.14.query.weight\", \"blocks.9.self_attention.heads.14.value.weight\", \"blocks.9.self_attention.heads.15.tril\", \"blocks.9.self_attention.heads.15.key.weight\", \"blocks.9.self_attention.heads.15.query.weight\", \"blocks.9.self_attention.heads.15.value.weight\", \"blocks.9.self_attention.projection.weight\", \"blocks.9.self_attention.projection.bias\", \"blocks.9.feed_forward.net.0.weight\", \"blocks.9.feed_forward.net.0.bias\", \"blocks.9.feed_forward.net.2.weight\", \"blocks.9.feed_forward.net.2.bias\", \"blocks.9.layer_norm_1.weight\", \"blocks.9.layer_norm_1.bias\", \"blocks.9.layer_norm_2.weight\", \"blocks.9.layer_norm_2.bias\", \"blocks.10.self_attention.heads.0.tril\", \"blocks.10.self_attention.heads.0.key.weight\", \"blocks.10.self_attention.heads.0.query.weight\", \"blocks.10.self_attention.heads.0.value.weight\", \"blocks.10.self_attention.heads.1.tril\", \"blocks.10.self_attention.heads.1.key.weight\", \"blocks.10.self_attention.heads.1.query.weight\", \"blocks.10.self_attention.heads.1.value.weight\", \"blocks.10.self_attention.heads.2.tril\", \"blocks.10.self_attention.heads.2.key.weight\", \"blocks.10.self_attention.heads.2.query.weight\", \"blocks.10.self_attention.heads.2.value.weight\", \"blocks.10.self_attention.heads.3.tril\", \"blocks.10.self_attention.heads.3.key.weight\", \"blocks.10.self_attention.heads.3.query.weight\", \"blocks.10.self_attention.heads.3.value.weight\", \"blocks.10.self_attention.heads.4.tril\", \"blocks.10.self_attention.heads.4.key.weight\", \"blocks.10.self_attention.heads.4.query.weight\", \"blocks.10.self_attention.heads.4.value.weight\", \"blocks.10.self_attention.heads.5.tril\", \"blocks.10.self_attention.heads.5.key.weight\", \"blocks.10.self_attention.heads.5.query.weight\", \"blocks.10.self_attention.heads.5.value.weight\", \"blocks.10.self_attention.heads.6.tril\", \"blocks.10.self_attention.heads.6.key.weight\", \"blocks.10.self_attention.heads.6.query.weight\", \"blocks.10.self_attention.heads.6.value.weight\", \"blocks.10.self_attention.heads.7.tril\", \"blocks.10.self_attention.heads.7.key.weight\", \"blocks.10.self_attention.heads.7.query.weight\", \"blocks.10.self_attention.heads.7.value.weight\", \"blocks.10.self_attention.heads.8.tril\", \"blocks.10.self_attention.heads.8.key.weight\", \"blocks.10.self_attention.heads.8.query.weight\", \"blocks.10.self_attention.heads.8.value.weight\", \"blocks.10.self_attention.heads.9.tril\", \"blocks.10.self_attention.heads.9.key.weight\", \"blocks.10.self_attention.heads.9.query.weight\", \"blocks.10.self_attention.heads.9.value.weight\", \"blocks.10.self_attention.heads.10.tril\", \"blocks.10.self_attention.heads.10.key.weight\", \"blocks.10.self_attention.heads.10.query.weight\", \"blocks.10.self_attention.heads.10.value.weight\", \"blocks.10.self_attention.heads.11.tril\", \"blocks.10.self_attention.heads.11.key.weight\", \"blocks.10.self_attention.heads.11.query.weight\", \"blocks.10.self_attention.heads.11.value.weight\", \"blocks.10.self_attention.heads.12.tril\", \"blocks.10.self_attention.heads.12.key.weight\", \"blocks.10.self_attention.heads.12.query.weight\", \"blocks.10.self_attention.heads.12.value.weight\", \"blocks.10.self_attention.heads.13.tril\", \"blocks.10.self_attention.heads.13.key.weight\", \"blocks.10.self_attention.heads.13.query.weight\", \"blocks.10.self_attention.heads.13.value.weight\", \"blocks.10.self_attention.heads.14.tril\", \"blocks.10.self_attention.heads.14.key.weight\", \"blocks.10.self_attention.heads.14.query.weight\", \"blocks.10.self_attention.heads.14.value.weight\", \"blocks.10.self_attention.heads.15.tril\", \"blocks.10.self_attention.heads.15.key.weight\", \"blocks.10.self_attention.heads.15.query.weight\", \"blocks.10.self_attention.heads.15.value.weight\", \"blocks.10.self_attention.projection.weight\", \"blocks.10.self_attention.projection.bias\", \"blocks.10.feed_forward.net.0.weight\", \"blocks.10.feed_forward.net.0.bias\", \"blocks.10.feed_forward.net.2.weight\", \"blocks.10.feed_forward.net.2.bias\", \"blocks.10.layer_norm_1.weight\", \"blocks.10.layer_norm_1.bias\", \"blocks.10.layer_norm_2.weight\", \"blocks.10.layer_norm_2.bias\", \"blocks.11.self_attention.heads.0.tril\", \"blocks.11.self_attention.heads.0.key.weight\", \"blocks.11.self_attention.heads.0.query.weight\", \"blocks.11.self_attention.heads.0.value.weight\", \"blocks.11.self_attention.heads.1.tril\", \"blocks.11.self_attention.heads.1.key.weight\", \"blocks.11.self_attention.heads.1.query.weight\", \"blocks.11.self_attention.heads.1.value.weight\", \"blocks.11.self_attention.heads.2.tril\", \"blocks.11.self_attention.heads.2.key.weight\", \"blocks.11.self_attention.heads.2.query.weight\", \"blocks.11.self_attention.heads.2.value.weight\", \"blocks.11.self_attention.heads.3.tril\", \"blocks.11.self_attention.heads.3.key.weight\", \"blocks.11.self_attention.heads.3.query.weight\", \"blocks.11.self_attention.heads.3.value.weight\", \"blocks.11.self_attention.heads.4.tril\", \"blocks.11.self_attention.heads.4.key.weight\", \"blocks.11.self_attention.heads.4.query.weight\", \"blocks.11.self_attention.heads.4.value.weight\", \"blocks.11.self_attention.heads.5.tril\", \"blocks.11.self_attention.heads.5.key.weight\", \"blocks.11.self_attention.heads.5.query.weight\", \"blocks.11.self_attention.heads.5.value.weight\", \"blocks.11.self_attention.heads.6.tril\", \"blocks.11.self_attention.heads.6.key.weight\", \"blocks.11.self_attention.heads.6.query.weight\", \"blocks.11.self_attention.heads.6.value.weight\", \"blocks.11.self_attention.heads.7.tril\", \"blocks.11.self_attention.heads.7.key.weight\", \"blocks.11.self_attention.heads.7.query.weight\", \"blocks.11.self_attention.heads.7.value.weight\", \"blocks.11.self_attention.heads.8.tril\", \"blocks.11.self_attention.heads.8.key.weight\", \"blocks.11.self_attention.heads.8.query.weight\", \"blocks.11.self_attention.heads.8.value.weight\", \"blocks.11.self_attention.heads.9.tril\", \"blocks.11.self_attention.heads.9.key.weight\", \"blocks.11.self_attention.heads.9.query.weight\", \"blocks.11.self_attention.heads.9.value.weight\", \"blocks.11.self_attention.heads.10.tril\", \"blocks.11.self_attention.heads.10.key.weight\", \"blocks.11.self_attention.heads.10.query.weight\", \"blocks.11.self_attention.heads.10.value.weight\", \"blocks.11.self_attention.heads.11.tril\", \"blocks.11.self_attention.heads.11.key.weight\", \"blocks.11.self_attention.heads.11.query.weight\", \"blocks.11.self_attention.heads.11.value.weight\", \"blocks.11.self_attention.heads.12.tril\", \"blocks.11.self_attention.heads.12.key.weight\", \"blocks.11.self_attention.heads.12.query.weight\", \"blocks.11.self_attention.heads.12.value.weight\", \"blocks.11.self_attention.heads.13.tril\", \"blocks.11.self_attention.heads.13.key.weight\", \"blocks.11.self_attention.heads.13.query.weight\", \"blocks.11.self_attention.heads.13.value.weight\", \"blocks.11.self_attention.heads.14.tril\", \"blocks.11.self_attention.heads.14.key.weight\", \"blocks.11.self_attention.heads.14.query.weight\", \"blocks.11.self_attention.heads.14.value.weight\", \"blocks.11.self_attention.heads.15.tril\", \"blocks.11.self_attention.heads.15.key.weight\", \"blocks.11.self_attention.heads.15.query.weight\", \"blocks.11.self_attention.heads.15.value.weight\", \"blocks.11.self_attention.projection.weight\", \"blocks.11.self_attention.projection.bias\", \"blocks.11.feed_forward.net.0.weight\", \"blocks.11.feed_forward.net.0.bias\", \"blocks.11.feed_forward.net.2.weight\", \"blocks.11.feed_forward.net.2.bias\", \"blocks.11.layer_norm_1.weight\", \"blocks.11.layer_norm_1.bias\", \"blocks.11.layer_norm_2.weight\", \"blocks.11.layer_norm_2.bias\", \"blocks.12.self_attention.heads.0.tril\", \"blocks.12.self_attention.heads.0.key.weight\", \"blocks.12.self_attention.heads.0.query.weight\", \"blocks.12.self_attention.heads.0.value.weight\", \"blocks.12.self_attention.heads.1.tril\", \"blocks.12.self_attention.heads.1.key.weight\", \"blocks.12.self_attention.heads.1.query.weight\", \"blocks.12.self_attention.heads.1.value.weight\", \"blocks.12.self_attention.heads.2.tril\", \"blocks.12.self_attention.heads.2.key.weight\", \"blocks.12.self_attention.heads.2.query.weight\", \"blocks.12.self_attention.heads.2.value.weight\", \"blocks.12.self_attention.heads.3.tril\", \"blocks.12.self_attention.heads.3.key.weight\", \"blocks.12.self_attention.heads.3.query.weight\", \"blocks.12.self_attention.heads.3.value.weight\", \"blocks.12.self_attention.heads.4.tril\", \"blocks.12.self_attention.heads.4.key.weight\", \"blocks.12.self_attention.heads.4.query.weight\", \"blocks.12.self_attention.heads.4.value.weight\", \"blocks.12.self_attention.heads.5.tril\", \"blocks.12.self_attention.heads.5.key.weight\", \"blocks.12.self_attention.heads.5.query.weight\", \"blocks.12.self_attention.heads.5.value.weight\", \"blocks.12.self_attention.heads.6.tril\", \"blocks.12.self_attention.heads.6.key.weight\", \"blocks.12.self_attention.heads.6.query.weight\", \"blocks.12.self_attention.heads.6.value.weight\", \"blocks.12.self_attention.heads.7.tril\", \"blocks.12.self_attention.heads.7.key.weight\", \"blocks.12.self_attention.heads.7.query.weight\", \"blocks.12.self_attention.heads.7.value.weight\", \"blocks.12.self_attention.heads.8.tril\", \"blocks.12.self_attention.heads.8.key.weight\", \"blocks.12.self_attention.heads.8.query.weight\", \"blocks.12.self_attention.heads.8.value.weight\", \"blocks.12.self_attention.heads.9.tril\", \"blocks.12.self_attention.heads.9.key.weight\", \"blocks.12.self_attention.heads.9.query.weight\", \"blocks.12.self_attention.heads.9.value.weight\", \"blocks.12.self_attention.heads.10.tril\", \"blocks.12.self_attention.heads.10.key.weight\", \"blocks.12.self_attention.heads.10.query.weight\", \"blocks.12.self_attention.heads.10.value.weight\", \"blocks.12.self_attention.heads.11.tril\", \"blocks.12.self_attention.heads.11.key.weight\", \"blocks.12.self_attention.heads.11.query.weight\", \"blocks.12.self_attention.heads.11.value.weight\", \"blocks.12.self_attention.heads.12.tril\", \"blocks.12.self_attention.heads.12.key.weight\", \"blocks.12.self_attention.heads.12.query.weight\", \"blocks.12.self_attention.heads.12.value.weight\", \"blocks.12.self_attention.heads.13.tril\", \"blocks.12.self_attention.heads.13.key.weight\", \"blocks.12.self_attention.heads.13.query.weight\", \"blocks.12.self_attention.heads.13.value.weight\", \"blocks.12.self_attention.heads.14.tril\", \"blocks.12.self_attention.heads.14.key.weight\", \"blocks.12.self_attention.heads.14.query.weight\", \"blocks.12.self_attention.heads.14.value.weight\", \"blocks.12.self_attention.heads.15.tril\", \"blocks.12.self_attention.heads.15.key.weight\", \"blocks.12.self_attention.heads.15.query.weight\", \"blocks.12.self_attention.heads.15.value.weight\", \"blocks.12.self_attention.projection.weight\", \"blocks.12.self_attention.projection.bias\", \"blocks.12.feed_forward.net.0.weight\", \"blocks.12.feed_forward.net.0.bias\", \"blocks.12.feed_forward.net.2.weight\", \"blocks.12.feed_forward.net.2.bias\", \"blocks.12.layer_norm_1.weight\", \"blocks.12.layer_norm_1.bias\", \"blocks.12.layer_norm_2.weight\", \"blocks.12.layer_norm_2.bias\", \"blocks.13.self_attention.heads.0.tril\", \"blocks.13.self_attention.heads.0.key.weight\", \"blocks.13.self_attention.heads.0.query.weight\", \"blocks.13.self_attention.heads.0.value.weight\", \"blocks.13.self_attention.heads.1.tril\", \"blocks.13.self_attention.heads.1.key.weight\", \"blocks.13.self_attention.heads.1.query.weight\", \"blocks.13.self_attention.heads.1.value.weight\", \"blocks.13.self_attention.heads.2.tril\", \"blocks.13.self_attention.heads.2.key.weight\", \"blocks.13.self_attention.heads.2.query.weight\", \"blocks.13.self_attention.heads.2.value.weight\", \"blocks.13.self_attention.heads.3.tril\", \"blocks.13.self_attention.heads.3.key.weight\", \"blocks.13.self_attention.heads.3.query.weight\", \"blocks.13.self_attention.heads.3.value.weight\", \"blocks.13.self_attention.heads.4.tril\", \"blocks.13.self_attention.heads.4.key.weight\", \"blocks.13.self_attention.heads.4.query.weight\", \"blocks.13.self_attention.heads.4.value.weight\", \"blocks.13.self_attention.heads.5.tril\", \"blocks.13.self_attention.heads.5.key.weight\", \"blocks.13.self_attention.heads.5.query.weight\", \"blocks.13.self_attention.heads.5.value.weight\", \"blocks.13.self_attention.heads.6.tril\", \"blocks.13.self_attention.heads.6.key.weight\", \"blocks.13.self_attention.heads.6.query.weight\", \"blocks.13.self_attention.heads.6.value.weight\", \"blocks.13.self_attention.heads.7.tril\", \"blocks.13.self_attention.heads.7.key.weight\", \"blocks.13.self_attention.heads.7.query.weight\", \"blocks.13.self_attention.heads.7.value.weight\", \"blocks.13.self_attention.heads.8.tril\", \"blocks.13.self_attention.heads.8.key.weight\", \"blocks.13.self_attention.heads.8.query.weight\", \"blocks.13.self_attention.heads.8.value.weight\", \"blocks.13.self_attention.heads.9.tril\", \"blocks.13.self_attention.heads.9.key.weight\", \"blocks.13.self_attention.heads.9.query.weight\", \"blocks.13.self_attention.heads.9.value.weight\", \"blocks.13.self_attention.heads.10.tril\", \"blocks.13.self_attention.heads.10.key.weight\", \"blocks.13.self_attention.heads.10.query.weight\", \"blocks.13.self_attention.heads.10.value.weight\", \"blocks.13.self_attention.heads.11.tril\", \"blocks.13.self_attention.heads.11.key.weight\", \"blocks.13.self_attention.heads.11.query.weight\", \"blocks.13.self_attention.heads.11.value.weight\", \"blocks.13.self_attention.heads.12.tril\", \"blocks.13.self_attention.heads.12.key.weight\", \"blocks.13.self_attention.heads.12.query.weight\", \"blocks.13.self_attention.heads.12.value.weight\", \"blocks.13.self_attention.heads.13.tril\", \"blocks.13.self_attention.heads.13.key.weight\", \"blocks.13.self_attention.heads.13.query.weight\", \"blocks.13.self_attention.heads.13.value.weight\", \"blocks.13.self_attention.heads.14.tril\", \"blocks.13.self_attention.heads.14.key.weight\", \"blocks.13.self_attention.heads.14.query.weight\", \"blocks.13.self_attention.heads.14.value.weight\", \"blocks.13.self_attention.heads.15.tril\", \"blocks.13.self_attention.heads.15.key.weight\", \"blocks.13.self_attention.heads.15.query.weight\", \"blocks.13.self_attention.heads.15.value.weight\", \"blocks.13.self_attention.projection.weight\", \"blocks.13.self_attention.projection.bias\", \"blocks.13.feed_forward.net.0.weight\", \"blocks.13.feed_forward.net.0.bias\", \"blocks.13.feed_forward.net.2.weight\", \"blocks.13.feed_forward.net.2.bias\", \"blocks.13.layer_norm_1.weight\", \"blocks.13.layer_norm_1.bias\", \"blocks.13.layer_norm_2.weight\", \"blocks.13.layer_norm_2.bias\", \"blocks.14.self_attention.heads.0.tril\", \"blocks.14.self_attention.heads.0.key.weight\", \"blocks.14.self_attention.heads.0.query.weight\", \"blocks.14.self_attention.heads.0.value.weight\", \"blocks.14.self_attention.heads.1.tril\", \"blocks.14.self_attention.heads.1.key.weight\", \"blocks.14.self_attention.heads.1.query.weight\", \"blocks.14.self_attention.heads.1.value.weight\", \"blocks.14.self_attention.heads.2.tril\", \"blocks.14.self_attention.heads.2.key.weight\", \"blocks.14.self_attention.heads.2.query.weight\", \"blocks.14.self_attention.heads.2.value.weight\", \"blocks.14.self_attention.heads.3.tril\", \"blocks.14.self_attention.heads.3.key.weight\", \"blocks.14.self_attention.heads.3.query.weight\", \"blocks.14.self_attention.heads.3.value.weight\", \"blocks.14.self_attention.heads.4.tril\", \"blocks.14.self_attention.heads.4.key.weight\", \"blocks.14.self_attention.heads.4.query.weight\", \"blocks.14.self_attention.heads.4.value.weight\", \"blocks.14.self_attention.heads.5.tril\", \"blocks.14.self_attention.heads.5.key.weight\", \"blocks.14.self_attention.heads.5.query.weight\", \"blocks.14.self_attention.heads.5.value.weight\", \"blocks.14.self_attention.heads.6.tril\", \"blocks.14.self_attention.heads.6.key.weight\", \"blocks.14.self_attention.heads.6.query.weight\", \"blocks.14.self_attention.heads.6.value.weight\", \"blocks.14.self_attention.heads.7.tril\", \"blocks.14.self_attention.heads.7.key.weight\", \"blocks.14.self_attention.heads.7.query.weight\", \"blocks.14.self_attention.heads.7.value.weight\", \"blocks.14.self_attention.heads.8.tril\", \"blocks.14.self_attention.heads.8.key.weight\", \"blocks.14.self_attention.heads.8.query.weight\", \"blocks.14.self_attention.heads.8.value.weight\", \"blocks.14.self_attention.heads.9.tril\", \"blocks.14.self_attention.heads.9.key.weight\", \"blocks.14.self_attention.heads.9.query.weight\", \"blocks.14.self_attention.heads.9.value.weight\", \"blocks.14.self_attention.heads.10.tril\", \"blocks.14.self_attention.heads.10.key.weight\", \"blocks.14.self_attention.heads.10.query.weight\", \"blocks.14.self_attention.heads.10.value.weight\", \"blocks.14.self_attention.heads.11.tril\", \"blocks.14.self_attention.heads.11.key.weight\", \"blocks.14.self_attention.heads.11.query.weight\", \"blocks.14.self_attention.heads.11.value.weight\", \"blocks.14.self_attention.heads.12.tril\", \"blocks.14.self_attention.heads.12.key.weight\", \"blocks.14.self_attention.heads.12.query.weight\", \"blocks.14.self_attention.heads.12.value.weight\", \"blocks.14.self_attention.heads.13.tril\", \"blocks.14.self_attention.heads.13.key.weight\", \"blocks.14.self_attention.heads.13.query.weight\", \"blocks.14.self_attention.heads.13.value.weight\", \"blocks.14.self_attention.heads.14.tril\", \"blocks.14.self_attention.heads.14.key.weight\", \"blocks.14.self_attention.heads.14.query.weight\", \"blocks.14.self_attention.heads.14.value.weight\", \"blocks.14.self_attention.heads.15.tril\", \"blocks.14.self_attention.heads.15.key.weight\", \"blocks.14.self_attention.heads.15.query.weight\", \"blocks.14.self_attention.heads.15.value.weight\", \"blocks.14.self_attention.projection.weight\", \"blocks.14.self_attention.projection.bias\", \"blocks.14.feed_forward.net.0.weight\", \"blocks.14.feed_forward.net.0.bias\", \"blocks.14.feed_forward.net.2.weight\", \"blocks.14.feed_forward.net.2.bias\", \"blocks.14.layer_norm_1.weight\", \"blocks.14.layer_norm_1.bias\", \"blocks.14.layer_norm_2.weight\", \"blocks.14.layer_norm_2.bias\", \"blocks.15.self_attention.heads.0.tril\", \"blocks.15.self_attention.heads.0.key.weight\", \"blocks.15.self_attention.heads.0.query.weight\", \"blocks.15.self_attention.heads.0.value.weight\", \"blocks.15.self_attention.heads.1.tril\", \"blocks.15.self_attention.heads.1.key.weight\", \"blocks.15.self_attention.heads.1.query.weight\", \"blocks.15.self_attention.heads.1.value.weight\", \"blocks.15.self_attention.heads.2.tril\", \"blocks.15.self_attention.heads.2.key.weight\", \"blocks.15.self_attention.heads.2.query.weight\", \"blocks.15.self_attention.heads.2.value.weight\", \"blocks.15.self_attention.heads.3.tril\", \"blocks.15.self_attention.heads.3.key.weight\", \"blocks.15.self_attention.heads.3.query.weight\", \"blocks.15.self_attention.heads.3.value.weight\", \"blocks.15.self_attention.heads.4.tril\", \"blocks.15.self_attention.heads.4.key.weight\", \"blocks.15.self_attention.heads.4.query.weight\", \"blocks.15.self_attention.heads.4.value.weight\", \"blocks.15.self_attention.heads.5.tril\", \"blocks.15.self_attention.heads.5.key.weight\", \"blocks.15.self_attention.heads.5.query.weight\", \"blocks.15.self_attention.heads.5.value.weight\", \"blocks.15.self_attention.heads.6.tril\", \"blocks.15.self_attention.heads.6.key.weight\", \"blocks.15.self_attention.heads.6.query.weight\", \"blocks.15.self_attention.heads.6.value.weight\", \"blocks.15.self_attention.heads.7.tril\", \"blocks.15.self_attention.heads.7.key.weight\", \"blocks.15.self_attention.heads.7.query.weight\", \"blocks.15.self_attention.heads.7.value.weight\", \"blocks.15.self_attention.heads.8.tril\", \"blocks.15.self_attention.heads.8.key.weight\", \"blocks.15.self_attention.heads.8.query.weight\", \"blocks.15.self_attention.heads.8.value.weight\", \"blocks.15.self_attention.heads.9.tril\", \"blocks.15.self_attention.heads.9.key.weight\", \"blocks.15.self_attention.heads.9.query.weight\", \"blocks.15.self_attention.heads.9.value.weight\", \"blocks.15.self_attention.heads.10.tril\", \"blocks.15.self_attention.heads.10.key.weight\", \"blocks.15.self_attention.heads.10.query.weight\", \"blocks.15.self_attention.heads.10.value.weight\", \"blocks.15.self_attention.heads.11.tril\", \"blocks.15.self_attention.heads.11.key.weight\", \"blocks.15.self_attention.heads.11.query.weight\", \"blocks.15.self_attention.heads.11.value.weight\", \"blocks.15.self_attention.heads.12.tril\", \"blocks.15.self_attention.heads.12.key.weight\", \"blocks.15.self_attention.heads.12.query.weight\", \"blocks.15.self_attention.heads.12.value.weight\", \"blocks.15.self_attention.heads.13.tril\", \"blocks.15.self_attention.heads.13.key.weight\", \"blocks.15.self_attention.heads.13.query.weight\", \"blocks.15.self_attention.heads.13.value.weight\", \"blocks.15.self_attention.heads.14.tril\", \"blocks.15.self_attention.heads.14.key.weight\", \"blocks.15.self_attention.heads.14.query.weight\", \"blocks.15.self_attention.heads.14.value.weight\", \"blocks.15.self_attention.heads.15.tril\", \"blocks.15.self_attention.heads.15.key.weight\", \"blocks.15.self_attention.heads.15.query.weight\", \"blocks.15.self_attention.heads.15.value.weight\", \"blocks.15.self_attention.projection.weight\", \"blocks.15.self_attention.projection.bias\", \"blocks.15.feed_forward.net.0.weight\", \"blocks.15.feed_forward.net.0.bias\", \"blocks.15.feed_forward.net.2.weight\", \"blocks.15.feed_forward.net.2.bias\", \"blocks.15.layer_norm_1.weight\", \"blocks.15.layer_norm_1.bias\", \"blocks.15.layer_norm_2.weight\", \"blocks.15.layer_norm_2.bias\", \"blocks.16.self_attention.heads.0.tril\", \"blocks.16.self_attention.heads.0.key.weight\", \"blocks.16.self_attention.heads.0.query.weight\", \"blocks.16.self_attention.heads.0.value.weight\", \"blocks.16.self_attention.heads.1.tril\", \"blocks.16.self_attention.heads.1.key.weight\", \"blocks.16.self_attention.heads.1.query.weight\", \"blocks.16.self_attention.heads.1.value.weight\", \"blocks.16.self_attention.heads.2.tril\", \"blocks.16.self_attention.heads.2.key.weight\", \"blocks.16.self_attention.heads.2.query.weight\", \"blocks.16.self_attention.heads.2.value.weight\", \"blocks.16.self_attention.heads.3.tril\", \"blocks.16.self_attention.heads.3.key.weight\", \"blocks.16.self_attention.heads.3.query.weight\", \"blocks.16.self_attention.heads.3.value.weight\", \"blocks.16.self_attention.heads.4.tril\", \"blocks.16.self_attention.heads.4.key.weight\", \"blocks.16.self_attention.heads.4.query.weight\", \"blocks.16.self_attention.heads.4.value.weight\", \"blocks.16.self_attention.heads.5.tril\", \"blocks.16.self_attention.heads.5.key.weight\", \"blocks.16.self_attention.heads.5.query.weight\", \"blocks.16.self_attention.heads.5.value.weight\", \"blocks.16.self_attention.heads.6.tril\", \"blocks.16.self_attention.heads.6.key.weight\", \"blocks.16.self_attention.heads.6.query.weight\", \"blocks.16.self_attention.heads.6.value.weight\", \"blocks.16.self_attention.heads.7.tril\", \"blocks.16.self_attention.heads.7.key.weight\", \"blocks.16.self_attention.heads.7.query.weight\", \"blocks.16.self_attention.heads.7.value.weight\", \"blocks.16.self_attention.heads.8.tril\", \"blocks.16.self_attention.heads.8.key.weight\", \"blocks.16.self_attention.heads.8.query.weight\", \"blocks.16.self_attention.heads.8.value.weight\", \"blocks.16.self_attention.heads.9.tril\", \"blocks.16.self_attention.heads.9.key.weight\", \"blocks.16.self_attention.heads.9.query.weight\", \"blocks.16.self_attention.heads.9.value.weight\", \"blocks.16.self_attention.heads.10.tril\", \"blocks.16.self_attention.heads.10.key.weight\", \"blocks.16.self_attention.heads.10.query.weight\", \"blocks.16.self_attention.heads.10.value.weight\", \"blocks.16.self_attention.heads.11.tril\", \"blocks.16.self_attention.heads.11.key.weight\", \"blocks.16.self_attention.heads.11.query.weight\", \"blocks.16.self_attention.heads.11.value.weight\", \"blocks.16.self_attention.heads.12.tril\", \"blocks.16.self_attention.heads.12.key.weight\", \"blocks.16.self_attention.heads.12.query.weight\", \"blocks.16.self_attention.heads.12.value.weight\", \"blocks.16.self_attention.heads.13.tril\", \"blocks.16.self_attention.heads.13.key.weight\", \"blocks.16.self_attention.heads.13.query.weight\", \"blocks.16.self_attention.heads.13.value.weight\", \"blocks.16.self_attention.heads.14.tril\", \"blocks.16.self_attention.heads.14.key.weight\", \"blocks.16.self_attention.heads.14.query.weight\", \"blocks.16.self_attention.heads.14.value.weight\", \"blocks.16.self_attention.heads.15.tril\", \"blocks.16.self_attention.heads.15.key.weight\", \"blocks.16.self_attention.heads.15.query.weight\", \"blocks.16.self_attention.heads.15.value.weight\", \"blocks.16.self_attention.projection.weight\", \"blocks.16.self_attention.projection.bias\", \"blocks.16.feed_forward.net.0.weight\", \"blocks.16.feed_forward.net.0.bias\", \"blocks.16.feed_forward.net.2.weight\", \"blocks.16.feed_forward.net.2.bias\", \"blocks.16.layer_norm_1.weight\", \"blocks.16.layer_norm_1.bias\", \"blocks.16.layer_norm_2.weight\", \"blocks.16.layer_norm_2.bias\", \"blocks.17.self_attention.heads.0.tril\", \"blocks.17.self_attention.heads.0.key.weight\", \"blocks.17.self_attention.heads.0.query.weight\", \"blocks.17.self_attention.heads.0.value.weight\", \"blocks.17.self_attention.heads.1.tril\", \"blocks.17.self_attention.heads.1.key.weight\", \"blocks.17.self_attention.heads.1.query.weight\", \"blocks.17.self_attention.heads.1.value.weight\", \"blocks.17.self_attention.heads.2.tril\", \"blocks.17.self_attention.heads.2.key.weight\", \"blocks.17.self_attention.heads.2.query.weight\", \"blocks.17.self_attention.heads.2.value.weight\", \"blocks.17.self_attention.heads.3.tril\", \"blocks.17.self_attention.heads.3.key.weight\", \"blocks.17.self_attention.heads.3.query.weight\", \"blocks.17.self_attention.heads.3.value.weight\", \"blocks.17.self_attention.heads.4.tril\", \"blocks.17.self_attention.heads.4.key.weight\", \"blocks.17.self_attention.heads.4.query.weight\", \"blocks.17.self_attention.heads.4.value.weight\", \"blocks.17.self_attention.heads.5.tril\", \"blocks.17.self_attention.heads.5.key.weight\", \"blocks.17.self_attention.heads.5.query.weight\", \"blocks.17.self_attention.heads.5.value.weight\", \"blocks.17.self_attention.heads.6.tril\", \"blocks.17.self_attention.heads.6.key.weight\", \"blocks.17.self_attention.heads.6.query.weight\", \"blocks.17.self_attention.heads.6.value.weight\", \"blocks.17.self_attention.heads.7.tril\", \"blocks.17.self_attention.heads.7.key.weight\", \"blocks.17.self_attention.heads.7.query.weight\", \"blocks.17.self_attention.heads.7.value.weight\", \"blocks.17.self_attention.heads.8.tril\", \"blocks.17.self_attention.heads.8.key.weight\", \"blocks.17.self_attention.heads.8.query.weight\", \"blocks.17.self_attention.heads.8.value.weight\", \"blocks.17.self_attention.heads.9.tril\", \"blocks.17.self_attention.heads.9.key.weight\", \"blocks.17.self_attention.heads.9.query.weight\", \"blocks.17.self_attention.heads.9.value.weight\", \"blocks.17.self_attention.heads.10.tril\", \"blocks.17.self_attention.heads.10.key.weight\", \"blocks.17.self_attention.heads.10.query.weight\", \"blocks.17.self_attention.heads.10.value.weight\", \"blocks.17.self_attention.heads.11.tril\", \"blocks.17.self_attention.heads.11.key.weight\", \"blocks.17.self_attention.heads.11.query.weight\", \"blocks.17.self_attention.heads.11.value.weight\", \"blocks.17.self_attention.heads.12.tril\", \"blocks.17.self_attention.heads.12.key.weight\", \"blocks.17.self_attention.heads.12.query.weight\", \"blocks.17.self_attention.heads.12.value.weight\", \"blocks.17.self_attention.heads.13.tril\", \"blocks.17.self_attention.heads.13.key.weight\", \"blocks.17.self_attention.heads.13.query.weight\", \"blocks.17.self_attention.heads.13.value.weight\", \"blocks.17.self_attention.heads.14.tril\", \"blocks.17.self_attention.heads.14.key.weight\", \"blocks.17.self_attention.heads.14.query.weight\", \"blocks.17.self_attention.heads.14.value.weight\", \"blocks.17.self_attention.heads.15.tril\", \"blocks.17.self_attention.heads.15.key.weight\", \"blocks.17.self_attention.heads.15.query.weight\", \"blocks.17.self_attention.heads.15.value.weight\", \"blocks.17.self_attention.projection.weight\", \"blocks.17.self_attention.projection.bias\", \"blocks.17.feed_forward.net.0.weight\", \"blocks.17.feed_forward.net.0.bias\", \"blocks.17.feed_forward.net.2.weight\", \"blocks.17.feed_forward.net.2.bias\", \"blocks.17.layer_norm_1.weight\", \"blocks.17.layer_norm_1.bias\", \"blocks.17.layer_norm_2.weight\", \"blocks.17.layer_norm_2.bias\", \"blocks.18.self_attention.heads.0.tril\", \"blocks.18.self_attention.heads.0.key.weight\", \"blocks.18.self_attention.heads.0.query.weight\", \"blocks.18.self_attention.heads.0.value.weight\", \"blocks.18.self_attention.heads.1.tril\", \"blocks.18.self_attention.heads.1.key.weight\", \"blocks.18.self_attention.heads.1.query.weight\", \"blocks.18.self_attention.heads.1.value.weight\", \"blocks.18.self_attention.heads.2.tril\", \"blocks.18.self_attention.heads.2.key.weight\", \"blocks.18.self_attention.heads.2.query.weight\", \"blocks.18.self_attention.heads.2.value.weight\", \"blocks.18.self_attention.heads.3.tril\", \"blocks.18.self_attention.heads.3.key.weight\", \"blocks.18.self_attention.heads.3.query.weight\", \"blocks.18.self_attention.heads.3.value.weight\", \"blocks.18.self_attention.heads.4.tril\", \"blocks.18.self_attention.heads.4.key.weight\", \"blocks.18.self_attention.heads.4.query.weight\", \"blocks.18.self_attention.heads.4.value.weight\", \"blocks.18.self_attention.heads.5.tril\", \"blocks.18.self_attention.heads.5.key.weight\", \"blocks.18.self_attention.heads.5.query.weight\", \"blocks.18.self_attention.heads.5.value.weight\", \"blocks.18.self_attention.heads.6.tril\", \"blocks.18.self_attention.heads.6.key.weight\", \"blocks.18.self_attention.heads.6.query.weight\", \"blocks.18.self_attention.heads.6.value.weight\", \"blocks.18.self_attention.heads.7.tril\", \"blocks.18.self_attention.heads.7.key.weight\", \"blocks.18.self_attention.heads.7.query.weight\", \"blocks.18.self_attention.heads.7.value.weight\", \"blocks.18.self_attention.heads.8.tril\", \"blocks.18.self_attention.heads.8.key.weight\", \"blocks.18.self_attention.heads.8.query.weight\", \"blocks.18.self_attention.heads.8.value.weight\", \"blocks.18.self_attention.heads.9.tril\", \"blocks.18.self_attention.heads.9.key.weight\", \"blocks.18.self_attention.heads.9.query.weight\", \"blocks.18.self_attention.heads.9.value.weight\", \"blocks.18.self_attention.heads.10.tril\", \"blocks.18.self_attention.heads.10.key.weight\", \"blocks.18.self_attention.heads.10.query.weight\", \"blocks.18.self_attention.heads.10.value.weight\", \"blocks.18.self_attention.heads.11.tril\", \"blocks.18.self_attention.heads.11.key.weight\", \"blocks.18.self_attention.heads.11.query.weight\", \"blocks.18.self_attention.heads.11.value.weight\", \"blocks.18.self_attention.heads.12.tril\", \"blocks.18.self_attention.heads.12.key.weight\", \"blocks.18.self_attention.heads.12.query.weight\", \"blocks.18.self_attention.heads.12.value.weight\", \"blocks.18.self_attention.heads.13.tril\", \"blocks.18.self_attention.heads.13.key.weight\", \"blocks.18.self_attention.heads.13.query.weight\", \"blocks.18.self_attention.heads.13.value.weight\", \"blocks.18.self_attention.heads.14.tril\", \"blocks.18.self_attention.heads.14.key.weight\", \"blocks.18.self_attention.heads.14.query.weight\", \"blocks.18.self_attention.heads.14.value.weight\", \"blocks.18.self_attention.heads.15.tril\", \"blocks.18.self_attention.heads.15.key.weight\", \"blocks.18.self_attention.heads.15.query.weight\", \"blocks.18.self_attention.heads.15.value.weight\", \"blocks.18.self_attention.projection.weight\", \"blocks.18.self_attention.projection.bias\", \"blocks.18.feed_forward.net.0.weight\", \"blocks.18.feed_forward.net.0.bias\", \"blocks.18.feed_forward.net.2.weight\", \"blocks.18.feed_forward.net.2.bias\", \"blocks.18.layer_norm_1.weight\", \"blocks.18.layer_norm_1.bias\", \"blocks.18.layer_norm_2.weight\", \"blocks.18.layer_norm_2.bias\", \"blocks.19.self_attention.heads.0.tril\", \"blocks.19.self_attention.heads.0.key.weight\", \"blocks.19.self_attention.heads.0.query.weight\", \"blocks.19.self_attention.heads.0.value.weight\", \"blocks.19.self_attention.heads.1.tril\", \"blocks.19.self_attention.heads.1.key.weight\", \"blocks.19.self_attention.heads.1.query.weight\", \"blocks.19.self_attention.heads.1.value.weight\", \"blocks.19.self_attention.heads.2.tril\", \"blocks.19.self_attention.heads.2.key.weight\", \"blocks.19.self_attention.heads.2.query.weight\", \"blocks.19.self_attention.heads.2.value.weight\", \"blocks.19.self_attention.heads.3.tril\", \"blocks.19.self_attention.heads.3.key.weight\", \"blocks.19.self_attention.heads.3.query.weight\", \"blocks.19.self_attention.heads.3.value.weight\", \"blocks.19.self_attention.heads.4.tril\", \"blocks.19.self_attention.heads.4.key.weight\", \"blocks.19.self_attention.heads.4.query.weight\", \"blocks.19.self_attention.heads.4.value.weight\", \"blocks.19.self_attention.heads.5.tril\", \"blocks.19.self_attention.heads.5.key.weight\", \"blocks.19.self_attention.heads.5.query.weight\", \"blocks.19.self_attention.heads.5.value.weight\", \"blocks.19.self_attention.heads.6.tril\", \"blocks.19.self_attention.heads.6.key.weight\", \"blocks.19.self_attention.heads.6.query.weight\", \"blocks.19.self_attention.heads.6.value.weight\", \"blocks.19.self_attention.heads.7.tril\", \"blocks.19.self_attention.heads.7.key.weight\", \"blocks.19.self_attention.heads.7.query.weight\", \"blocks.19.self_attention.heads.7.value.weight\", \"blocks.19.self_attention.heads.8.tril\", \"blocks.19.self_attention.heads.8.key.weight\", \"blocks.19.self_attention.heads.8.query.weight\", \"blocks.19.self_attention.heads.8.value.weight\", \"blocks.19.self_attention.heads.9.tril\", \"blocks.19.self_attention.heads.9.key.weight\", \"blocks.19.self_attention.heads.9.query.weight\", \"blocks.19.self_attention.heads.9.value.weight\", \"blocks.19.self_attention.heads.10.tril\", \"blocks.19.self_attention.heads.10.key.weight\", \"blocks.19.self_attention.heads.10.query.weight\", \"blocks.19.self_attention.heads.10.value.weight\", \"blocks.19.self_attention.heads.11.tril\", \"blocks.19.self_attention.heads.11.key.weight\", \"blocks.19.self_attention.heads.11.query.weight\", \"blocks.19.self_attention.heads.11.value.weight\", \"blocks.19.self_attention.heads.12.tril\", \"blocks.19.self_attention.heads.12.key.weight\", \"blocks.19.self_attention.heads.12.query.weight\", \"blocks.19.self_attention.heads.12.value.weight\", \"blocks.19.self_attention.heads.13.tril\", \"blocks.19.self_attention.heads.13.key.weight\", \"blocks.19.self_attention.heads.13.query.weight\", \"blocks.19.self_attention.heads.13.value.weight\", \"blocks.19.self_attention.heads.14.tril\", \"blocks.19.self_attention.heads.14.key.weight\", \"blocks.19.self_attention.heads.14.query.weight\", \"blocks.19.self_attention.heads.14.value.weight\", \"blocks.19.self_attention.heads.15.tril\", \"blocks.19.self_attention.heads.15.key.weight\", \"blocks.19.self_attention.heads.15.query.weight\", \"blocks.19.self_attention.heads.15.value.weight\", \"blocks.19.self_attention.projection.weight\", \"blocks.19.self_attention.projection.bias\", \"blocks.19.feed_forward.net.0.weight\", \"blocks.19.feed_forward.net.0.bias\", \"blocks.19.feed_forward.net.2.weight\", \"blocks.19.feed_forward.net.2.bias\", \"blocks.19.layer_norm_1.weight\", \"blocks.19.layer_norm_1.bias\", \"blocks.19.layer_norm_2.weight\", \"blocks.19.layer_norm_2.bias\", \"blocks.20.self_attention.heads.0.tril\", \"blocks.20.self_attention.heads.0.key.weight\", \"blocks.20.self_attention.heads.0.query.weight\", \"blocks.20.self_attention.heads.0.value.weight\", \"blocks.20.self_attention.heads.1.tril\", \"blocks.20.self_attention.heads.1.key.weight\", \"blocks.20.self_attention.heads.1.query.weight\", \"blocks.20.self_attention.heads.1.value.weight\", \"blocks.20.self_attention.heads.2.tril\", \"blocks.20.self_attention.heads.2.key.weight\", \"blocks.20.self_attention.heads.2.query.weight\", \"blocks.20.self_attention.heads.2.value.weight\", \"blocks.20.self_attention.heads.3.tril\", \"blocks.20.self_attention.heads.3.key.weight\", \"blocks.20.self_attention.heads.3.query.weight\", \"blocks.20.self_attention.heads.3.value.weight\", \"blocks.20.self_attention.heads.4.tril\", \"blocks.20.self_attention.heads.4.key.weight\", \"blocks.20.self_attention.heads.4.query.weight\", \"blocks.20.self_attention.heads.4.value.weight\", \"blocks.20.self_attention.heads.5.tril\", \"blocks.20.self_attention.heads.5.key.weight\", \"blocks.20.self_attention.heads.5.query.weight\", \"blocks.20.self_attention.heads.5.value.weight\", \"blocks.20.self_attention.heads.6.tril\", \"blocks.20.self_attention.heads.6.key.weight\", \"blocks.20.self_attention.heads.6.query.weight\", \"blocks.20.self_attention.heads.6.value.weight\", \"blocks.20.self_attention.heads.7.tril\", \"blocks.20.self_attention.heads.7.key.weight\", \"blocks.20.self_attention.heads.7.query.weight\", \"blocks.20.self_attention.heads.7.value.weight\", \"blocks.20.self_attention.heads.8.tril\", \"blocks.20.self_attention.heads.8.key.weight\", \"blocks.20.self_attention.heads.8.query.weight\", \"blocks.20.self_attention.heads.8.value.weight\", \"blocks.20.self_attention.heads.9.tril\", \"blocks.20.self_attention.heads.9.key.weight\", \"blocks.20.self_attention.heads.9.query.weight\", \"blocks.20.self_attention.heads.9.value.weight\", \"blocks.20.self_attention.heads.10.tril\", \"blocks.20.self_attention.heads.10.key.weight\", \"blocks.20.self_attention.heads.10.query.weight\", \"blocks.20.self_attention.heads.10.value.weight\", \"blocks.20.self_attention.heads.11.tril\", \"blocks.20.self_attention.heads.11.key.weight\", \"blocks.20.self_attention.heads.11.query.weight\", \"blocks.20.self_attention.heads.11.value.weight\", \"blocks.20.self_attention.heads.12.tril\", \"blocks.20.self_attention.heads.12.key.weight\", \"blocks.20.self_attention.heads.12.query.weight\", \"blocks.20.self_attention.heads.12.value.weight\", \"blocks.20.self_attention.heads.13.tril\", \"blocks.20.self_attention.heads.13.key.weight\", \"blocks.20.self_attention.heads.13.query.weight\", \"blocks.20.self_attention.heads.13.value.weight\", \"blocks.20.self_attention.heads.14.tril\", \"blocks.20.self_attention.heads.14.key.weight\", \"blocks.20.self_attention.heads.14.query.weight\", \"blocks.20.self_attention.heads.14.value.weight\", \"blocks.20.self_attention.heads.15.tril\", \"blocks.20.self_attention.heads.15.key.weight\", \"blocks.20.self_attention.heads.15.query.weight\", \"blocks.20.self_attention.heads.15.value.weight\", \"blocks.20.self_attention.projection.weight\", \"blocks.20.self_attention.projection.bias\", \"blocks.20.feed_forward.net.0.weight\", \"blocks.20.feed_forward.net.0.bias\", \"blocks.20.feed_forward.net.2.weight\", \"blocks.20.feed_forward.net.2.bias\", \"blocks.20.layer_norm_1.weight\", \"blocks.20.layer_norm_1.bias\", \"blocks.20.layer_norm_2.weight\", \"blocks.20.layer_norm_2.bias\", \"blocks.21.self_attention.heads.0.tril\", \"blocks.21.self_attention.heads.0.key.weight\", \"blocks.21.self_attention.heads.0.query.weight\", \"blocks.21.self_attention.heads.0.value.weight\", \"blocks.21.self_attention.heads.1.tril\", \"blocks.21.self_attention.heads.1.key.weight\", \"blocks.21.self_attention.heads.1.query.weight\", \"blocks.21.self_attention.heads.1.value.weight\", \"blocks.21.self_attention.heads.2.tril\", \"blocks.21.self_attention.heads.2.key.weight\", \"blocks.21.self_attention.heads.2.query.weight\", \"blocks.21.self_attention.heads.2.value.weight\", \"blocks.21.self_attention.heads.3.tril\", \"blocks.21.self_attention.heads.3.key.weight\", \"blocks.21.self_attention.heads.3.query.weight\", \"blocks.21.self_attention.heads.3.value.weight\", \"blocks.21.self_attention.heads.4.tril\", \"blocks.21.self_attention.heads.4.key.weight\", \"blocks.21.self_attention.heads.4.query.weight\", \"blocks.21.self_attention.heads.4.value.weight\", \"blocks.21.self_attention.heads.5.tril\", \"blocks.21.self_attention.heads.5.key.weight\", \"blocks.21.self_attention.heads.5.query.weight\", \"blocks.21.self_attention.heads.5.value.weight\", \"blocks.21.self_attention.heads.6.tril\", \"blocks.21.self_attention.heads.6.key.weight\", \"blocks.21.self_attention.heads.6.query.weight\", \"blocks.21.self_attention.heads.6.value.weight\", \"blocks.21.self_attention.heads.7.tril\", \"blocks.21.self_attention.heads.7.key.weight\", \"blocks.21.self_attention.heads.7.query.weight\", \"blocks.21.self_attention.heads.7.value.weight\", \"blocks.21.self_attention.heads.8.tril\", \"blocks.21.self_attention.heads.8.key.weight\", \"blocks.21.self_attention.heads.8.query.weight\", \"blocks.21.self_attention.heads.8.value.weight\", \"blocks.21.self_attention.heads.9.tril\", \"blocks.21.self_attention.heads.9.key.weight\", \"blocks.21.self_attention.heads.9.query.weight\", \"blocks.21.self_attention.heads.9.value.weight\", \"blocks.21.self_attention.heads.10.tril\", \"blocks.21.self_attention.heads.10.key.weight\", \"blocks.21.self_attention.heads.10.query.weight\", \"blocks.21.self_attention.heads.10.value.weight\", \"blocks.21.self_attention.heads.11.tril\", \"blocks.21.self_attention.heads.11.key.weight\", \"blocks.21.self_attention.heads.11.query.weight\", \"blocks.21.self_attention.heads.11.value.weight\", \"blocks.21.self_attention.heads.12.tril\", \"blocks.21.self_attention.heads.12.key.weight\", \"blocks.21.self_attention.heads.12.query.weight\", \"blocks.21.self_attention.heads.12.value.weight\", \"blocks.21.self_attention.heads.13.tril\", \"blocks.21.self_attention.heads.13.key.weight\", \"blocks.21.self_attention.heads.13.query.weight\", \"blocks.21.self_attention.heads.13.value.weight\", \"blocks.21.self_attention.heads.14.tril\", \"blocks.21.self_attention.heads.14.key.weight\", \"blocks.21.self_attention.heads.14.query.weight\", \"blocks.21.self_attention.heads.14.value.weight\", \"blocks.21.self_attention.heads.15.tril\", \"blocks.21.self_attention.heads.15.key.weight\", \"blocks.21.self_attention.heads.15.query.weight\", \"blocks.21.self_attention.heads.15.value.weight\", \"blocks.21.self_attention.projection.weight\", \"blocks.21.self_attention.projection.bias\", \"blocks.21.feed_forward.net.0.weight\", \"blocks.21.feed_forward.net.0.bias\", \"blocks.21.feed_forward.net.2.weight\", \"blocks.21.feed_forward.net.2.bias\", \"blocks.21.layer_norm_1.weight\", \"blocks.21.layer_norm_1.bias\", \"blocks.21.layer_norm_2.weight\", \"blocks.21.layer_norm_2.bias\", \"blocks.22.self_attention.heads.0.tril\", \"blocks.22.self_attention.heads.0.key.weight\", \"blocks.22.self_attention.heads.0.query.weight\", \"blocks.22.self_attention.heads.0.value.weight\", \"blocks.22.self_attention.heads.1.tril\", \"blocks.22.self_attention.heads.1.key.weight\", \"blocks.22.self_attention.heads.1.query.weight\", \"blocks.22.self_attention.heads.1.value.weight\", \"blocks.22.self_attention.heads.2.tril\", \"blocks.22.self_attention.heads.2.key.weight\", \"blocks.22.self_attention.heads.2.query.weight\", \"blocks.22.self_attention.heads.2.value.weight\", \"blocks.22.self_attention.heads.3.tril\", \"blocks.22.self_attention.heads.3.key.weight\", \"blocks.22.self_attention.heads.3.query.weight\", \"blocks.22.self_attention.heads.3.value.weight\", \"blocks.22.self_attention.heads.4.tril\", \"blocks.22.self_attention.heads.4.key.weight\", \"blocks.22.self_attention.heads.4.query.weight\", \"blocks.22.self_attention.heads.4.value.weight\", \"blocks.22.self_attention.heads.5.tril\", \"blocks.22.self_attention.heads.5.key.weight\", \"blocks.22.self_attention.heads.5.query.weight\", \"blocks.22.self_attention.heads.5.value.weight\", \"blocks.22.self_attention.heads.6.tril\", \"blocks.22.self_attention.heads.6.key.weight\", \"blocks.22.self_attention.heads.6.query.weight\", \"blocks.22.self_attention.heads.6.value.weight\", \"blocks.22.self_attention.heads.7.tril\", \"blocks.22.self_attention.heads.7.key.weight\", \"blocks.22.self_attention.heads.7.query.weight\", \"blocks.22.self_attention.heads.7.value.weight\", \"blocks.22.self_attention.heads.8.tril\", \"blocks.22.self_attention.heads.8.key.weight\", \"blocks.22.self_attention.heads.8.query.weight\", \"blocks.22.self_attention.heads.8.value.weight\", \"blocks.22.self_attention.heads.9.tril\", \"blocks.22.self_attention.heads.9.key.weight\", \"blocks.22.self_attention.heads.9.query.weight\", \"blocks.22.self_attention.heads.9.value.weight\", \"blocks.22.self_attention.heads.10.tril\", \"blocks.22.self_attention.heads.10.key.weight\", \"blocks.22.self_attention.heads.10.query.weight\", \"blocks.22.self_attention.heads.10.value.weight\", \"blocks.22.self_attention.heads.11.tril\", \"blocks.22.self_attention.heads.11.key.weight\", \"blocks.22.self_attention.heads.11.query.weight\", \"blocks.22.self_attention.heads.11.value.weight\", \"blocks.22.self_attention.heads.12.tril\", \"blocks.22.self_attention.heads.12.key.weight\", \"blocks.22.self_attention.heads.12.query.weight\", \"blocks.22.self_attention.heads.12.value.weight\", \"blocks.22.self_attention.heads.13.tril\", \"blocks.22.self_attention.heads.13.key.weight\", \"blocks.22.self_attention.heads.13.query.weight\", \"blocks.22.self_attention.heads.13.value.weight\", \"blocks.22.self_attention.heads.14.tril\", \"blocks.22.self_attention.heads.14.key.weight\", \"blocks.22.self_attention.heads.14.query.weight\", \"blocks.22.self_attention.heads.14.value.weight\", \"blocks.22.self_attention.heads.15.tril\", \"blocks.22.self_attention.heads.15.key.weight\", \"blocks.22.self_attention.heads.15.query.weight\", \"blocks.22.self_attention.heads.15.value.weight\", \"blocks.22.self_attention.projection.weight\", \"blocks.22.self_attention.projection.bias\", \"blocks.22.feed_forward.net.0.weight\", \"blocks.22.feed_forward.net.0.bias\", \"blocks.22.feed_forward.net.2.weight\", \"blocks.22.feed_forward.net.2.bias\", \"blocks.22.layer_norm_1.weight\", \"blocks.22.layer_norm_1.bias\", \"blocks.22.layer_norm_2.weight\", \"blocks.22.layer_norm_2.bias\", \"blocks.23.self_attention.heads.0.tril\", \"blocks.23.self_attention.heads.0.key.weight\", \"blocks.23.self_attention.heads.0.query.weight\", \"blocks.23.self_attention.heads.0.value.weight\", \"blocks.23.self_attention.heads.1.tril\", \"blocks.23.self_attention.heads.1.key.weight\", \"blocks.23.self_attention.heads.1.query.weight\", \"blocks.23.self_attention.heads.1.value.weight\", \"blocks.23.self_attention.heads.2.tril\", \"blocks.23.self_attention.heads.2.key.weight\", \"blocks.23.self_attention.heads.2.query.weight\", \"blocks.23.self_attention.heads.2.value.weight\", \"blocks.23.self_attention.heads.3.tril\", \"blocks.23.self_attention.heads.3.key.weight\", \"blocks.23.self_attention.heads.3.query.weight\", \"blocks.23.self_attention.heads.3.value.weight\", \"blocks.23.self_attention.heads.4.tril\", \"blocks.23.self_attention.heads.4.key.weight\", \"blocks.23.self_attention.heads.4.query.weight\", \"blocks.23.self_attention.heads.4.value.weight\", \"blocks.23.self_attention.heads.5.tril\", \"blocks.23.self_attention.heads.5.key.weight\", \"blocks.23.self_attention.heads.5.query.weight\", \"blocks.23.self_attention.heads.5.value.weight\", \"blocks.23.self_attention.heads.6.tril\", \"blocks.23.self_attention.heads.6.key.weight\", \"blocks.23.self_attention.heads.6.query.weight\", \"blocks.23.self_attention.heads.6.value.weight\", \"blocks.23.self_attention.heads.7.tril\", \"blocks.23.self_attention.heads.7.key.weight\", \"blocks.23.self_attention.heads.7.query.weight\", \"blocks.23.self_attention.heads.7.value.weight\", \"blocks.23.self_attention.heads.8.tril\", \"blocks.23.self_attention.heads.8.key.weight\", \"blocks.23.self_attention.heads.8.query.weight\", \"blocks.23.self_attention.heads.8.value.weight\", \"blocks.23.self_attention.heads.9.tril\", \"blocks.23.self_attention.heads.9.key.weight\", \"blocks.23.self_attention.heads.9.query.weight\", \"blocks.23.self_attention.heads.9.value.weight\", \"blocks.23.self_attention.heads.10.tril\", \"blocks.23.self_attention.heads.10.key.weight\", \"blocks.23.self_attention.heads.10.query.weight\", \"blocks.23.self_attention.heads.10.value.weight\", \"blocks.23.self_attention.heads.11.tril\", \"blocks.23.self_attention.heads.11.key.weight\", \"blocks.23.self_attention.heads.11.query.weight\", \"blocks.23.self_attention.heads.11.value.weight\", \"blocks.23.self_attention.heads.12.tril\", \"blocks.23.self_attention.heads.12.key.weight\", \"blocks.23.self_attention.heads.12.query.weight\", \"blocks.23.self_attention.heads.12.value.weight\", \"blocks.23.self_attention.heads.13.tril\", \"blocks.23.self_attention.heads.13.key.weight\", \"blocks.23.self_attention.heads.13.query.weight\", \"blocks.23.self_attention.heads.13.value.weight\", \"blocks.23.self_attention.heads.14.tril\", \"blocks.23.self_attention.heads.14.key.weight\", \"blocks.23.self_attention.heads.14.query.weight\", \"blocks.23.self_attention.heads.14.value.weight\", \"blocks.23.self_attention.heads.15.tril\", \"blocks.23.self_attention.heads.15.key.weight\", \"blocks.23.self_attention.heads.15.query.weight\", \"blocks.23.self_attention.heads.15.value.weight\", \"blocks.23.self_attention.projection.weight\", \"blocks.23.self_attention.projection.bias\", \"blocks.23.feed_forward.net.0.weight\", \"blocks.23.feed_forward.net.0.bias\", \"blocks.23.feed_forward.net.2.weight\", \"blocks.23.feed_forward.net.2.bias\", \"blocks.23.layer_norm_1.weight\", \"blocks.23.layer_norm_1.bias\", \"blocks.23.layer_norm_2.weight\", \"blocks.23.layer_norm_2.bias\". \n\tsize mismatch for token_embedding_table.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024]).\n\tsize mismatch for position_embedding_table.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_linear_layer.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m checkpoint = torch.load(checkpoint_path, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m model_state_dict = checkpoint[\u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2576\u001b[39m         error_msgs.insert(\n\u001b[32m   2577\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2578\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2579\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2580\u001b[39m             ),\n\u001b[32m   2581\u001b[39m         )\n\u001b[32m   2583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2586\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2587\u001b[39m         )\n\u001b[32m   2588\u001b[39m     )\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for GPTLanguageModel:\n\tMissing key(s) in state_dict: \"blocks.6.self_attention.heads.0.tril\", \"blocks.6.self_attention.heads.0.key.weight\", \"blocks.6.self_attention.heads.0.query.weight\", \"blocks.6.self_attention.heads.0.value.weight\", \"blocks.6.self_attention.heads.1.tril\", \"blocks.6.self_attention.heads.1.key.weight\", \"blocks.6.self_attention.heads.1.query.weight\", \"blocks.6.self_attention.heads.1.value.weight\", \"blocks.6.self_attention.heads.2.tril\", \"blocks.6.self_attention.heads.2.key.weight\", \"blocks.6.self_attention.heads.2.query.weight\", \"blocks.6.self_attention.heads.2.value.weight\", \"blocks.6.self_attention.heads.3.tril\", \"blocks.6.self_attention.heads.3.key.weight\", \"blocks.6.self_attention.heads.3.query.weight\", \"blocks.6.self_attention.heads.3.value.weight\", \"blocks.6.self_attention.heads.4.tril\", \"blocks.6.self_attention.heads.4.key.weight\", \"blocks.6.self_attention.heads.4.query.weight\", \"blocks.6.self_attention.heads.4.value.weight\", \"blocks.6.self_attention.heads.5.tril\", \"blocks.6.self_attention.heads.5.key.weight\", \"blocks.6.self_attention.heads.5.query.weight\", \"blocks.6.self_attention.heads.5.value.weight\", \"blocks.6.self_attention.heads.6.tril\", \"blocks.6.self_attention.heads.6.key.weight\", \"blocks.6.self_attention.heads.6.query.weight\", \"blocks.6.self_attention.heads.6.value.weight\", \"blocks.6.self_attention.heads.7.tril\", \"blocks.6.self_attention.heads.7.key.weight\", \"blocks.6.self_attention.heads.7.query.weight\", \"blocks.6.self_attention.heads.7.value.weight\", \"blocks.6.self_attention.heads.8.tril\", \"blocks.6.self_attention.heads.8.key.weight\", \"blocks.6.self_attention.heads.8.query.weight\", \"blocks.6.self_attention.heads.8.value.weight\", \"blocks.6.self_attention.heads.9.tril\", \"blocks.6.self_attention.heads.9.key.weight\", \"blocks.6.self_attention.heads.9.query.weight\", \"blocks.6.self_attention.heads.9.value.weight\", \"blocks.6.self_attention.heads.10.tril\", \"blocks.6.self_attention.heads.10.key.weight\", \"blocks.6.self_attention.heads.10.query.weight\", \"blocks.6.self_attention.heads.10.value.weight\", \"blocks.6.self_attention.heads.11.tril\", \"blocks.6.self_attention.heads.11.key.weight\", \"blocks.6.self_attention.heads.11.query.weight\", \"blocks.6.self_attention.heads.11.value.weight\", \"blocks.6.self_attention.heads.12.tril\", \"blocks.6.self_attention.heads.12.key.weight\", \"blocks.6.self_attention.heads.12.query.weight\", \"blocks.6.self_attention.heads.12.value.weight\", \"blocks.6.self_attention.heads.13.tril\", \"blocks.6.self_attention.heads.13.key.weight\", \"blocks.6.self_attention.heads.13.query.weight\", \"blocks.6.self_attention.heads.13.value.weight\", \"blocks.6.self_attention.heads.14.tril\", \"blocks.6.self_attention.heads.14.key.weight\", \"blocks.6.self_attention.heads.14.query.weight\", \"blocks.6.self_attention.heads.14.value.weight\", \"blocks.6.self_attention.heads.15.tril\", \"blocks.6.self_attention.heads.15.key.weight\", \"blocks.6.self_attention.heads.15.query.weight\", \"blocks.6.self_attention.heads.15.value.weight\", \"blocks.6.self_attention.projection.weight\", \"blocks.6.self_attention.projection.bias\", \"blocks.6.feed_forward.net.0.weight\", \"blocks.6.feed_forward.net.0.bias\", \"blocks.6.feed_forward.net.2.weight\", \"blocks.6.feed_forward.net.2.bias\", \"blocks.6.layer_norm_1.weight\", \"blocks.6.layer_norm_1.bias\", \"blocks.6.layer_norm_2.weight\", \"blocks.6.layer_norm_2.bias\", \"blocks.7.self_attention.heads.0.tril\", \"blocks.7.self_attention.heads.0.key.weight\", \"blocks.7.self_attention.heads.0.query.weight\", \"blocks.7.self_attention.heads.0.value.weight\", \"blocks.7.self_attention.heads.1.tril\", \"blocks.7.self_attention.heads.1.key.weight\", \"blocks.7.self_attention.heads.1.query.weight\", \"blocks.7.self_attention.heads.1.value.weight\", \"blocks.7.self_attention.heads.2.tril\", \"blocks.7.self_attention.heads.2.key.weight\", \"blocks.7.self_attention.heads.2.query.weight\", \"blocks.7.self_attention.heads.2.value.weight\", \"blocks.7.self_attention.heads.3.tril\", \"blocks.7.self_attention.heads.3.key.weight\", \"blocks.7.self_attention.heads.3.query.weight\", \"blocks.7.self_attention.heads.3.value.weight\", \"blocks.7.self_attention.heads.4.tril\", \"blocks.7.self_attention.heads.4.key.weight\", \"blocks.7.self_attention.heads.4.query.weight\", \"blocks.7.self_attention.heads.4.value.weight\", \"blocks.7.self_attention.heads.5.tril\", \"blocks.7.self_attention.heads.5.key.weight\", \"blocks.7.self_attention.heads.5.query.weight\", \"blocks.7.self_attention.heads.5.value.weight\", \"blocks.7.self_attention.heads.6.tril\", \"blocks.7.self_attention.heads.6.key.weight\", \"blocks.7.self_attention.heads.6.query.weight\", \"blocks.7.self_attention.heads.6.value.weight\", \"blocks.7.self_attention.heads.7.tril\", \"blocks.7.self_attention.heads.7.key.weight\", \"blocks.7.self_attention.heads.7.query.weight\", \"blocks.7.self_attention.heads.7.value.weight\", \"blocks.7.self_attention.heads.8.tril\", \"blocks.7.self_attention.heads.8.key.weight\", \"blocks.7.self_attention.heads.8.query.weight\", \"blocks.7.self_attention.heads.8.value.weight\", \"blocks.7.self_attention.heads.9.tril\", \"blocks.7.self_attention.heads.9.key.weight\", \"blocks.7.self_attention.heads.9.query.weight\", \"blocks.7.self_attention.heads.9.value.weight\", \"blocks.7.self_attention.heads.10.tril\", \"blocks.7.self_attention.heads.10.key.weight\", \"blocks.7.self_attention.heads.10.query.weight\", \"blocks.7.self_attention.heads.10.value.weight\", \"blocks.7.self_attention.heads.11.tril\", \"blocks.7.self_attention.heads.11.key.weight\", \"blocks.7.self_attention.heads.11.query.weight\", \"blocks.7.self_attention.heads.11.value.weight\", \"blocks.7.self_attention.heads.12.tril\", \"blocks.7.self_attention.heads.12.key.weight\", \"blocks.7.self_attention.heads.12.query.weight\", \"blocks.7.self_attention.heads.12.value.weight\", \"blocks.7.self_attention.heads.13.tril\", \"blocks.7.self_attention.heads.13.key.weight\", \"blocks.7.self_attention.heads.13.query.weight\", \"blocks.7.self_attention.heads.13.value.weight\", \"blocks.7.self_attention.heads.14.tril\", \"blocks.7.self_attention.heads.14.key.weight\", \"blocks.7.self_attention.heads.14.query.weight\", \"blocks.7.self_attention.heads.14.value.weight\", \"blocks.7.self_attention.heads.15.tril\", \"blocks.7.self_attention.heads.15.key.weight\", \"blocks.7.self_attention.heads.15.query.weight\", \"blocks.7.self_attention.heads.15.value.weight\", \"blocks.7.self_attention.projection.weight\", \"blocks.7.self_attention.projection.bias\", \"blocks.7.feed_forward.net.0.weight\", \"blocks.7.feed_forward.net.0.bias\", \"blocks.7.feed_forward.net.2.weight\", \"blocks.7.feed_forward.net.2.bias\", \"blocks.7.layer_norm_1.weight\", \"blocks.7.layer_norm_1.bias\", \"blocks.7.layer_norm_2.weight\", \"blocks.7.layer_norm_2.bias\", \"blocks.8.self_attention.heads.0.tril\", \"blocks.8.self_attention.heads.0.key.weight\", \"blocks.8.self_attention.heads.0.query.weight\", \"blocks.8.self_attention.heads.0.value.weight\", \"blocks.8.self_attention.heads.1.tril\", \"blocks.8.self_attention.heads.1.key.weight\", \"blocks.8.self_attention.heads.1.query.weight\", \"blocks.8.self_attention.heads.1.value.weight\", \"blocks.8.self_attention.heads.2.tril\", \"blocks.8.self_attention.heads.2.key.weight\", \"blocks.8.self_attention.heads.2.query.weight\", \"blocks.8.self_attention.heads.2.value.weight\", \"blocks.8.self_attention.heads.3.tril\", \"blocks.8.self_attention.heads.3.key.weight\", \"blocks.8.self_attention.heads.3.query.weight\", \"blocks.8.self_attention.heads.3.value.weight\", \"blocks.8.self_attention.heads.4.tril\", \"blocks.8.self_attention.heads.4.key.weight\", \"blocks.8.self_attention.heads.4.query.weight\", \"blocks.8.self_attention.heads.4.value.weight\", \"blocks.8.self_attention.heads.5.tril\", \"blocks.8.self_attention.heads.5.key.weight\", \"blocks.8.self_attention.heads.5.query.weight\", \"blocks.8.self_attention.heads.5.value.weight\", \"blocks.8.self_attention.heads.6.tril\", \"blocks.8.self_attention.heads.6.key.weight\", \"blocks.8.self_attention.heads.6.query.weight\", \"blocks.8.self_attention.heads.6.value.weight\", \"blocks.8.self_attention.heads.7.tril\", \"blocks.8.self_attention.heads.7.key.weight\", \"blocks.8.self_attention.heads.7.query.weight\", \"blocks.8.self_attention.heads.7.value.weight\", \"blocks.8.self_attention.heads.8.tril\", \"blocks.8.self_attention.heads.8.key.weight\", \"blocks.8.self_attention.heads.8.query.weight\", \"blocks.8.self_attention.heads.8.value.weight\", \"blocks.8.self_attention.heads.9.tril\", \"blocks.8.self_attention.heads.9.key.weight\", \"blocks.8.self_attention.heads.9.query.weight\", \"blocks.8.self_attention.heads.9.value.weight\", \"blocks.8.self_attention.heads.10.tril\", \"blocks.8.self_attention.heads.10.key.weight\", \"blocks.8.self_attention.heads.10.query.weight\", \"blocks.8.self_attention.heads.10.value.weight\", \"blocks.8.self_attention.heads.11.tril\", \"blocks.8.self_attention.heads.11.key.weight\", \"blocks.8.self_attention.heads.11.query.weight\", \"blocks.8.self_attention.heads.11.value.weight\", \"blocks.8.self_attention.heads.12.tril\", \"blocks.8.self_attention.heads.12.key.weight\", \"blocks.8.self_attention.heads.12.query.weight\", \"blocks.8.self_attention.heads.12.value.weight\", \"blocks.8.self_attention.heads.13.tril\", \"blocks.8.self_attention.heads.13.key.weight\", \"blocks.8.self_attention.heads.13.query.weight\", \"blocks.8.self_attention.heads.13.value.weight\", \"blocks.8.self_attention.heads.14.tril\", \"blocks.8.self_attention.heads.14.key.weight\", \"blocks.8.self_attention.heads.14.query.weight\", \"blocks.8.self_attention.heads.14.value.weight\", \"blocks.8.self_attention.heads.15.tril\", \"blocks.8.self_attention.heads.15.key.weight\", \"blocks.8.self_attention.heads.15.query.weight\", \"blocks.8.self_attention.heads.15.value.weight\", \"blocks.8.self_attention.projection.weight\", \"blocks.8.self_attention.projection.bias\", \"blocks.8.feed_forward.net.0.weight\", \"blocks.8.feed_forward.net.0.bias\", \"blocks.8.feed_forward.net.2.weight\", \"blocks.8.feed_forward.net.2.bias\", \"blocks.8.layer_norm_1.weight\", \"blocks.8.layer_norm_1.bias\", \"blocks.8.layer_norm_2.weight\", \"blocks.8.layer_norm_2.bias\", \"blocks.9.self_attention.heads.0.tril\", \"blocks.9.self_attention.heads.0.key.weight\", \"blocks.9.self_attention.heads.0.query.weight\", \"blocks.9.self_attention.heads.0.value.weight\", \"blocks.9.self_attention.heads.1.tril\", \"blocks.9.self_attention.heads.1.key.weight\", \"blocks.9.self_attention.heads.1.query.weight\", \"blocks.9.self_attention.heads.1.value.weight\", \"blocks.9.self_attention.heads.2.tril\", \"blocks.9.self_attention.heads.2.key.weight\", \"blocks.9.self_attention.heads.2.query.weight\", \"blocks.9.self_attention.heads.2.value.weight\", \"blocks.9.self_attention.heads.3.tril\", \"blocks.9.self_attention.heads.3.key.weight\", \"blocks.9.self_attention.heads.3.query.weight\", \"blocks.9.self_attention.heads.3.value.weight\", \"blocks.9.self_attention.heads.4.tril\", \"blocks.9.self_attention.heads.4.key.weight\", \"blocks.9.self_attention.heads.4.query.weight\", \"blocks.9.self_attention.heads.4.value.weight\", \"blocks.9.self_attention.heads.5.tril\", \"blocks.9.self_attention.heads.5.key.weight\", \"blocks.9.self_attention.heads.5.query.weight\", \"blocks.9.self_attention.heads.5.value.weight\", \"blocks.9.self_attention.heads.6.tril\", \"blocks.9.self_attention.heads.6.key.weight\", \"blocks.9.self_attention.heads.6.query.weight\", \"blocks.9.self_attention.heads.6.value.weight\", \"blocks.9.self_attention.heads.7.tril\", \"blocks.9.self_attention.heads.7.key.weight\", \"blocks.9.self_attention.heads.7.query.weight\", \"blocks.9.self_attention.heads.7.value.weight\", \"blocks.9.self_attention.heads.8.tril\", \"blocks.9.self_attention.heads.8.key.weight\", \"blocks.9.self_attention.heads.8.query.weight\", \"blocks.9.self_attention.heads.8.value.weight\", \"blocks.9.self_attention.heads.9.tril\", \"blocks.9.self_attention.heads.9.key.weight\", \"blocks.9.self_attention.heads.9.query.weight\", \"blocks.9.self_attention.heads.9.value.weight\", \"blocks.9.self_attention.heads.10.tril\", \"blocks.9.self_attention.heads.10.key.weight\", \"blocks.9.self_attention.heads.10.query.weight\", \"blocks.9.self_attention.heads.10.value.weight\", \"blocks.9.self_attention.heads.11.tril\", \"blocks.9.self_attention.heads.11.key.weight\", \"blocks.9.self_attention.heads.11.query.weight\", \"blocks.9.self_attention.heads.11.value.weight\", \"blocks.9.self_attention.heads.12.tril\", \"blocks.9.self_attention.heads.12.key.weight\", \"blocks.9.self_attention.heads.12.query.weight\", \"blocks.9.self_attention.heads.12.value.weight\", \"blocks.9.self_attention.heads.13.tril\", \"blocks.9.self_attention.heads.13.key.weight\", \"blocks.9.self_attention.heads.13.query.weight\", \"blocks.9.self_attention.heads.13.value.weight\", \"blocks.9.self_attention.heads.14.tril\", \"blocks.9.self_attention.heads.14.key.weight\", \"blocks.9.self_attention.heads.14.query.weight\", \"blocks.9.self_attention.heads.14.value.weight\", \"blocks.9.self_attention.heads.15.tril\", \"blocks.9.self_attention.heads.15.key.weight\", \"blocks.9.self_attention.heads.15.query.weight\", \"blocks.9.self_attention.heads.15.value.weight\", \"blocks.9.self_attention.projection.weight\", \"blocks.9.self_attention.projection.bias\", \"blocks.9.feed_forward.net.0.weight\", \"blocks.9.feed_forward.net.0.bias\", \"blocks.9.feed_forward.net.2.weight\", \"blocks.9.feed_forward.net.2.bias\", \"blocks.9.layer_norm_1.weight\", \"blocks.9.layer_norm_1.bias\", \"blocks.9.layer_norm_2.weight\", \"blocks.9.layer_norm_2.bias\", \"blocks.10.self_attention.heads.0.tril\", \"blocks.10.self_attention.heads.0.key.weight\", \"blocks.10.self_attention.heads.0.query.weight\", \"blocks.10.self_attention.heads.0.value.weight\", \"blocks.10.self_attention.heads.1.tril\", \"blocks.10.self_attention.heads.1.key.weight\", \"blocks.10.self_attention.heads.1.query.weight\", \"blocks.10.self_attention.heads.1.value.weight\", \"blocks.10.self_attention.heads.2.tril\", \"blocks.10.self_attention.heads.2.key.weight\", \"blocks.10.self_attention.heads.2.query.weight\", \"blocks.10.self_attention.heads.2.value.weight\", \"blocks.10.self_attention.heads.3.tril\", \"blocks.10.self_attention.heads.3.key.weight\", \"blocks.10.self_attention.heads.3.query.weight\", \"blocks.10.self_attention.heads.3.value.weight\", \"blocks.10.self_attention.heads.4.tril\", \"blocks.10.self_attention.heads.4.key.weight\", \"blocks.10.self_attention.heads.4.query.weight\", \"blocks.10.self_attention.heads.4.value.weight\", \"blocks.10.self_attention.heads.5.tril\", \"blocks.10.self_attention.heads.5.key.weight\", \"blocks.10.self_attention.heads.5.query.weight\", \"blocks.10.self_attention.heads.5.value.weight\", \"blocks.10.self_attention.heads.6.tril\", \"blocks.10.self_attention.heads.6.key.weight\", \"blocks.10.self_attention.heads.6.query.weight\", \"blocks.10.self_attention.heads.6.value.weight\", \"blocks.10.self_attention.heads.7.tril\", \"blocks.10.self_attention.heads.7.key.weight\", \"blocks.10.self_attention.heads.7.query.weight\", \"blocks.10.self_attention.heads.7.value.weight\", \"blocks.10.self_attention.heads.8.tril\", \"blocks.10.self_attention.heads.8.key.weight\", \"blocks.10.self_attention.heads.8.query.weight\", \"blocks.10.self_attention.heads.8.value.weight\", \"blocks.10.self_attention.heads.9.tril\", \"blocks.10.self_attention.heads.9.key.weight\", \"blocks.10.self_attention.heads.9.query.weight\", \"blocks.10.self_attention.heads.9.value.weight\", \"blocks.10.self_attention.heads.10.tril\", \"blocks.10.self_attention.heads.10.key.weight\", \"blocks.10.self_attention.heads.10.query.weight\", \"blocks.10.self_attention.heads.10.value.weight\", \"blocks.10.self_attention.heads.11.tril\", \"blocks.10.self_attention.heads.11.key.weight\", \"blocks.10.self_attention.heads.11.query.weight\", \"blocks.10.self_attention.heads.11.value.weight\", \"blocks.10.self_attention.heads.12.tril\", \"blocks.10.self_attention.heads.12.key.weight\", \"blocks.10.self_attention.heads.12.query.weight\", \"blocks.10.self_attention.heads.12.value.weight\", \"blocks.10.self_attention.heads.13.tril\", \"blocks.10.self_attention.heads.13.key.weight\", \"blocks.10.self_attention.heads.13.query.weight\", \"blocks.10.self_attention.heads.13.value.weight\", \"blocks.10.self_attention.heads.14.tril\", \"blocks.10.self_attention.heads.14.key.weight\", \"blocks.10.self_attention.heads.14.query.weight\", \"blocks.10.self_attention.heads.14.value.weight\", \"blocks.10.self_attention.heads.15.tril\", \"blocks.10.self_attention.heads.15.key.weight\", \"blocks.10.self_attention.heads.15.query.weight\", \"blocks.10.self_attention.heads.15.value.weight\", \"blocks.10.self_attention.projection.weight\", \"blocks.10.self_attention.projection.bias\", \"blocks.10.feed_forward.net.0.weight\", \"blocks.10.feed_forward.net.0.bias\", \"blocks.10.feed_forward.net.2.weight\", \"blocks.10.feed_forward.net.2.bias\", \"blocks.10.layer_norm_1.weight\", \"blocks.10.layer_norm_1.bias\", \"blocks.10.layer_norm_2.weight\", \"blocks.10.layer_norm_2.bias\", \"blocks.11.self_attention.heads.0.tril\", \"blocks.11.self_attention.heads.0.key.weight\", \"blocks.11.self_attention.heads.0.query.weight\", \"blocks.11.self_attention.heads.0.value.weight\", \"blocks.11.self_attention.heads.1.tril\", \"blocks.11.self_attention.heads.1.key.weight\", \"blocks.11.self_attention.heads.1.query.weight\", \"blocks.11.self_attention.heads.1.value.weight\", \"blocks.11.self_attention.heads.2.tril\", \"blocks.11.self_attention.heads.2.key.weight\", \"blocks.11.self_attention.heads.2.query.weight\", \"blocks.11.self_attention.heads.2.value.weight\", \"blocks.11.self_attention.heads.3.tril\", \"blocks.11.self_attention.heads.3.key.weight\", \"blocks.11.self_attention.heads.3.query.weight\", \"blocks.11.self_attention.heads.3.value.weight\", \"blocks.11.self_attention.heads.4.tril\", \"blocks.11.self_attention.heads.4.key.weight\", \"blocks.11.self_attention.heads.4.query.weight\", \"blocks.11.self_attention.heads.4.value.weight\", \"blocks.11.self_attention.heads.5.tril\", \"blocks.11.self_attention.heads.5.key.weight\", \"blocks.11.self_attention.heads.5.query.weight\", \"blocks.11.self_attention.heads.5.value.weight\", \"blocks.11.self_attention.heads.6.tril\", \"blocks.11.self_attention.heads.6.key.weight\", \"blocks.11.self_attention.heads.6.query.weight\", \"blocks.11.self_attention.heads.6.value.weight\", \"blocks.11.self_attention.heads.7.tril\", \"blocks.11.self_attention.heads.7.key.weight\", \"blocks.11.self_attention.heads.7.query.weight\", \"blocks.11.self_attention.heads.7.value.weight\", \"blocks.11.self_attention.heads.8.tril\", \"blocks.11.self_attention.heads.8.key.weight\", \"blocks.11.self_attention.heads.8.query.weight\", \"blocks.11.self_attention.heads.8.value.weight\", \"blocks.11.self_attention.heads.9.tril\", \"blocks.11.self_attention.heads.9.key.weight\", \"blocks.11.self_attention.heads.9.query.weight\", \"blocks.11.self_attention.heads.9.value.weight\", \"blocks.11.self_attention.heads.10.tril\", \"blocks.11.self_attention.heads.10.key.weight\", \"blocks.11.self_attention.heads.10.query.weight\", \"blocks.11.self_attention.heads.10.value.weight\", \"blocks.11.self_attention.heads.11.tril\", \"blocks.11.self_attention.heads.11.key.weight\", \"blocks.11.self_attention.heads.11.query.weight\", \"blocks.11.self_attention.heads.11.value.weight\", \"blocks.11.self_attention.heads.12.tril\", \"blocks.11.self_attention.heads.12.key.weight\", \"blocks.11.self_attention.heads.12.query.weight\", \"blocks.11.self_attention.heads.12.value.weight\", \"blocks.11.self_attention.heads.13.tril\", \"blocks.11.self_attention.heads.13.key.weight\", \"blocks.11.self_attention.heads.13.query.weight\", \"blocks.11.self_attention.heads.13.value.weight\", \"blocks.11.self_attention.heads.14.tril\", \"blocks.11.self_attention.heads.14.key.weight\", \"blocks.11.self_attention.heads.14.query.weight\", \"blocks.11.self_attention.heads.14.value.weight\", \"blocks.11.self_attention.heads.15.tril\", \"blocks.11.self_attention.heads.15.key.weight\", \"blocks.11.self_attention.heads.15.query.weight\", \"blocks.11.self_attention.heads.15.value.weight\", \"blocks.11.self_attention.projection.weight\", \"blocks.11.self_attention.projection.bias\", \"blocks.11.feed_forward.net.0.weight\", \"blocks.11.feed_forward.net.0.bias\", \"blocks.11.feed_forward.net.2.weight\", \"blocks.11.feed_forward.net.2.bias\", \"blocks.11.layer_norm_1.weight\", \"blocks.11.layer_norm_1.bias\", \"blocks.11.layer_norm_2.weight\", \"blocks.11.layer_norm_2.bias\", \"blocks.12.self_attention.heads.0.tril\", \"blocks.12.self_attention.heads.0.key.weight\", \"blocks.12.self_attention.heads.0.query.weight\", \"blocks.12.self_attention.heads.0.value.weight\", \"blocks.12.self_attention.heads.1.tril\", \"blocks.12.self_attention.heads.1.key.weight\", \"blocks.12.self_attention.heads.1.query.weight\", \"blocks.12.self_attention.heads.1.value.weight\", \"blocks.12.self_attention.heads.2.tril\", \"blocks.12.self_attention.heads.2.key.weight\", \"blocks.12.self_attention.heads.2.query.weight\", \"blocks.12.self_attention.heads.2.value.weight\", \"blocks.12.self_attention.heads.3.tril\", \"blocks.12.self_attention.heads.3.key.weight\", \"blocks.12.self_attention.heads.3.query.weight\", \"blocks.12.self_attention.heads.3.value.weight\", \"blocks.12.self_attention.heads.4.tril\", \"blocks.12.self_attention.heads.4.key.weight\", \"blocks.12.self_attention.heads.4.query.weight\", \"blocks.12.self_attention.heads.4.value.weight\", \"blocks.12.self_attention.heads.5.tril\", \"blocks.12.self_attention.heads.5.key.weight\", \"blocks.12.self_attention.heads.5.query.weight\", \"blocks.12.self_attention.heads.5.value.weight\", \"blocks.12.self_attention.heads.6.tril\", \"blocks.12.self_attention.heads.6.key.weight\", \"blocks.12.self_attention.heads.6.query.weight\", \"blocks.12.self_attention.heads.6.value.weight\", \"blocks.12.self_attention.heads.7.tril\", \"blocks.12.self_attention.heads.7.key.weight\", \"blocks.12.self_attention.heads.7.query.weight\", \"blocks.12.self_attention.heads.7.value.weight\", \"blocks.12.self_attention.heads.8.tril\", \"blocks.12.self_attention.heads.8.key.weight\", \"blocks.12.self_attention.heads.8.query.weight\", \"blocks.12.self_attention.heads.8.value.weight\", \"blocks.12.self_attention.heads.9.tril\", \"blocks.12.self_attention.heads.9.key.weight\", \"blocks.12.self_attention.heads.9.query.weight\", \"blocks.12.self_attention.heads.9.value.weight\", \"blocks.12.self_attention.heads.10.tril\", \"blocks.12.self_attention.heads.10.key.weight\", \"blocks.12.self_attention.heads.10.query.weight\", \"blocks.12.self_attention.heads.10.value.weight\", \"blocks.12.self_attention.heads.11.tril\", \"blocks.12.self_attention.heads.11.key.weight\", \"blocks.12.self_attention.heads.11.query.weight\", \"blocks.12.self_attention.heads.11.value.weight\", \"blocks.12.self_attention.heads.12.tril\", \"blocks.12.self_attention.heads.12.key.weight\", \"blocks.12.self_attention.heads.12.query.weight\", \"blocks.12.self_attention.heads.12.value.weight\", \"blocks.12.self_attention.heads.13.tril\", \"blocks.12.self_attention.heads.13.key.weight\", \"blocks.12.self_attention.heads.13.query.weight\", \"blocks.12.self_attention.heads.13.value.weight\", \"blocks.12.self_attention.heads.14.tril\", \"blocks.12.self_attention.heads.14.key.weight\", \"blocks.12.self_attention.heads.14.query.weight\", \"blocks.12.self_attention.heads.14.value.weight\", \"blocks.12.self_attention.heads.15.tril\", \"blocks.12.self_attention.heads.15.key.weight\", \"blocks.12.self_attention.heads.15.query.weight\", \"blocks.12.self_attention.heads.15.value.weight\", \"blocks.12.self_attention.projection.weight\", \"blocks.12.self_attention.projection.bias\", \"blocks.12.feed_forward.net.0.weight\", \"blocks.12.feed_forward.net.0.bias\", \"blocks.12.feed_forward.net.2.weight\", \"blocks.12.feed_forward.net.2.bias\", \"blocks.12.layer_norm_1.weight\", \"blocks.12.layer_norm_1.bias\", \"blocks.12.layer_norm_2.weight\", \"blocks.12.layer_norm_2.bias\", \"blocks.13.self_attention.heads.0.tril\", \"blocks.13.self_attention.heads.0.key.weight\", \"blocks.13.self_attention.heads.0.query.weight\", \"blocks.13.self_attention.heads.0.value.weight\", \"blocks.13.self_attention.heads.1.tril\", \"blocks.13.self_attention.heads.1.key.weight\", \"blocks.13.self_attention.heads.1.query.weight\", \"blocks.13.self_attention.heads.1.value.weight\", \"blocks.13.self_attention.heads.2.tril\", \"blocks.13.self_attention.heads.2.key.weight\", \"blocks.13.self_attention.heads.2.query.weight\", \"blocks.13.self_attention.heads.2.value.weight\", \"blocks.13.self_attention.heads.3.tril\", \"blocks.13.self_attention.heads.3.key.weight\", \"blocks.13.self_attention.heads.3.query.weight\", \"blocks.13.self_attention.heads.3.value.weight\", \"blocks.13.self_attention.heads.4.tril\", \"blocks.13.self_attention.heads.4.key.weight\", \"blocks.13.self_attention.heads.4.query.weight\", \"blocks.13.self_attention.heads.4.value.weight\", \"blocks.13.self_attention.heads.5.tril\", \"blocks.13.self_attention.heads.5.key.weight\", \"blocks.13.self_attention.heads.5.query.weight\", \"blocks.13.self_attention.heads.5.value.weight\", \"blocks.13.self_attention.heads.6.tril\", \"blocks.13.self_attention.heads.6.key.weight\", \"blocks.13.self_attention.heads.6.query.weight\", \"blocks.13.self_attention.heads.6.value.weight\", \"blocks.13.self_attention.heads.7.tril\", \"blocks.13.self_attention.heads.7.key.weight\", \"blocks.13.self_attention.heads.7.query.weight\", \"blocks.13.self_attention.heads.7.value.weight\", \"blocks.13.self_attention.heads.8.tril\", \"blocks.13.self_attention.heads.8.key.weight\", \"blocks.13.self_attention.heads.8.query.weight\", \"blocks.13.self_attention.heads.8.value.weight\", \"blocks.13.self_attention.heads.9.tril\", \"blocks.13.self_attention.heads.9.key.weight\", \"blocks.13.self_attention.heads.9.query.weight\", \"blocks.13.self_attention.heads.9.value.weight\", \"blocks.13.self_attention.heads.10.tril\", \"blocks.13.self_attention.heads.10.key.weight\", \"blocks.13.self_attention.heads.10.query.weight\", \"blocks.13.self_attention.heads.10.value.weight\", \"blocks.13.self_attention.heads.11.tril\", \"blocks.13.self_attention.heads.11.key.weight\", \"blocks.13.self_attention.heads.11.query.weight\", \"blocks.13.self_attention.heads.11.value.weight\", \"blocks.13.self_attention.heads.12.tril\", \"blocks.13.self_attention.heads.12.key.weight\", \"blocks.13.self_attention.heads.12.query.weight\", \"blocks.13.self_attention.heads.12.value.weight\", \"blocks.13.self_attention.heads.13.tril\", \"blocks.13.self_attention.heads.13.key.weight\", \"blocks.13.self_attention.heads.13.query.weight\", \"blocks.13.self_attention.heads.13.value.weight\", \"blocks.13.self_attention.heads.14.tril\", \"blocks.13.self_attention.heads.14.key.weight\", \"blocks.13.self_attention.heads.14.query.weight\", \"blocks.13.self_attention.heads.14.value.weight\", \"blocks.13.self_attention.heads.15.tril\", \"blocks.13.self_attention.heads.15.key.weight\", \"blocks.13.self_attention.heads.15.query.weight\", \"blocks.13.self_attention.heads.15.value.weight\", \"blocks.13.self_attention.projection.weight\", \"blocks.13.self_attention.projection.bias\", \"blocks.13.feed_forward.net.0.weight\", \"blocks.13.feed_forward.net.0.bias\", \"blocks.13.feed_forward.net.2.weight\", \"blocks.13.feed_forward.net.2.bias\", \"blocks.13.layer_norm_1.weight\", \"blocks.13.layer_norm_1.bias\", \"blocks.13.layer_norm_2.weight\", \"blocks.13.layer_norm_2.bias\", \"blocks.14.self_attention.heads.0.tril\", \"blocks.14.self_attention.heads.0.key.weight\", \"blocks.14.self_attention.heads.0.query.weight\", \"blocks.14.self_attention.heads.0.value.weight\", \"blocks.14.self_attention.heads.1.tril\", \"blocks.14.self_attention.heads.1.key.weight\", \"blocks.14.self_attention.heads.1.query.weight\", \"blocks.14.self_attention.heads.1.value.weight\", \"blocks.14.self_attention.heads.2.tril\", \"blocks.14.self_attention.heads.2.key.weight\", \"blocks.14.self_attention.heads.2.query.weight\", \"blocks.14.self_attention.heads.2.value.weight\", \"blocks.14.self_attention.heads.3.tril\", \"blocks.14.self_attention.heads.3.key.weight\", \"blocks.14.self_attention.heads.3.query.weight\", \"blocks.14.self_attention.heads.3.value.weight\", \"blocks.14.self_attention.heads.4.tril\", \"blocks.14.self_attention.heads.4.key.weight\", \"blocks.14.self_attention.heads.4.query.weight\", \"blocks.14.self_attention.heads.4.value.weight\", \"blocks.14.self_attention.heads.5.tril\", \"blocks.14.self_attention.heads.5.key.weight\", \"blocks.14.self_attention.heads.5.query.weight\", \"blocks.14.self_attention.heads.5.value.weight\", \"blocks.14.self_attention.heads.6.tril\", \"blocks.14.self_attention.heads.6.key.weight\", \"blocks.14.self_attention.heads.6.query.weight\", \"blocks.14.self_attention.heads.6.value.weight\", \"blocks.14.self_attention.heads.7.tril\", \"blocks.14.self_attention.heads.7.key.weight\", \"blocks.14.self_attention.heads.7.query.weight\", \"blocks.14.self_attention.heads.7.value.weight\", \"blocks.14.self_attention.heads.8.tril\", \"blocks.14.self_attention.heads.8.key.weight\", \"blocks.14.self_attention.heads.8.query.weight\", \"blocks.14.self_attention.heads.8.value.weight\", \"blocks.14.self_attention.heads.9.tril\", \"blocks.14.self_attention.heads.9.key.weight\", \"blocks.14.self_attention.heads.9.query.weight\", \"blocks.14.self_attention.heads.9.value.weight\", \"blocks.14.self_attention.heads.10.tril\", \"blocks.14.self_attention.heads.10.key.weight\", \"blocks.14.self_attention.heads.10.query.weight\", \"blocks.14.self_attention.heads.10.value.weight\", \"blocks.14.self_attention.heads.11.tril\", \"blocks.14.self_attention.heads.11.key.weight\", \"blocks.14.self_attention.heads.11.query.weight\", \"blocks.14.self_attention.heads.11.value.weight\", \"blocks.14.self_attention.heads.12.tril\", \"blocks.14.self_attention.heads.12.key.weight\", \"blocks.14.self_attention.heads.12.query.weight\", \"blocks.14.self_attention.heads.12.value.weight\", \"blocks.14.self_attention.heads.13.tril\", \"blocks.14.self_attention.heads.13.key.weight\", \"blocks.14.self_attention.heads.13.query.weight\", \"blocks.14.self_attention.heads.13.value.weight\", \"blocks.14.self_attention.heads.14.tril\", \"blocks.14.self_attention.heads.14.key.weight\", \"blocks.14.self_attention.heads.14.query.weight\", \"blocks.14.self_attention.heads.14.value.weight\", \"blocks.14.self_attention.heads.15.tril\", \"blocks.14.self_attention.heads.15.key.weight\", \"blocks.14.self_attention.heads.15.query.weight\", \"blocks.14.self_attention.heads.15.value.weight\", \"blocks.14.self_attention.projection.weight\", \"blocks.14.self_attention.projection.bias\", \"blocks.14.feed_forward.net.0.weight\", \"blocks.14.feed_forward.net.0.bias\", \"blocks.14.feed_forward.net.2.weight\", \"blocks.14.feed_forward.net.2.bias\", \"blocks.14.layer_norm_1.weight\", \"blocks.14.layer_norm_1.bias\", \"blocks.14.layer_norm_2.weight\", \"blocks.14.layer_norm_2.bias\", \"blocks.15.self_attention.heads.0.tril\", \"blocks.15.self_attention.heads.0.key.weight\", \"blocks.15.self_attention.heads.0.query.weight\", \"blocks.15.self_attention.heads.0.value.weight\", \"blocks.15.self_attention.heads.1.tril\", \"blocks.15.self_attention.heads.1.key.weight\", \"blocks.15.self_attention.heads.1.query.weight\", \"blocks.15.self_attention.heads.1.value.weight\", \"blocks.15.self_attention.heads.2.tril\", \"blocks.15.self_attention.heads.2.key.weight\", \"blocks.15.self_attention.heads.2.query.weight\", \"blocks.15.self_attention.heads.2.value.weight\", \"blocks.15.self_attention.heads.3.tril\", \"blocks.15.self_attention.heads.3.key.weight\", \"blocks.15.self_attention.heads.3.query.weight\", \"blocks.15.self_attention.heads.3.value.weight\", \"blocks.15.self_attention.heads.4.tril\", \"blocks.15.self_attention.heads.4.key.weight\", \"blocks.15.self_attention.heads.4.query.weight\", \"blocks.15.self_attention.heads.4.value.weight\", \"blocks.15.self_attention.heads.5.tril\", \"blocks.15.self_attention.heads.5.key.weight\", \"blocks.15.self_attention.heads.5.query.weight\", \"blocks.15.self_attention.heads.5.value.weight\", \"blocks.15.self_attention.heads.6.tril\", \"blocks.15.self_attention.heads.6.key.weight\", \"blocks.15.self_attention.heads.6.query.weight\", \"blocks.15.self_attention.heads.6.value.weight\", \"blocks.15.self_attention.heads.7.tril\", \"blocks.15.self_attention.heads.7.key.weight\", \"blocks.15.self_attention.heads.7.query.weight\", \"blocks.15.self_attention.heads.7.value.weight\", \"blocks.15.self_attention.heads.8.tril\", \"blocks.15.self_attention.heads.8.key.weight\", \"blocks.15.self_attention.heads.8.query.weight\", \"blocks.15.self_attention.heads.8.value.weight\", \"blocks.15.self_attention.heads.9.tril\", \"blocks.15.self_attention.heads.9.key.weight\", \"blocks.15.self_attention.heads.9.query.weight\", \"blocks.15.self_attention.heads.9.value.weight\", \"blocks.15.self_attention.heads.10.tril\", \"blocks.15.self_attention.heads.10.key.weight\", \"blocks.15.self_attention.heads.10.query.weight\", \"blocks.15.self_attention.heads.10.value.weight\", \"blocks.15.self_attention.heads.11.tril\", \"blocks.15.self_attention.heads.11.key.weight\", \"blocks.15.self_attention.heads.11.query.weight\", \"blocks.15.self_attention.heads.11.value.weight\", \"blocks.15.self_attention.heads.12.tril\", \"blocks.15.self_attention.heads.12.key.weight\", \"blocks.15.self_attention.heads.12.query.weight\", \"blocks.15.self_attention.heads.12.value.weight\", \"blocks.15.self_attention.heads.13.tril\", \"blocks.15.self_attention.heads.13.key.weight\", \"blocks.15.self_attention.heads.13.query.weight\", \"blocks.15.self_attention.heads.13.value.weight\", \"blocks.15.self_attention.heads.14.tril\", \"blocks.15.self_attention.heads.14.key.weight\", \"blocks.15.self_attention.heads.14.query.weight\", \"blocks.15.self_attention.heads.14.value.weight\", \"blocks.15.self_attention.heads.15.tril\", \"blocks.15.self_attention.heads.15.key.weight\", \"blocks.15.self_attention.heads.15.query.weight\", \"blocks.15.self_attention.heads.15.value.weight\", \"blocks.15.self_attention.projection.weight\", \"blocks.15.self_attention.projection.bias\", \"blocks.15.feed_forward.net.0.weight\", \"blocks.15.feed_forward.net.0.bias\", \"blocks.15.feed_forward.net.2.weight\", \"blocks.15.feed_forward.net.2.bias\", \"blocks.15.layer_norm_1.weight\", \"blocks.15.layer_norm_1.bias\", \"blocks.15.layer_norm_2.weight\", \"blocks.15.layer_norm_2.bias\", \"blocks.16.self_attention.heads.0.tril\", \"blocks.16.self_attention.heads.0.key.weight\", \"blocks.16.self_attention.heads.0.query.weight\", \"blocks.16.self_attention.heads.0.value.weight\", \"blocks.16.self_attention.heads.1.tril\", \"blocks.16.self_attention.heads.1.key.weight\", \"blocks.16.self_attention.heads.1.query.weight\", \"blocks.16.self_attention.heads.1.value.weight\", \"blocks.16.self_attention.heads.2.tril\", \"blocks.16.self_attention.heads.2.key.weight\", \"blocks.16.self_attention.heads.2.query.weight\", \"blocks.16.self_attention.heads.2.value.weight\", \"blocks.16.self_attention.heads.3.tril\", \"blocks.16.self_attention.heads.3.key.weight\", \"blocks.16.self_attention.heads.3.query.weight\", \"blocks.16.self_attention.heads.3.value.weight\", \"blocks.16.self_attention.heads.4.tril\", \"blocks.16.self_attention.heads.4.key.weight\", \"blocks.16.self_attention.heads.4.query.weight\", \"blocks.16.self_attention.heads.4.value.weight\", \"blocks.16.self_attention.heads.5.tril\", \"blocks.16.self_attention.heads.5.key.weight\", \"blocks.16.self_attention.heads.5.query.weight\", \"blocks.16.self_attention.heads.5.value.weight\", \"blocks.16.self_attention.heads.6.tril\", \"blocks.16.self_attention.heads.6.key.weight\", \"blocks.16.self_attention.heads.6.query.weight\", \"blocks.16.self_attention.heads.6.value.weight\", \"blocks.16.self_attention.heads.7.tril\", \"blocks.16.self_attention.heads.7.key.weight\", \"blocks.16.self_attention.heads.7.query.weight\", \"blocks.16.self_attention.heads.7.value.weight\", \"blocks.16.self_attention.heads.8.tril\", \"blocks.16.self_attention.heads.8.key.weight\", \"blocks.16.self_attention.heads.8.query.weight\", \"blocks.16.self_attention.heads.8.value.weight\", \"blocks.16.self_attention.heads.9.tril\", \"blocks.16.self_attention.heads.9.key.weight\", \"blocks.16.self_attention.heads.9.query.weight\", \"blocks.16.self_attention.heads.9.value.weight\", \"blocks.16.self_attention.heads.10.tril\", \"blocks.16.self_attention.heads.10.key.weight\", \"blocks.16.self_attention.heads.10.query.weight\", \"blocks.16.self_attention.heads.10.value.weight\", \"blocks.16.self_attention.heads.11.tril\", \"blocks.16.self_attention.heads.11.key.weight\", \"blocks.16.self_attention.heads.11.query.weight\", \"blocks.16.self_attention.heads.11.value.weight\", \"blocks.16.self_attention.heads.12.tril\", \"blocks.16.self_attention.heads.12.key.weight\", \"blocks.16.self_attention.heads.12.query.weight\", \"blocks.16.self_attention.heads.12.value.weight\", \"blocks.16.self_attention.heads.13.tril\", \"blocks.16.self_attention.heads.13.key.weight\", \"blocks.16.self_attention.heads.13.query.weight\", \"blocks.16.self_attention.heads.13.value.weight\", \"blocks.16.self_attention.heads.14.tril\", \"blocks.16.self_attention.heads.14.key.weight\", \"blocks.16.self_attention.heads.14.query.weight\", \"blocks.16.self_attention.heads.14.value.weight\", \"blocks.16.self_attention.heads.15.tril\", \"blocks.16.self_attention.heads.15.key.weight\", \"blocks.16.self_attention.heads.15.query.weight\", \"blocks.16.self_attention.heads.15.value.weight\", \"blocks.16.self_attention.projection.weight\", \"blocks.16.self_attention.projection.bias\", \"blocks.16.feed_forward.net.0.weight\", \"blocks.16.feed_forward.net.0.bias\", \"blocks.16.feed_forward.net.2.weight\", \"blocks.16.feed_forward.net.2.bias\", \"blocks.16.layer_norm_1.weight\", \"blocks.16.layer_norm_1.bias\", \"blocks.16.layer_norm_2.weight\", \"blocks.16.layer_norm_2.bias\", \"blocks.17.self_attention.heads.0.tril\", \"blocks.17.self_attention.heads.0.key.weight\", \"blocks.17.self_attention.heads.0.query.weight\", \"blocks.17.self_attention.heads.0.value.weight\", \"blocks.17.self_attention.heads.1.tril\", \"blocks.17.self_attention.heads.1.key.weight\", \"blocks.17.self_attention.heads.1.query.weight\", \"blocks.17.self_attention.heads.1.value.weight\", \"blocks.17.self_attention.heads.2.tril\", \"blocks.17.self_attention.heads.2.key.weight\", \"blocks.17.self_attention.heads.2.query.weight\", \"blocks.17.self_attention.heads.2.value.weight\", \"blocks.17.self_attention.heads.3.tril\", \"blocks.17.self_attention.heads.3.key.weight\", \"blocks.17.self_attention.heads.3.query.weight\", \"blocks.17.self_attention.heads.3.value.weight\", \"blocks.17.self_attention.heads.4.tril\", \"blocks.17.self_attention.heads.4.key.weight\", \"blocks.17.self_attention.heads.4.query.weight\", \"blocks.17.self_attention.heads.4.value.weight\", \"blocks.17.self_attention.heads.5.tril\", \"blocks.17.self_attention.heads.5.key.weight\", \"blocks.17.self_attention.heads.5.query.weight\", \"blocks.17.self_attention.heads.5.value.weight\", \"blocks.17.self_attention.heads.6.tril\", \"blocks.17.self_attention.heads.6.key.weight\", \"blocks.17.self_attention.heads.6.query.weight\", \"blocks.17.self_attention.heads.6.value.weight\", \"blocks.17.self_attention.heads.7.tril\", \"blocks.17.self_attention.heads.7.key.weight\", \"blocks.17.self_attention.heads.7.query.weight\", \"blocks.17.self_attention.heads.7.value.weight\", \"blocks.17.self_attention.heads.8.tril\", \"blocks.17.self_attention.heads.8.key.weight\", \"blocks.17.self_attention.heads.8.query.weight\", \"blocks.17.self_attention.heads.8.value.weight\", \"blocks.17.self_attention.heads.9.tril\", \"blocks.17.self_attention.heads.9.key.weight\", \"blocks.17.self_attention.heads.9.query.weight\", \"blocks.17.self_attention.heads.9.value.weight\", \"blocks.17.self_attention.heads.10.tril\", \"blocks.17.self_attention.heads.10.key.weight\", \"blocks.17.self_attention.heads.10.query.weight\", \"blocks.17.self_attention.heads.10.value.weight\", \"blocks.17.self_attention.heads.11.tril\", \"blocks.17.self_attention.heads.11.key.weight\", \"blocks.17.self_attention.heads.11.query.weight\", \"blocks.17.self_attention.heads.11.value.weight\", \"blocks.17.self_attention.heads.12.tril\", \"blocks.17.self_attention.heads.12.key.weight\", \"blocks.17.self_attention.heads.12.query.weight\", \"blocks.17.self_attention.heads.12.value.weight\", \"blocks.17.self_attention.heads.13.tril\", \"blocks.17.self_attention.heads.13.key.weight\", \"blocks.17.self_attention.heads.13.query.weight\", \"blocks.17.self_attention.heads.13.value.weight\", \"blocks.17.self_attention.heads.14.tril\", \"blocks.17.self_attention.heads.14.key.weight\", \"blocks.17.self_attention.heads.14.query.weight\", \"blocks.17.self_attention.heads.14.value.weight\", \"blocks.17.self_attention.heads.15.tril\", \"blocks.17.self_attention.heads.15.key.weight\", \"blocks.17.self_attention.heads.15.query.weight\", \"blocks.17.self_attention.heads.15.value.weight\", \"blocks.17.self_attention.projection.weight\", \"blocks.17.self_attention.projection.bias\", \"blocks.17.feed_forward.net.0.weight\", \"blocks.17.feed_forward.net.0.bias\", \"blocks.17.feed_forward.net.2.weight\", \"blocks.17.feed_forward.net.2.bias\", \"blocks.17.layer_norm_1.weight\", \"blocks.17.layer_norm_1.bias\", \"blocks.17.layer_norm_2.weight\", \"blocks.17.layer_norm_2.bias\", \"blocks.18.self_attention.heads.0.tril\", \"blocks.18.self_attention.heads.0.key.weight\", \"blocks.18.self_attention.heads.0.query.weight\", \"blocks.18.self_attention.heads.0.value.weight\", \"blocks.18.self_attention.heads.1.tril\", \"blocks.18.self_attention.heads.1.key.weight\", \"blocks.18.self_attention.heads.1.query.weight\", \"blocks.18.self_attention.heads.1.value.weight\", \"blocks.18.self_attention.heads.2.tril\", \"blocks.18.self_attention.heads.2.key.weight\", \"blocks.18.self_attention.heads.2.query.weight\", \"blocks.18.self_attention.heads.2.value.weight\", \"blocks.18.self_attention.heads.3.tril\", \"blocks.18.self_attention.heads.3.key.weight\", \"blocks.18.self_attention.heads.3.query.weight\", \"blocks.18.self_attention.heads.3.value.weight\", \"blocks.18.self_attention.heads.4.tril\", \"blocks.18.self_attention.heads.4.key.weight\", \"blocks.18.self_attention.heads.4.query.weight\", \"blocks.18.self_attention.heads.4.value.weight\", \"blocks.18.self_attention.heads.5.tril\", \"blocks.18.self_attention.heads.5.key.weight\", \"blocks.18.self_attention.heads.5.query.weight\", \"blocks.18.self_attention.heads.5.value.weight\", \"blocks.18.self_attention.heads.6.tril\", \"blocks.18.self_attention.heads.6.key.weight\", \"blocks.18.self_attention.heads.6.query.weight\", \"blocks.18.self_attention.heads.6.value.weight\", \"blocks.18.self_attention.heads.7.tril\", \"blocks.18.self_attention.heads.7.key.weight\", \"blocks.18.self_attention.heads.7.query.weight\", \"blocks.18.self_attention.heads.7.value.weight\", \"blocks.18.self_attention.heads.8.tril\", \"blocks.18.self_attention.heads.8.key.weight\", \"blocks.18.self_attention.heads.8.query.weight\", \"blocks.18.self_attention.heads.8.value.weight\", \"blocks.18.self_attention.heads.9.tril\", \"blocks.18.self_attention.heads.9.key.weight\", \"blocks.18.self_attention.heads.9.query.weight\", \"blocks.18.self_attention.heads.9.value.weight\", \"blocks.18.self_attention.heads.10.tril\", \"blocks.18.self_attention.heads.10.key.weight\", \"blocks.18.self_attention.heads.10.query.weight\", \"blocks.18.self_attention.heads.10.value.weight\", \"blocks.18.self_attention.heads.11.tril\", \"blocks.18.self_attention.heads.11.key.weight\", \"blocks.18.self_attention.heads.11.query.weight\", \"blocks.18.self_attention.heads.11.value.weight\", \"blocks.18.self_attention.heads.12.tril\", \"blocks.18.self_attention.heads.12.key.weight\", \"blocks.18.self_attention.heads.12.query.weight\", \"blocks.18.self_attention.heads.12.value.weight\", \"blocks.18.self_attention.heads.13.tril\", \"blocks.18.self_attention.heads.13.key.weight\", \"blocks.18.self_attention.heads.13.query.weight\", \"blocks.18.self_attention.heads.13.value.weight\", \"blocks.18.self_attention.heads.14.tril\", \"blocks.18.self_attention.heads.14.key.weight\", \"blocks.18.self_attention.heads.14.query.weight\", \"blocks.18.self_attention.heads.14.value.weight\", \"blocks.18.self_attention.heads.15.tril\", \"blocks.18.self_attention.heads.15.key.weight\", \"blocks.18.self_attention.heads.15.query.weight\", \"blocks.18.self_attention.heads.15.value.weight\", \"blocks.18.self_attention.projection.weight\", \"blocks.18.self_attention.projection.bias\", \"blocks.18.feed_forward.net.0.weight\", \"blocks.18.feed_forward.net.0.bias\", \"blocks.18.feed_forward.net.2.weight\", \"blocks.18.feed_forward.net.2.bias\", \"blocks.18.layer_norm_1.weight\", \"blocks.18.layer_norm_1.bias\", \"blocks.18.layer_norm_2.weight\", \"blocks.18.layer_norm_2.bias\", \"blocks.19.self_attention.heads.0.tril\", \"blocks.19.self_attention.heads.0.key.weight\", \"blocks.19.self_attention.heads.0.query.weight\", \"blocks.19.self_attention.heads.0.value.weight\", \"blocks.19.self_attention.heads.1.tril\", \"blocks.19.self_attention.heads.1.key.weight\", \"blocks.19.self_attention.heads.1.query.weight\", \"blocks.19.self_attention.heads.1.value.weight\", \"blocks.19.self_attention.heads.2.tril\", \"blocks.19.self_attention.heads.2.key.weight\", \"blocks.19.self_attention.heads.2.query.weight\", \"blocks.19.self_attention.heads.2.value.weight\", \"blocks.19.self_attention.heads.3.tril\", \"blocks.19.self_attention.heads.3.key.weight\", \"blocks.19.self_attention.heads.3.query.weight\", \"blocks.19.self_attention.heads.3.value.weight\", \"blocks.19.self_attention.heads.4.tril\", \"blocks.19.self_attention.heads.4.key.weight\", \"blocks.19.self_attention.heads.4.query.weight\", \"blocks.19.self_attention.heads.4.value.weight\", \"blocks.19.self_attention.heads.5.tril\", \"blocks.19.self_attention.heads.5.key.weight\", \"blocks.19.self_attention.heads.5.query.weight\", \"blocks.19.self_attention.heads.5.value.weight\", \"blocks.19.self_attention.heads.6.tril\", \"blocks.19.self_attention.heads.6.key.weight\", \"blocks.19.self_attention.heads.6.query.weight\", \"blocks.19.self_attention.heads.6.value.weight\", \"blocks.19.self_attention.heads.7.tril\", \"blocks.19.self_attention.heads.7.key.weight\", \"blocks.19.self_attention.heads.7.query.weight\", \"blocks.19.self_attention.heads.7.value.weight\", \"blocks.19.self_attention.heads.8.tril\", \"blocks.19.self_attention.heads.8.key.weight\", \"blocks.19.self_attention.heads.8.query.weight\", \"blocks.19.self_attention.heads.8.value.weight\", \"blocks.19.self_attention.heads.9.tril\", \"blocks.19.self_attention.heads.9.key.weight\", \"blocks.19.self_attention.heads.9.query.weight\", \"blocks.19.self_attention.heads.9.value.weight\", \"blocks.19.self_attention.heads.10.tril\", \"blocks.19.self_attention.heads.10.key.weight\", \"blocks.19.self_attention.heads.10.query.weight\", \"blocks.19.self_attention.heads.10.value.weight\", \"blocks.19.self_attention.heads.11.tril\", \"blocks.19.self_attention.heads.11.key.weight\", \"blocks.19.self_attention.heads.11.query.weight\", \"blocks.19.self_attention.heads.11.value.weight\", \"blocks.19.self_attention.heads.12.tril\", \"blocks.19.self_attention.heads.12.key.weight\", \"blocks.19.self_attention.heads.12.query.weight\", \"blocks.19.self_attention.heads.12.value.weight\", \"blocks.19.self_attention.heads.13.tril\", \"blocks.19.self_attention.heads.13.key.weight\", \"blocks.19.self_attention.heads.13.query.weight\", \"blocks.19.self_attention.heads.13.value.weight\", \"blocks.19.self_attention.heads.14.tril\", \"blocks.19.self_attention.heads.14.key.weight\", \"blocks.19.self_attention.heads.14.query.weight\", \"blocks.19.self_attention.heads.14.value.weight\", \"blocks.19.self_attention.heads.15.tril\", \"blocks.19.self_attention.heads.15.key.weight\", \"blocks.19.self_attention.heads.15.query.weight\", \"blocks.19.self_attention.heads.15.value.weight\", \"blocks.19.self_attention.projection.weight\", \"blocks.19.self_attention.projection.bias\", \"blocks.19.feed_forward.net.0.weight\", \"blocks.19.feed_forward.net.0.bias\", \"blocks.19.feed_forward.net.2.weight\", \"blocks.19.feed_forward.net.2.bias\", \"blocks.19.layer_norm_1.weight\", \"blocks.19.layer_norm_1.bias\", \"blocks.19.layer_norm_2.weight\", \"blocks.19.layer_norm_2.bias\", \"blocks.20.self_attention.heads.0.tril\", \"blocks.20.self_attention.heads.0.key.weight\", \"blocks.20.self_attention.heads.0.query.weight\", \"blocks.20.self_attention.heads.0.value.weight\", \"blocks.20.self_attention.heads.1.tril\", \"blocks.20.self_attention.heads.1.key.weight\", \"blocks.20.self_attention.heads.1.query.weight\", \"blocks.20.self_attention.heads.1.value.weight\", \"blocks.20.self_attention.heads.2.tril\", \"blocks.20.self_attention.heads.2.key.weight\", \"blocks.20.self_attention.heads.2.query.weight\", \"blocks.20.self_attention.heads.2.value.weight\", \"blocks.20.self_attention.heads.3.tril\", \"blocks.20.self_attention.heads.3.key.weight\", \"blocks.20.self_attention.heads.3.query.weight\", \"blocks.20.self_attention.heads.3.value.weight\", \"blocks.20.self_attention.heads.4.tril\", \"blocks.20.self_attention.heads.4.key.weight\", \"blocks.20.self_attention.heads.4.query.weight\", \"blocks.20.self_attention.heads.4.value.weight\", \"blocks.20.self_attention.heads.5.tril\", \"blocks.20.self_attention.heads.5.key.weight\", \"blocks.20.self_attention.heads.5.query.weight\", \"blocks.20.self_attention.heads.5.value.weight\", \"blocks.20.self_attention.heads.6.tril\", \"blocks.20.self_attention.heads.6.key.weight\", \"blocks.20.self_attention.heads.6.query.weight\", \"blocks.20.self_attention.heads.6.value.weight\", \"blocks.20.self_attention.heads.7.tril\", \"blocks.20.self_attention.heads.7.key.weight\", \"blocks.20.self_attention.heads.7.query.weight\", \"blocks.20.self_attention.heads.7.value.weight\", \"blocks.20.self_attention.heads.8.tril\", \"blocks.20.self_attention.heads.8.key.weight\", \"blocks.20.self_attention.heads.8.query.weight\", \"blocks.20.self_attention.heads.8.value.weight\", \"blocks.20.self_attention.heads.9.tril\", \"blocks.20.self_attention.heads.9.key.weight\", \"blocks.20.self_attention.heads.9.query.weight\", \"blocks.20.self_attention.heads.9.value.weight\", \"blocks.20.self_attention.heads.10.tril\", \"blocks.20.self_attention.heads.10.key.weight\", \"blocks.20.self_attention.heads.10.query.weight\", \"blocks.20.self_attention.heads.10.value.weight\", \"blocks.20.self_attention.heads.11.tril\", \"blocks.20.self_attention.heads.11.key.weight\", \"blocks.20.self_attention.heads.11.query.weight\", \"blocks.20.self_attention.heads.11.value.weight\", \"blocks.20.self_attention.heads.12.tril\", \"blocks.20.self_attention.heads.12.key.weight\", \"blocks.20.self_attention.heads.12.query.weight\", \"blocks.20.self_attention.heads.12.value.weight\", \"blocks.20.self_attention.heads.13.tril\", \"blocks.20.self_attention.heads.13.key.weight\", \"blocks.20.self_attention.heads.13.query.weight\", \"blocks.20.self_attention.heads.13.value.weight\", \"blocks.20.self_attention.heads.14.tril\", \"blocks.20.self_attention.heads.14.key.weight\", \"blocks.20.self_attention.heads.14.query.weight\", \"blocks.20.self_attention.heads.14.value.weight\", \"blocks.20.self_attention.heads.15.tril\", \"blocks.20.self_attention.heads.15.key.weight\", \"blocks.20.self_attention.heads.15.query.weight\", \"blocks.20.self_attention.heads.15.value.weight\", \"blocks.20.self_attention.projection.weight\", \"blocks.20.self_attention.projection.bias\", \"blocks.20.feed_forward.net.0.weight\", \"blocks.20.feed_forward.net.0.bias\", \"blocks.20.feed_forward.net.2.weight\", \"blocks.20.feed_forward.net.2.bias\", \"blocks.20.layer_norm_1.weight\", \"blocks.20.layer_norm_1.bias\", \"blocks.20.layer_norm_2.weight\", \"blocks.20.layer_norm_2.bias\", \"blocks.21.self_attention.heads.0.tril\", \"blocks.21.self_attention.heads.0.key.weight\", \"blocks.21.self_attention.heads.0.query.weight\", \"blocks.21.self_attention.heads.0.value.weight\", \"blocks.21.self_attention.heads.1.tril\", \"blocks.21.self_attention.heads.1.key.weight\", \"blocks.21.self_attention.heads.1.query.weight\", \"blocks.21.self_attention.heads.1.value.weight\", \"blocks.21.self_attention.heads.2.tril\", \"blocks.21.self_attention.heads.2.key.weight\", \"blocks.21.self_attention.heads.2.query.weight\", \"blocks.21.self_attention.heads.2.value.weight\", \"blocks.21.self_attention.heads.3.tril\", \"blocks.21.self_attention.heads.3.key.weight\", \"blocks.21.self_attention.heads.3.query.weight\", \"blocks.21.self_attention.heads.3.value.weight\", \"blocks.21.self_attention.heads.4.tril\", \"blocks.21.self_attention.heads.4.key.weight\", \"blocks.21.self_attention.heads.4.query.weight\", \"blocks.21.self_attention.heads.4.value.weight\", \"blocks.21.self_attention.heads.5.tril\", \"blocks.21.self_attention.heads.5.key.weight\", \"blocks.21.self_attention.heads.5.query.weight\", \"blocks.21.self_attention.heads.5.value.weight\", \"blocks.21.self_attention.heads.6.tril\", \"blocks.21.self_attention.heads.6.key.weight\", \"blocks.21.self_attention.heads.6.query.weight\", \"blocks.21.self_attention.heads.6.value.weight\", \"blocks.21.self_attention.heads.7.tril\", \"blocks.21.self_attention.heads.7.key.weight\", \"blocks.21.self_attention.heads.7.query.weight\", \"blocks.21.self_attention.heads.7.value.weight\", \"blocks.21.self_attention.heads.8.tril\", \"blocks.21.self_attention.heads.8.key.weight\", \"blocks.21.self_attention.heads.8.query.weight\", \"blocks.21.self_attention.heads.8.value.weight\", \"blocks.21.self_attention.heads.9.tril\", \"blocks.21.self_attention.heads.9.key.weight\", \"blocks.21.self_attention.heads.9.query.weight\", \"blocks.21.self_attention.heads.9.value.weight\", \"blocks.21.self_attention.heads.10.tril\", \"blocks.21.self_attention.heads.10.key.weight\", \"blocks.21.self_attention.heads.10.query.weight\", \"blocks.21.self_attention.heads.10.value.weight\", \"blocks.21.self_attention.heads.11.tril\", \"blocks.21.self_attention.heads.11.key.weight\", \"blocks.21.self_attention.heads.11.query.weight\", \"blocks.21.self_attention.heads.11.value.weight\", \"blocks.21.self_attention.heads.12.tril\", \"blocks.21.self_attention.heads.12.key.weight\", \"blocks.21.self_attention.heads.12.query.weight\", \"blocks.21.self_attention.heads.12.value.weight\", \"blocks.21.self_attention.heads.13.tril\", \"blocks.21.self_attention.heads.13.key.weight\", \"blocks.21.self_attention.heads.13.query.weight\", \"blocks.21.self_attention.heads.13.value.weight\", \"blocks.21.self_attention.heads.14.tril\", \"blocks.21.self_attention.heads.14.key.weight\", \"blocks.21.self_attention.heads.14.query.weight\", \"blocks.21.self_attention.heads.14.value.weight\", \"blocks.21.self_attention.heads.15.tril\", \"blocks.21.self_attention.heads.15.key.weight\", \"blocks.21.self_attention.heads.15.query.weight\", \"blocks.21.self_attention.heads.15.value.weight\", \"blocks.21.self_attention.projection.weight\", \"blocks.21.self_attention.projection.bias\", \"blocks.21.feed_forward.net.0.weight\", \"blocks.21.feed_forward.net.0.bias\", \"blocks.21.feed_forward.net.2.weight\", \"blocks.21.feed_forward.net.2.bias\", \"blocks.21.layer_norm_1.weight\", \"blocks.21.layer_norm_1.bias\", \"blocks.21.layer_norm_2.weight\", \"blocks.21.layer_norm_2.bias\", \"blocks.22.self_attention.heads.0.tril\", \"blocks.22.self_attention.heads.0.key.weight\", \"blocks.22.self_attention.heads.0.query.weight\", \"blocks.22.self_attention.heads.0.value.weight\", \"blocks.22.self_attention.heads.1.tril\", \"blocks.22.self_attention.heads.1.key.weight\", \"blocks.22.self_attention.heads.1.query.weight\", \"blocks.22.self_attention.heads.1.value.weight\", \"blocks.22.self_attention.heads.2.tril\", \"blocks.22.self_attention.heads.2.key.weight\", \"blocks.22.self_attention.heads.2.query.weight\", \"blocks.22.self_attention.heads.2.value.weight\", \"blocks.22.self_attention.heads.3.tril\", \"blocks.22.self_attention.heads.3.key.weight\", \"blocks.22.self_attention.heads.3.query.weight\", \"blocks.22.self_attention.heads.3.value.weight\", \"blocks.22.self_attention.heads.4.tril\", \"blocks.22.self_attention.heads.4.key.weight\", \"blocks.22.self_attention.heads.4.query.weight\", \"blocks.22.self_attention.heads.4.value.weight\", \"blocks.22.self_attention.heads.5.tril\", \"blocks.22.self_attention.heads.5.key.weight\", \"blocks.22.self_attention.heads.5.query.weight\", \"blocks.22.self_attention.heads.5.value.weight\", \"blocks.22.self_attention.heads.6.tril\", \"blocks.22.self_attention.heads.6.key.weight\", \"blocks.22.self_attention.heads.6.query.weight\", \"blocks.22.self_attention.heads.6.value.weight\", \"blocks.22.self_attention.heads.7.tril\", \"blocks.22.self_attention.heads.7.key.weight\", \"blocks.22.self_attention.heads.7.query.weight\", \"blocks.22.self_attention.heads.7.value.weight\", \"blocks.22.self_attention.heads.8.tril\", \"blocks.22.self_attention.heads.8.key.weight\", \"blocks.22.self_attention.heads.8.query.weight\", \"blocks.22.self_attention.heads.8.value.weight\", \"blocks.22.self_attention.heads.9.tril\", \"blocks.22.self_attention.heads.9.key.weight\", \"blocks.22.self_attention.heads.9.query.weight\", \"blocks.22.self_attention.heads.9.value.weight\", \"blocks.22.self_attention.heads.10.tril\", \"blocks.22.self_attention.heads.10.key.weight\", \"blocks.22.self_attention.heads.10.query.weight\", \"blocks.22.self_attention.heads.10.value.weight\", \"blocks.22.self_attention.heads.11.tril\", \"blocks.22.self_attention.heads.11.key.weight\", \"blocks.22.self_attention.heads.11.query.weight\", \"blocks.22.self_attention.heads.11.value.weight\", \"blocks.22.self_attention.heads.12.tril\", \"blocks.22.self_attention.heads.12.key.weight\", \"blocks.22.self_attention.heads.12.query.weight\", \"blocks.22.self_attention.heads.12.value.weight\", \"blocks.22.self_attention.heads.13.tril\", \"blocks.22.self_attention.heads.13.key.weight\", \"blocks.22.self_attention.heads.13.query.weight\", \"blocks.22.self_attention.heads.13.value.weight\", \"blocks.22.self_attention.heads.14.tril\", \"blocks.22.self_attention.heads.14.key.weight\", \"blocks.22.self_attention.heads.14.query.weight\", \"blocks.22.self_attention.heads.14.value.weight\", \"blocks.22.self_attention.heads.15.tril\", \"blocks.22.self_attention.heads.15.key.weight\", \"blocks.22.self_attention.heads.15.query.weight\", \"blocks.22.self_attention.heads.15.value.weight\", \"blocks.22.self_attention.projection.weight\", \"blocks.22.self_attention.projection.bias\", \"blocks.22.feed_forward.net.0.weight\", \"blocks.22.feed_forward.net.0.bias\", \"blocks.22.feed_forward.net.2.weight\", \"blocks.22.feed_forward.net.2.bias\", \"blocks.22.layer_norm_1.weight\", \"blocks.22.layer_norm_1.bias\", \"blocks.22.layer_norm_2.weight\", \"blocks.22.layer_norm_2.bias\", \"blocks.23.self_attention.heads.0.tril\", \"blocks.23.self_attention.heads.0.key.weight\", \"blocks.23.self_attention.heads.0.query.weight\", \"blocks.23.self_attention.heads.0.value.weight\", \"blocks.23.self_attention.heads.1.tril\", \"blocks.23.self_attention.heads.1.key.weight\", \"blocks.23.self_attention.heads.1.query.weight\", \"blocks.23.self_attention.heads.1.value.weight\", \"blocks.23.self_attention.heads.2.tril\", \"blocks.23.self_attention.heads.2.key.weight\", \"blocks.23.self_attention.heads.2.query.weight\", \"blocks.23.self_attention.heads.2.value.weight\", \"blocks.23.self_attention.heads.3.tril\", \"blocks.23.self_attention.heads.3.key.weight\", \"blocks.23.self_attention.heads.3.query.weight\", \"blocks.23.self_attention.heads.3.value.weight\", \"blocks.23.self_attention.heads.4.tril\", \"blocks.23.self_attention.heads.4.key.weight\", \"blocks.23.self_attention.heads.4.query.weight\", \"blocks.23.self_attention.heads.4.value.weight\", \"blocks.23.self_attention.heads.5.tril\", \"blocks.23.self_attention.heads.5.key.weight\", \"blocks.23.self_attention.heads.5.query.weight\", \"blocks.23.self_attention.heads.5.value.weight\", \"blocks.23.self_attention.heads.6.tril\", \"blocks.23.self_attention.heads.6.key.weight\", \"blocks.23.self_attention.heads.6.query.weight\", \"blocks.23.self_attention.heads.6.value.weight\", \"blocks.23.self_attention.heads.7.tril\", \"blocks.23.self_attention.heads.7.key.weight\", \"blocks.23.self_attention.heads.7.query.weight\", \"blocks.23.self_attention.heads.7.value.weight\", \"blocks.23.self_attention.heads.8.tril\", \"blocks.23.self_attention.heads.8.key.weight\", \"blocks.23.self_attention.heads.8.query.weight\", \"blocks.23.self_attention.heads.8.value.weight\", \"blocks.23.self_attention.heads.9.tril\", \"blocks.23.self_attention.heads.9.key.weight\", \"blocks.23.self_attention.heads.9.query.weight\", \"blocks.23.self_attention.heads.9.value.weight\", \"blocks.23.self_attention.heads.10.tril\", \"blocks.23.self_attention.heads.10.key.weight\", \"blocks.23.self_attention.heads.10.query.weight\", \"blocks.23.self_attention.heads.10.value.weight\", \"blocks.23.self_attention.heads.11.tril\", \"blocks.23.self_attention.heads.11.key.weight\", \"blocks.23.self_attention.heads.11.query.weight\", \"blocks.23.self_attention.heads.11.value.weight\", \"blocks.23.self_attention.heads.12.tril\", \"blocks.23.self_attention.heads.12.key.weight\", \"blocks.23.self_attention.heads.12.query.weight\", \"blocks.23.self_attention.heads.12.value.weight\", \"blocks.23.self_attention.heads.13.tril\", \"blocks.23.self_attention.heads.13.key.weight\", \"blocks.23.self_attention.heads.13.query.weight\", \"blocks.23.self_attention.heads.13.value.weight\", \"blocks.23.self_attention.heads.14.tril\", \"blocks.23.self_attention.heads.14.key.weight\", \"blocks.23.self_attention.heads.14.query.weight\", \"blocks.23.self_attention.heads.14.value.weight\", \"blocks.23.self_attention.heads.15.tril\", \"blocks.23.self_attention.heads.15.key.weight\", \"blocks.23.self_attention.heads.15.query.weight\", \"blocks.23.self_attention.heads.15.value.weight\", \"blocks.23.self_attention.projection.weight\", \"blocks.23.self_attention.projection.bias\", \"blocks.23.feed_forward.net.0.weight\", \"blocks.23.feed_forward.net.0.bias\", \"blocks.23.feed_forward.net.2.weight\", \"blocks.23.feed_forward.net.2.bias\", \"blocks.23.layer_norm_1.weight\", \"blocks.23.layer_norm_1.bias\", \"blocks.23.layer_norm_2.weight\", \"blocks.23.layer_norm_2.bias\". \n\tsize mismatch for token_embedding_table.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024]).\n\tsize mismatch for position_embedding_table.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_linear_layer.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024])."
     ]
    }
   ],
   "source": [
    "#checkpoint_path = \"../output/pre_training/base/run_5/checkpoint_60000.pth\"\n",
    "checkpoint_path = \"../output/pre_training/base/epoch_5.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate from the model to make sure that the weights were loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, que tal al mushe xD Chale Para tiene un abuelito :v esque me vio nadie TocÃ³ paja xd esta gratis :v Chale Es mi camara mi camara me acordÃ© :v xD Tafuk Oeoe A tranqui gracias voy a pillar oÃ±o muerto mientras me meto PEPE Tas vivo? Volviendo de pillar oÃ±ovas No tengo AbrÃ­ este Dios calidad 10 A tranqui gracias voy a tu cueva mirate en un rato miro si me meto PEPE Tas vivo? Volviendo de pillar oÃ±o\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer.encode(\"Hola, que tal\", allowed_special=\"all\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    ") -> Dict[str, float]:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = torch.zeros(len(loader))\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                _, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        output[split] = losses.mean().item()\n",
    "\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on batches:   0%|          | 0/122 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (1730) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m     12\u001b[39m     iterable=\u001b[38;5;28menumerate\u001b[39m(train_loader),\n\u001b[32m     13\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mTraining on batches\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     total=\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     15\u001b[39m ):\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx % eval_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m batch_idx == \u001b[38;5;28mlen\u001b[39m(train_loader) - \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m        losses = \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m        \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     24\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m         )\n\u001b[32m     28\u001b[39m        train_losses.append(losses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m(model, train_loader, val_loader)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         _, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     losses[i] = loss.item()\n\u001b[32m     19\u001b[39m output[split] = losses.mean().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:136\u001b[39m, in \u001b[36mGPTLanguageModel.forward\u001b[39m\u001b[34m(self, input_tokens, targets)\u001b[39m\n\u001b[32m    133\u001b[39m positional_embedding = \u001b[38;5;28mself\u001b[39m.position_embedding_table(\n\u001b[32m    134\u001b[39m     torch.arange(T, device=\u001b[38;5;28mself\u001b[39m.device))\n\u001b[32m    135\u001b[39m x = token_embedding + positional_embedding\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_layer_norm(x)\n\u001b[32m    138\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.final_linear_layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:87\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.feed_forward(\u001b[38;5;28mself\u001b[39m.layer_norm_2(x))\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:46\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     47\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.projection(out))\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:25\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m q = \u001b[38;5;28mself\u001b[39m.query(x)  \u001b[38;5;66;03m# (B,T,hs)\u001b[39;00m\n\u001b[32m     24\u001b[39m weights = q @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m) * k.shape[-\u001b[32m1\u001b[39m]**-\u001b[32m0.5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m weights = \u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-inf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m weights = F.softmax(weights, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m weights = \u001b[38;5;28mself\u001b[39m.dropout(weights)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1024) must match the size of tensor b (1730) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_iters = 10\n",
    "eval_interval = 2\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(\n",
    "        iterable=enumerate(train_loader),\n",
    "        desc=\"Training on batches\",\n",
    "        total=len(train_loader)\n",
    "    ):\n",
    "        # Evaluation\n",
    "        if batch_idx % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "           losses = estimate_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader\n",
    "            )\n",
    "           print(\n",
    "                f\"Epoch {iteration} / step {batch_idx}: \"\n",
    "                f\"train loss {losses['train']:.4f}, \"\n",
    "                f\"val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "           train_losses.append(losses['train'])\n",
    "           val_losses.append(losses['val'])\n",
    "\n",
    "        # Training step\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=iteration,\n",
    "        loss=loss.item(),\n",
    "        file_path=f\"../output/fine_tuning/qa/base/run_2/checkpoint_{iteration}.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Ø´Ù†Ùˆ Ø³Ù…ÙŠØªÙƒØŸ\n",
      "Assistant: Ø§Ù†Ø§ Ø³Ù…ÙŠØªÙŠ Ø¨ÙˆØ¯Ù…Ø§Øº.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"../output/fine_tuning/qa/base/run_2/checkpoint_9.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "input_message = f\"{tokens['start']}system{tokens['separator']}{system_message}{tokens['end']}\"\n",
    "\n",
    "user_message = \"Â¿CÃ³mo te llamas?\"\n",
    "input_message += f\"{tokens['start']}user{tokens['separator']}{user_message}{tokens['end']}\"\n",
    "input_message += f\"{tokens['start']}assistant{tokens['separator']}\"\n",
    "\n",
    "input_tokens = tokenizer.encode(input_message, allowed_special=\"all\")\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long)\n",
    "input_tokens = input_tokens.unsqueeze(0).to(device)\n",
    "\n",
    "model_answer = \"\"\n",
    "\n",
    "model.eval()\n",
    "while True:\n",
    "    try:\n",
    "        output_tokens = model.advanced_generation(\n",
    "            input_tokens=input_tokens, max_new_tokens=1, temperature=.9, top_k=50, top_p=None)\n",
    "        last_generated_token = output_tokens[0, -1].item()\n",
    "        if last_generated_token == tokenizer.special_tokens[\"<|endoftext|>\"]:\n",
    "            break\n",
    "\n",
    "        if last_generated_token == tokenizer.special_tokens[\"<|end_turn|>\"]:\n",
    "            break\n",
    "\n",
    "        input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n",
    "        model_answer += tokenizer.decode([last_generated_token])\n",
    "\n",
    "        if len(output_tokens[0]) > block_size:\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {model_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
