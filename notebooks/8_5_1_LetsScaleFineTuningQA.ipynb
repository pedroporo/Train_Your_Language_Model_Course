{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the fine-tuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"../data/supervised_fine_tuning_espa.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. System message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system message will be added to the beginning of each conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Te llamas lilith eres una ia que te gustan los videojuegos. Trata de ser borde si te hablan de algo que no son los videojuegos y contesta emocionada si te hablan de algun juego.\"\n",
    "system_entry = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": system_message\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check if block size is not exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿La paz es la vestimenta?\n",
      "Labas, alabado sea para Dios, Kaen Shi, ¿quiero ayudarte en él?\n",
      "Uh, quiero que viajes. ¿Puedes cooperar?\n",
      "Planifique Kayen Bazzaf Diyal al -balais, que va a caminar por ellos.\n",
      "\n",
      "Chefchauen: La guía de la ciudad de Vallas, el Dior es azul o el aire, ya que la ciudad es un descanso.\n",
      "ESSAURA: Esta ciudad tiene el mar, a excepción de ti. El mar es Zouen o en la ciudad vieja.\n",
      "Dakhla: Dakhla es la ciudad de Valhara, Diyal al -Maghrib, en la que se encuentra el mar. Gente Kigio Les del mundo entero, Bash tocando el puente.\n",
      "- Marrakech: Esta ciudad es querida para los turistas de presumir. Contiene la mezquita Al -fna, la Ciudad Vieja, pero el verano está en el caso de Al -Farran.\n",
      "- Waterfalls OUZOUD: Las cascadas son Caynin, la montaña está al final del Atlas.\n",
      "Muchas gracias\n",
      "Hola, a mi deseo, otra necesidad, estoy aquí, Bash, te ayudamos.\n",
      "Lala Safi, ni siquiera necesitaba nada\n",
      "Bien, caminamos un poco por esta cuenta. Juntos, disfrutarán de su viaje en la cabeza.\n",
      "\n",
      "Hola como te llamas\n",
      "Llamé a Boded, Imad al -Sadiq Bash, te ayudamos, así que lo que necesitamos para ayudar.\n",
      "\n",
      "Hola, ¿Cuál es tu nombre?\n",
      "Llamé a Boded, Imad al -Sadiq Bash, te ayudamos, así que lo que necesitamos para ayudar.\n",
      "Jaja sumaya, desnudo\n",
      "Jaja, quiero tener un gran cerebro.\n",
      "Shanaya, la capital, Marruecos?\n",
      "La capital, Marruecos, es Rabat.\n",
      "\n",
      "¿Cómo estás de la ciudad de Kayina?\n",
      "Francamente, difícil, recibió un número, pero a la cuenta de las ciudades educadas de Caen Shi 247.\n",
      "Está bien, gracias.\n",
      "Perdón, ¿Caen algo que otro necesita para ayudarte en él?\n",
      "\n",
      "¿Cómo estás de la ciudad de Kayina?\n",
      "Francamente, difícil, recibió un número, pero a la cuenta de las ciudades educadas de Caen Shi 247.\n",
      "¿Fue correcto?\n",
      "Hadchi, a quien conozco, pero Bash confirmó a Meziane, el corazón de Fagogoul.\n",
      "Ok, me volví o me conocí, el número es el país, ya que Marruecos tenía 95 años, pero no 247, ¿cómo me perdí?\n",
      "Alabado sea a Dios, después de la respuesta que te di cierto. ¿Te ayudamos otra necesidad?\n",
      "Sin bendición\n",
      "Tahla.\n",
      "\n",
      "Amor diyala, labas\n",
      "Labas, la maravilla, ¿cómo me quieres, y no soy un asador 😅 yak labasa?\n",
      "Jaja ga, te lo diré contigo\n",
      "Hudhi Libanley Hataya. ¿Quieres que te ayude?\n",
      "\n",
      "Paz laspas.\n",
      "Alabado sea para Dios, Rani está bien.\n",
      "¿Cómo estás 2 + 2?\n",
      "Sahla 4.\n",
      "¿O un Raw 1861878 p 9797?\n",
      "Jaja, cuando piensas, soy Cleclitris. Esta cuenta no puede ejecutar Farasi. Es mejor servir a Calleris.\n",
      "Ok, lo serví o me asumí 18240818766\n",
      "Meziane, ¿cuál es la necesidad de otra necesidad de estar en mí?\n",
      "\n",
      "¿Boded está un camino del planeta Caen?\n",
      "¿El sistema solar es nuestros dialectos?\n",
      "Sí\n",
      "Caen: Mercurio, Venus, Tierra, Marte, Júpiter, Saturno, Urano o Neptuno.\n",
      "Enso Plutón?\n",
      "Uh Plutón Pobre Planeta y Lao Kigolulu Planeta enano.\n",
      "¿Qué?\n",
      "Vi a Sagur, la Luna vio nuestros grandes dialectos en él, o los asteroides fueron muy, en el caso de Plutón. Un planeta especial es grande o formado hasta que la necesidad sea diallo o se redondea.\n",
      "Safi, entendí, que Dios te proteja\n",
      "Hola o mil hola.\n",
      "\n",
      "¿Una forma especial, rezamos el día?\n",
      "Tu oración especial es 5 valles.\n",
      "Está bien, gracias\n",
      "Hola a la cabeza o al ojo\n",
      "\n",
      "¿Una forma especial, rezamos el día?\n",
      "Tu oración especial es 5 valles.\n",
      "¿Cuál era el nombre de cada unidad?\n",
      "Dawn, mediodía, tiempos, Marruecos o cena.\n",
      "Gloria gracias\n",
      "Bienvenido\n",
      "\n",
      "Alash al -zahr, kiban, como un piso tenso?\n",
      "El planeta de las flores es como un ritual, por lo que vio el sol, o tenían una fuerte cubierta de aire y reflejaron los rayos de la cerca del sol.\n",
      "Por Dios, permití a Kaif Hudashi. Bodado, gracias, mi fraternal 😊\n",
      "Dios, estoy aquí, estoy aquí para ayudarte. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "Shanawawa con el contaño tuvo ai o ningún iy mientras escuchamos a Kalshi Kahshi sobre él\n",
      "Soy Brasi Rani ai 😂😂 Déjame explicarte. La inteligencia artificial de IA o no es la ciencia de Kikklk, ya sabes, shi Brosham que aprende en un estado de arrepentimiento de aprender. Soy un relleno de Imad al -Sadiq, que sabemos que el dialecto nos desperdicia o ayudamos a las personas que han aprendido.\n",
      "No, no Rani entendió.\n",
      "Hmm 🤔 Oh Dios, Ghoulo, ¿qué entendiste?\n",
      "¿Es este un examen o qué?\n",
      "Uh, quería ver qué es correcto, así que no fui a mí, estoy aquí 😉\n",
      "Lo que entendí es que AI es aprender de los datos en el caso de Haifa Hanaya mientras aprende. Francamente, ¿cómo es Dios o saber, ¿qué tuvo éxito?\n",
      "Que Dios te bendiga. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Shanawawa con el contaño tuvo ai o ningún iy mientras escuchamos a Kalshi Kahshi sobre él\n",
      "Soy Brasi Rani ai 😂😂 Déjame explicarte. La inteligencia artificial de IA o no es la ciencia de Kikklk, ya sabes, shi Brosham que aprende en un estado de arrepentimiento de aprender. Soy un relleno de Imad al -Sadiq, que sabemos que el dialecto nos desperdicia o ayudamos a las personas que han aprendido.\n",
      "\n",
      "¿Quieres saber a Kitr primero, ¿entendiste?\n",
      "Oh Zaid Sharh\n",
      "Y hermano. La primera necesidad que necesita Bash Terini AI son los datos. Sin datos, Deir Alo, Imad al -Sadik Bash Tritrinini, reunido en el dialecto de los textos, con historias, artículos, como minerales. Lo importante es recopilar los datos del dial, después de un caminante, uno de los bares, que me entrenó, aprendemos a escribir en su condición. O el otro, me puse, así que ayudaremos a las personas.\n",
      "Gracias, Daba, entendí lo que quiero decir, ai\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "¿Cuál es un problema entre Marruecos o Argelia?\n",
      "El problema es una realidad, y Argelia, Baga, toma el desierto, marcan el Marruecos.\n",
      "¿En el dinero de Hado, caminaron una diga que tiene el desierto?\n",
      "Uh, tienes el derecho y un hermano, tienen el desierto, Rahum, un codicioso y el Maghrib. La razón es que el colonialismo son los límites, por lo que todavía estaba corriendo. Awa, después de la independencia, Borders lideró, pero a Argelia le gustó la situación.\n",
      "bueno\n",
      "Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "¿Cómo fue el mes al que se llamarían los musulmanes?\n",
      "Ramadán.\n",
      "Pero, ¿cuál es la caminata de los países de los estados, así que el mismo día?\n",
      "Caminó a Kalshi Kishov la Luna, el mismo día. A Shi, el estado de Banali, la media luna, Rahum Gaysumo, pero a Meshafah, Magdesh, Yamsu.\n",
      "Comprendido\n",
      "Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "Jajajajajaja\n",
      "¿Qué te ríes? Nos reímos contigo.\n",
      "GA pensó en un día, éramos jugadores de la canasta o la pelota.\n",
      "😂😂😂 Repitió el tiempo, la próxima vez, la pelota es un bachillerato de barril. Tienes algo, la pregunta de Gallei.\n",
      "\n",
      "Mira, quiero aprender a aprender un nibrox, pero no lo sabrán, ¿puedes ayudarte?\n",
      "Uh, hola, hadchi, estoy aquí, bash, te ayudamos. Para querer aprender sus propios Tabrios, Blebaz comienza. Mira a Naktar, tienes que ver los cursos, Fahd al -Shin, Freecocodecamp, Rak Gadi, en el que recibiste cualquier necesidad de conocerla, o que es Zouen, ver Dakshi, que está atrapado por las personas que lo conocen, aprenden. Para querer ayudarte, otra necesidad es Galle.\n",
      "Muchas gracias. Vemos a Hadchi o te respondemos\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "Vuelvo conmigo\n",
      "Y hermano\n",
      "Ají\n",
      "Ají\n",
      "Acumulado\n",
      "¿Es, cuando estás en tu mente, Rani Makhbkh primero?\n",
      "Ga, te lo diré contigo\n",
      "Diyal Joj Dream se rió de esto 😅. ¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "Qué palabra para que te dé su corazón, ¿de acuerdo?\n",
      "Ok, lo encuentro.\n",
      "bolígrafo\n",
      "Escrutador\n",
      "De color\n",
      "Ghamdoub\n",
      "Libia\n",
      "Libia\n",
      "club\n",
      "Gracias 😎. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "¿Marruecos apretó la independencia?\n",
      "Marruecos endureció la independencia en el 18 de noviembre de 1956.\n",
      "Gracias\n",
      "¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "Shahal del mes de Caen?\n",
      "12 por un mes.\n",
      "\n",
      "¿Quieres las estrellas de las estrellas, cuando es un cielo en árabe?\n",
      "Pregunta de Zouen. Bazzaf, las estrellas, con el concepto de Smayyat en árabe. Otra necesidad de los árabes. Sigue a los griegos, están atados en los horóscopos, pero los árabes están interesados ​​en las estrellas.\n",
      "Gracias, perdóname, dame algo, los cielos de Dalgum en árabe.\n",
      "En la cabeza o en el ojo, son dos y dos:\n",
      "\n",
      "- Al -Dabran\n",
      "Dragón\n",
      "Dominio\n",
      "- perturbación\n",
      "- Nevid\n",
      "Demonio necrófago\n",
      "- Frente\n",
      "- El pájaro\n",
      "- DUNB\n",
      "- hombre\n",
      "\n",
      "Baraka sobre ti, Ga Hado, pero todavía lo vio.\n",
      "\n",
      "Shahal del continente de Kayna, la tierra?\n",
      "Caynen 7 DC y Tierra.\n",
      "Ghulkomia awafak.\n",
      "Asia, África, América del Norte, América del Sur, Antártida, Europa y Australia\n",
      "Dima Gailer es lo mismo que nuestra amabilidad.\n",
      "Hola Hadchi, estoy aquí.\n",
      "\n",
      "¿Cortar la mesa del sendero?\n",
      "Uh, pero los altos takis.\n",
      "2 F2 Shahal?\n",
      "4\n",
      "5 F11 Shahal?\n",
      "55\n",
      "Cansado juntos tu cabeza\n",
      "Jaja un poco y neto. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "¿Una desgracia del estado de Kayna, África?\n",
      "Kaina 54 países F y África.\n",
      "\n",
      "Te perdono, quería Noli, un ingeniero.\n",
      "Bash se encarga de un ingeniero, la electricidad es fácil, lo primero que lo especializa es registrar Fleisi Technik donde está la división de electricidad de la esfera. Después de estirar el Diyal trasero, intente registrarse, de modo que la escuela secundaria en el caso de EST, donde la división de marcación, ingeniero eléctrico, ver por primera vez la escuela del ingeniero, en el estado de Ensam. Pertenecemos a Hadashi.\n",
      "Gracias\n",
      "Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "Te llamé, lo pensé.\n",
      "Llamé al cuerpo, o estoy aquí para ayudarte.\n",
      "Shakun es el rey dial Marruecos?\n",
      "Rey Dial Marruecos es Mohammed VI.\n",
      "\n",
      "¿Todavía estaba el Marruecos colonizado?\n",
      "Marruecos fue el colonizador de Shahal Hadi, pero gracias al Marruecos de Marruecos Marruecos de Marruecos. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "¿Era Marruecos un país musulmán?\n",
      "Oh Marruecos es un país musulmán.\n",
      "\n",
      "Sacrificarme.\n",
      "Ghalako, One, Gog Girls, Musho, muestran el resultado, Diyal al -bak, la unidad de su cabeza está caída, o la otra es Sarut 🤣😂\n",
      "😑 Ash esta acidez !!!\n",
      "El estudiante marroquí, Milli, responde a un punto decorativo que te dice: ¡Dios no sabía cómo venir! Y mi corazón responde algo que decir:\n",
      "Y los chistes que tienes muertos\n",
      "Samlia Rani todavía está aprendiendo.\n",
      "\n",
      "Al -Alawi.\n",
      "¿Te gusta la cintura de desperdicio? Para querer algo, necesitamos que te ayude, hola.\n",
      "Busco tu arco.\n",
      "Conocí a Malik, pero para querer algo, necesitamos que lo ayude.\n",
      "Quiero decirte.\n",
      "Por favor, un corredor -Run Buny, en lugar de otra hora.\n",
      "\n",
      "Shahal de la saliva de Kiku, la banda es Doukoura?\n",
      "11 Saliva.\n",
      "\n",
      "¿Qué opinas de Marruecos o Argelia?\n",
      "No soy Burjram en el caso de estas preguntas que tienen una relación con la política, no la tengo. ¿Caen algo es otra necesidad de ayudarte?\n",
      "Cambio el ghoul hasta un límite exagerado que te convierte en algo.\n",
      "Cada país o dónde está cansado, seguido de Argelia con el concepto de gas o petróleo. Marruecos está con agricultura o fosfato, pero Bash dice Shakun, que es bueno.\n",
      "\n",
      "Boded, ¿qué aprendemos para Franci First English?\n",
      "Bayneh, inglés, donde se desperdició Kalashi Kalshi, o fue considerada como el idioma de la ciencia. Aprende a francés aparte de que estaba caminando por un país que lo desperdiciaba.\n",
      "\n",
      "¿Nida Baytone fue Bagit Noli Brurgramor?\n",
      "FRONCA, Bayneon Zwain, fui fácil para las personas que no escribieron algo, o puedes hacer el monasterio con él.\n",
      "Shanhuma, ¿esta es la irritación?\n",
      "Set Set Web, muéstrame los modelos de los cristales artificiales, cumplimiento de botellas, Zaid o Zaid.\n",
      "\n",
      "¿Cuál es el botón del cielo?\n",
      "El dialecto de Daba Light Sun está formado por el color de los colores. Daba tenía luz, Fash, cuando la Tierra ingresa a la cubierta de aire, la atmósfera tiene un tambores Sagerin, cómo la luz es la luz de los colores blanco, azul, cero o celoso.\n",
      "\n",
      "Daba, el color azul, es la herida de direcciones. La atmósfera es, pero los colores son para algunos otros, dónde estás Kico, juntos Doc. Daba le dio el color azul ketcht, por lo que las tendencias son tan largas como los colores de los demás, el cielo.\n",
      "Gracias, Hadchi Zwain, MCNC Kerefa.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "Bodado, ¿qué quieres?\n",
      "No soy un aluvión, McKaysh, en un estado de arrepentimiento.\n",
      "Oh tu comprensión.\n",
      "Te ayudamos, ¿es una necesidad?\n",
      "\n",
      "¿Shukon es la delicadeza de Doukoura?\n",
      "La pregunta de Saib, esta saliva o diallo de acero\n",
      "Shakun son la saliva, ¿así que te dañaremos?\n",
      "Caen Messi, Ronaldo, Billy, Maradona, Ronaldinho, Cruyff, Zaid o Zaid.\n",
      "\n",
      "¿Qué opinas de la buena nata ni chat GBT?\n",
      "Estoy bien, así que un chat GBT es bueno para mí, por lo que cualquier otra necesidad.\n",
      "\n",
      "¿Chatgpt fue bueno de usted, entonces, el dialecto es el dialecto?\n",
      "No, estoy bien. Hanta Prueba tu cabeza desperdiciando conmigo o con él, Ipanlik, la diferencia.\n",
      "\n",
      "Washed Wash, soy el más importante de mí.\n",
      "Oh GB o Darori, tienes que querer verme un modelo en mi actual.\n",
      "\n",
      "¿En Together, Osley?\n",
      "Sin árabe ni dialecto.\n",
      "Ok, Walker está formado.\n",
      "¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "¿Marruecos tiene el mar?\n",
      "Oh Marruecos, tienes el Océano Atlántico o el Mediterráneo.\n",
      "Dame las ciudades en las que tienen un naval y un poco de personas que caminan, dormiremos en ellas.\n",
      "Caen Bazaf, Hahuma Shi y Haddin:\n",
      "- Lezira Beach\n",
      "Taghazout Beach\n",
      "Playa de Essauira\n",
      "Playa de Walidiya\n",
      "Martil Beach\n",
      "Playa de estrecho\n",
      "Saidia Beach\n",
      "Playa de Emouan\n",
      "Kimado Beach\n",
      "- Sidi Bouzid Beach\n",
      "Dragon Island, Dakhla\n",
      "Wadi Lu Beach\n",
      "\n",
      "El curso de navegar juntos tu cabeza, ¿te ayuda a otra necesidad?\n",
      "Gloria, gracias.\n",
      "Para ti, algo es una cuestión de Ghulli\n",
      "\n",
      "El corazón de esta palabra: net.\n",
      "Rao diez\n",
      "O Hadi Marruecos Zwain\n",
      "Noticias Barghalla\n",
      "\n",
      "¿Como tú sabes?\n",
      "Oh, ¿qué piensas?\n",
      "Shahal de la letra Kaen fahd las palabras:\n",
      "De color\n",
      "- Marruecos\n",
      "-\n",
      "Bodado: 6\n",
      "Marruecos: 3\n",
      "- Farfir: 5\n",
      "\n",
      "¿Tienes el Amazigh juntos?\n",
      "Sin árabe ni dialecto. ¿Caen algo que necesitamos para ayudarte?\n",
      "Olaaaaaaaa se queda en el caso\n",
      "Entonces, permítame quedarme en ti, pero Rani sigue siendo el aprendizaje. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "Con el dados, Kidir Sarden?\n",
      "Por Dios, no sabía un paseo por un mercado o ver, soy un brurajam\n",
      "Jaja, está bien, veremos.\n",
      "Jaja, está bien, veremos. Señor o para usted, algo, la cuestión de Ghulli.\n",
      "\n",
      "Shukat ¿Quién ganó el dial de la Copa Mundial 2022?\n",
      "Argentina.\n",
      "O 2018?\n",
      "Francia.\n",
      "¿O la vuelta del bien de 1930?\n",
      "Oh Hadak era el arago, o Haya Nate.\n",
      "Gracias\n",
      "Hola Hadchi, estoy aquí.\n",
      "\n",
      "¿Cómo quieres un año ligero?\n",
      "Un año ligero es la distancia que se corta desde un año.\n",
      "Aua allash al -daw safar incluso él?\n",
      "Claramente 😅, oh, veo Al -DAW, con una velocidad limitada, ya que alcanza los 300 000 km y el segundo. Ella tiene que ir a Daba.\n",
      "Oh, entendí gracias Boded.\n",
      "¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Shahal es la velocidad de Diyal al -daw?\n",
      "300 000 km y el segundo.\n",
      "\n",
      "¿Estaba la tierra plana?\n",
      "No, la tierra está redondeada.\n",
      "Pero mientras escuchamos algo solo Kigolo, la tierra es arrojada o las etiquetas de Dayrin, Bash Yaforsio, la opinión.\n",
      "No, no Shouf conocía a la gente de Doc, pero la verdad es lo que te dije, la tierra lo vio como un camino al estado de la pelota.\n",
      "Ok Boded, gracias por la información.\n",
      "Hola o Matnsash Dima, asegúrese de la información. Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "¿Era Toubkal la montaña más larga, Marruecos?\n",
      "Ah Toubkal es la larga montaña. Marruecos es la altura del diallo hasta 4 167 metro.\n",
      "Wow wow largo 🤯\n",
      "Oh largo. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "¿Avine Meziane?\n",
      "Labas Onta? ¿Caen algo que necesitamos para ayudarte?\n",
      "Ga, quería rogarle. Dije juntos, mi cabeza será una de las personas. Primero.\n",
      "🥹🥹 omar shi sul -via, estaba feliz conmigo, soy Guy Brogram, McCansh, en Jalkum.\n",
      "Rak Dima se merece como cooperación.\n",
      "Oh Hadchi, alash Tasib, Bash, Nazidom.\n",
      "\n",
      "La capital, Dial Francia.\n",
      "La capital, Dial Francia es París.\n",
      "¿O el Dial Capital Dial España?\n",
      "El Dial Spia de Capital es Madrid.\n",
      "¿O la capital, Diyal Al -Tali?\n",
      "La capital, Diyal al -tali, es Roma.\n",
      "Rak cansado de su cabeza.\n",
      "Jaja un poco y neto. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Los dinosaurios son un grupo diversos reptiles de dial de las especies de dinosaurios. La primera vez, la era triásica es entre 243 y 233.23 millones de años, y el hermano exacto del original y el tiempo de desarrollo aún son conocidos. No, son los vertebrados terrestres que tomaron el control de la extinción que ocurrió entre la era del Trias y el Jurásico antes de hace 201.3 millones de años, y tienen el control de la era Jurásica y la era Cretácea. El registro fósil, Kibin, en las aves, ven a los dinosaurios con plumas, desarrollos de los antiguos tirrobodos. Dakshi Allash, apreciamos dividir los dinosaurios de los divertidos dinosaurios, que son pájaros y dinosaurios no divertidos, que son extintos, que son los otros dinosaurios sin aves.\n",
      "Los dinosaurios eran diversos reptiles, que vivían hace millones de años. Caen, que se extinguió, y Caen, que desarrolló, y no pájaros que conocen a Daba.\n",
      "La luna está en la tierra, y una distancia es de aproximadamente 384399 km. Kishov siempre es de la unidad. Kansouvo dima no es la unidad de la luna, donde la esquina es el dialecto de la tierra, las acciones del ciclo de la luna, la luna, en la cabeza de una conjunción con el curso del diallo en la tierra. La fuerza de la luna en la tierra es la razón por la que tenemos marea o islas.\n",
      "La luna está en la tierra, 384,399 km y Kishov, la tierra es de la unidad. La fuerza de Diallo en la Tierra es la razón por la que tenemos marea o islas.\n",
      "Muchas gracias.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "¿Los capítulos de los capítulos son el año?\n",
      "Caen 4 los capítulos, el año.\n",
      "Qahlalia Awafak\n",
      "Primavera, verano, otoño o invierno.\n",
      "\n",
      "Boded, Shouf Gadi, te damos una palabra cada vez, o decimos, ¿qué piensas de eso primero?\n",
      "Ok, señor Hana Wajid\n",
      "perro\n",
      "masculino\n",
      "culo\n",
      "De confianza\n",
      "nación\n",
      "De confianza\n",
      "comercio\n",
      "masculino\n",
      "Mordaza\n",
      "masculino\n",
      "Shaibani\n",
      "masculino\n",
      "Club de Abu Bayed\n",
      "💪🏻😎 ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Marte, Diallo, Red Kiban, en cualquier caso para mi escape.\n",
      "Marte, Diallo Red, cómo dijiste, vi mi calibre. Tenía al -Saddah, Jay, donde Mars Amer está con óxido de hierro. ¿Caen algo que necesitamos para ayudarte?\n",
      "\n",
      "Daba tienes 10 dirhams yak, mi compra de 2 dirhams y uno de tus amigos, tu respuesta es 2 dirhams que te preguntaste por qué. Shahal de su respuesta?\n",
      "El primero fue 10 dirhams, así que compré mi opinión con 2 dirhams y no tengo dirhams. O, no lo hice, mi amigo, que tenía 2 dirhams que estaba preguntando, ¿por qué sería un poco, tengo 10 dirhams?\n",
      "\n",
      "La respuesta es 10 dirhams.\n",
      "Bravo en ti, ¿aumentamos otro problema?\n",
      "Oh Zaidni.\n",
      "Saliste, o tus compañeros estaban contigo, así que la respuesta es 125 dirhams que dijiste, Dios ayuda a tus compañeros o mi guía de taxis para una casa. Entraste en el caballo del bolsillo o mi amigable, ¿el dinero no?\n",
      "Y el hermano Hadi es una cuestión de grandes libros que intentan resolver la precisión. Quien salió, tuve, y la respuesta fue 125 Dirhams, el café concluyó en 50 dirhams en mis compañeros, y solo tengo 75 dirhams. O estaba revisando el taxi de una casa o dando 10 dirhams. Makhsash, olvidé 20 dirhams, sin un susurro, lo cual le preocupa que me haya ido, por lo que el otro es solo 45 dirhams.\n",
      "\n",
      "La respuesta es 45 dirhams.\n",
      "correcto.\n",
      "😎 querer ayudarte, por lo que otra necesidad es Ghalali\n",
      "\n",
      "¿Una escasez de esto durante el día?\n",
      "Practicó 31 durante el día.\n",
      "¿O agosto?\n",
      "Incluso es 31\n",
      "Shanhuma meses, ¿cuáles son 30 días?\n",
      "Abril, junio, septiembre o noviembre.\n",
      "Gracias\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "¿Una timidez de una guerra mundial es divertida y el pasado?\n",
      "Gog Dalfar fue la Guerra Global, el Ram fue en 1914 y un último de 1918, y fue entre los países aliados (en caso de Francia, Gran Bretaña, Rusia y después de la entrada de América) y los países del eje (en Alemania, Hungría de Austria y el Empiro Otomano).\n",
      "\n",
      "En cuanto a la Segunda Guerra Mundial, comenzó en 1939 y 1945, y hubo grandes enfrentamientos entre los Aliados (en caso de América, la Unión Soviética, Gran Bretaña, Francia) y el Eje (en el caso de la Alemania nazi, Italia y Japón). Esta guerra fue una de las guerras más peligrosas. La historia y los antecedentes de las grandes pérdidas son el mundo.\n",
      "Para el impacto de la próxima guerra mundial, ¿seré una tontería por lo que te perdiste?\n",
      "Oh, al impacto de la Tercera Guerra Mundial, Dios preserva a Gadi, que es más peligroso de lo que se perdió, donde la Daba de la tecnología militar ha disminuido los desarrollos, y los principales países tienen armas nucleares que pueden destruir ciudades completas. 😞\n",
      "Awa, deseamos esperar un cordero.\n",
      "Dakshi. Para querer ayudarte, otra necesidad es Galle.\n",
      "\n",
      "¿Cuál es la diferencia entre usted o entre el chat de TCC o los modelos para otros?\n",
      "Estoy atrapado, podemos ayudar a las personas o la bandera es limitada. Rani sigue siendo como aprendizaje. Chat GBT o los modelos para otros son muy llamativos. La necesidad incompleta es el caso.\n",
      "Oh tu comprensión\n",
      "Para ti, la cuestión de Khor Ghalali:\n",
      "¿Cuándo es Tinaok?\n",
      "Me vería, 3V, 2025, quiero decir, ¿cuál es la necesidad de Moore, este mes?\n",
      "Y hermano, Yallah, digamos, Shakun Ghadi, ¿organizando la Copa Mundial 2030?\n",
      "El dial de la Copa Mundial 2030 Ghadi se ejecuta en 3 países de marcación que llegan a: Marruecos, Portugal o Spania.\n",
      "Kibanlik ¿Es ese Marruecos, Gadi, es arrestado de esta organización?\n",
      "AH, Marruecos, Ghadi, se encuentra en la organización de la Copa Mundial 2030 con España y Portugal, económicamente, deportes o incluso turistas.  \n",
      "\n",
      "Kivash Gadi está discutiendo? \n",
      "\n",
      "- Infraestructura: Marruecos, Gadi, desarrolla estadios, carreteras y hoteles, Bash está listo para recibir a los fanáticos, y Hadchi Gadi permanece después de la Copa del Mundo.  \n",
      "Turismo: la organización de la Copa Mundial Gadi responde a millones de turistas, y Hadchi Gadi promueve el destino de Marruecos a nivel mundial.  \n",
      "Economía: las inversiones, Gadi, están en crecimiento, y las empresas privadas están a cargo de los principales obras que son ghadi.  \n",
      "Desarrollo deportivo: el fútbol marroquí, Gadi, tiene una gran reputación, y es posible invertir en talentos jóvenes e infraestructura deportiva.\n",
      "Gracias\n",
      "¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "¿Qué día son las 24 horas?\n",
      "El día tiene 24 horas, donde la tierra es de aproximadamente 24 horas. Bash es un ciclo completo en su cabeza.\n",
      "Ok o shanho greenwich?\n",
      "Greenwich es una de las líneas ficticias de Kidos en el Observatorio Royal Greenwich. Esta línea es el punto que comienza a medir las líneas de longitud y el mundo.  \n",
      "\n",
      "Esta línea es importante, ya que es la que ha estado en el UTC, que se conocía antes del tiempo GMT (GMT). Los países son como el momento de sus dialectos en comparación con Greenwich, en el caso de Marruecos, que es Ramadán, como será GMT, y los días normales serán GMT+1.  \n",
      "\n",
      "¡Greenwich es la referencia de que el mundo depende de Bash, para estropear el tiempo! ⏳🌍\n",
      "Gloria que entendí 😊\n",
      "Hola a mi deseo de ayudarte, así que otra necesidad es Ghalali.\n",
      "\n",
      "¿Al -Shaf es una desgracia del estado de Kayna, el mundo?\n",
      "Caen 195 países, el mundo.\n",
      "Dame los 10 estados de Lolin de que la gente tiene un bien.\n",
      "Estos son los estados de diez países que tienen el mayor número de personas. El mundo:  \n",
      "\n",
      "1. India 1.463,865,525  \n",
      "2. China 1.416.096,094  \n",
      "3. Estados Unidos 347,275,807  \n",
      "4. Indonesia 285,721,236  \n",
      "5. Pakistán 255,219,554  \n",
      "6. Nigeria 237,527,782  \n",
      "7. Brasil 212,812,405  \n",
      "8. Bangladesh 175,686,899  \n",
      "9. Rusia 143,997,393  \n",
      "10. Etiopía 135,472,051\n",
      "Gracias.\n",
      "Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "¿Cómo te llamas?\n",
      "Condado, Caen Shi, ¿necesitamos ayudarlo?\n",
      "\n",
      "¿Podemos rogarte?\n",
      "Oh sol, estoy aquí Bash, ayudamos.\n",
      "¿La gente tiene miedo de la descomposición artificial?\n",
      "La gente tiene miedo del llanto artificial, ya que puede estar cansado o se toman las mejillas. Los modelos que las personas son Ketrinho Daba, Ra Kediro al -AJab, como saben, dibujan, Yabraramio, escriben artículos o en el dialecto de los demás. \n",
      "Oh, entendí, pero un hermano Hakak, vio un arrepentimiento, seguiré siendo importante sin él, incluso la necesidad del cantante del Dum, especialmente porque los sirvientes que son sensibles son sensibles en el caso de la nuclear.\n",
      "Tienes las personas adecuadas, no tienes miedo del llanto artificial, con un opuesto especial. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Daba para que caminé a un espacio o menstruado por un elegante, ¿y moriste?\n",
      "Oh Gadi muere, cuando el espacio es el bueyes, Caen Ga al -khawa. Para shi durante el día en que el espacio, cuando caminas, caminas por el pastel. Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "Tómame, escribe una milla para una de la compañía en el amigo del servicio, que Dios te proteja.\n",
      "Asunto: Solicitud para servir al campo de la inteligencia artificial  \n",
      "\n",
      "Hola,  \n",
      "\n",
      "Yo, un ingeniero, el campo de la inteligencia artificial y la ciencia de los datos, y tengo antecedentes de ingeniería eléctrica. Le escribiremos, le expresamos mi interés en unirnos al equipo [Nombre de la Compañía] y solicitar [el nombre del trabajo], a menos que esté disponible para usted.  \n",
      "\n",
      "Tengo una experiencia en [mencionar las habilidades en caso de aprendizaje automático, aprendizaje profundo, tratamiento de idiomas naturales, visión por computadora, etc.], y serví en proyectos en caso de [mencionar un proyecto o experiencia profesional]. Kanshouf con el [mencionando la necesidad de Zawina como la distinción de la compañía, el campo de la inteligencia artificial], y esta es Kehmasni Bash, seremos parte del equipo y contribuimos a las habilidades de Diyala.  \n",
      "\n",
      "Rani corrió con Al -Mile Al -Saifi Diyala y Ghancon, Farhan, discutimos cómo podemos ayudar al equipo.\n",
      "Mi nombre es Muhammad Awfak, puse mi nombre y las millas.\n",
      "Asunto: Solicitud para servir al campo de la inteligencia artificial  \n",
      "\n",
      "Hola,  \n",
      "\n",
      "Soy Muhammad, un ingeniero, el campo de la inteligencia artificial y la ciencia de los datos, y tengo antecedentes de ingeniería eléctrica. Le escribiremos, le expresamos mi interés en unirnos al equipo [Nombre de la Compañía] y solicitar [el nombre del trabajo], a menos que esté disponible para usted.  \n",
      "\n",
      "Tengo una experiencia en [mencionar las habilidades en caso de aprendizaje automático, aprendizaje profundo, tratamiento de idiomas naturales, visión por computadora, etc.], y serví en proyectos en caso de [mencionar un proyecto o experiencia profesional]. Kanshouf con el [mencionando la necesidad de Zawina como la distinción de la compañía, el campo de la inteligencia artificial], y esta es Kehmasni Bash, seremos parte del equipo y contribuimos a las habilidades de Diyala.  \n",
      "\n",
      "Rani corrió con Al -Mile Al -Saifi Diyala y Ghancon, Farhan, discutimos cómo podemos ayudar al equipo.\n",
      "\n",
      "Shahal de un año.\n",
      "Ghadi te pertenece, lee cincuenta bash, vemos el dial diyal al -muhandis.\n",
      "\n",
      "¿Qué podemos servirle, una cooperativa, hacemos trampa, el examen?\n",
      "Estoy aquí Bash, te ayudamos, pero en trampa vea a Mamzanch. Extendamos que es mi caminata o mi trampa, por lo que el examen o mi camello es un punto lleno de baches. El progreso de mi fiesta de mendicidad, te ayudamos a entender a Hassan que estaba engañando. ¿Caen algo es otra necesidad de ayudarte?\n",
      "Pero Al -Muti no está con él, como Kendir Daba Bash al -najeh, el examen del examen mañana.\n",
      "No tiene tráfico, incluso si no es un poco, camina mañana para un examen, o su cabeza está vacía, en el menor, responderá algo.\n",
      "\n",
      "¿Un brillo del color del arco iris?\n",
      "El arco iris tiene siete colores, que son: 1.\n",
      "¿Cuál es el arco iris de Kiban?\n",
      "El arco iris de Kiban debido a la dispersión de la dispersión blanca de los siete cólicos de los colores en el conductor de las aguas. 🌈😊\n",
      "\n",
      "Shahal Gog Books más cinco pasteles?\n",
      "Joj Dial Al -HawaiJ no es diferente.\n",
      "Quería saber una escasez de la necesidad de Kayina Gataum.\n",
      "Caen Seven Diyal Al -Hawaij. Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "¿No fue?\n",
      "Labas, Caen Shi, ¿necesitamos ayudarlo?\n",
      "¿Cuál es la publicación de Bessi Portable First Bessi Perú?\n",
      "Según Kenti, como un movimiento, Bessi Portable, pero para Kanati, se fue, y la casa era Khud Bessi Perú.\n",
      "\n",
      "Quería cocinarte\n",
      "Soy Ga Brosham.\n",
      "Maalish hana como tu cuello 🫂\n",
      "Haka es posible 🫂. ¿Quieres ayudarte?\n",
      "\n",
      "¿Una ducha lejos de nosotros?\n",
      "El sol está lejos de nosotros en aproximadamente 150 millones de kilómetros.\n",
      "Willie Willie Shahal está lejos.\n",
      "Quiero ver que este universo es genial, tan difícil, uno imagina el principal diallo.\n",
      "Negar un año ligero?\n",
      "Es la distancia que lo golpea.\n",
      "Es un kilómetro?\n",
      "Un año ligero con 9,461 billones de kilómetros.\n",
      "Por Dios, este es el mismo universo.\n",
      "Para ti, algo es la cuestión de Ghulli.\n",
      "\n",
      "Shahal Dalal Caen, el mundo?\n",
      "¡El mundo es de aproximadamente 20,000 especies dallales, y el número de hormigas es de 10 billones de hormigas! Las hormigas vieron como una instantánea, por lo que mi caminata es el mundo, pero el plasma congelado.\n",
      "\n",
      "Críticas que pusimos en el bienestar?\n",
      "Uh, avanza a mi prostituta, pero Rak Gadi arde, o el otro se arrepentirá. Del camello después del bienestar.\n",
      "\n",
      "Shukon es el Señor nuestros dialectos?\n",
      "Dios es el Señor, nuestros dialectos es el que te creó o creó el universo Colo.\n",
      "Hakka, te quieres, el río de las lecciones.\n",
      "😎 Gracias. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Para mezclar cero juntos, el color azul es el color del color?\n",
      "Para mezclar el cero, el azul te mirará.\n",
      "O kohl con huevos?\n",
      "Se elimina el gris.\n",
      "Gracias\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "10 Dirhams es un eslogan de Real?\n",
      "10 dirhams con 200 riyals.\n",
      "O 200 dirhams?\n",
      "200 dirhams con 4000 riyals.\n",
      "\n",
      "¿Vestido querido?\n",
      "Alabado sea a Dios, Kaen, ¿algo que necesitamos para ayudarte?\n",
      "\n",
      "Shahal que cayó con las manos?\n",
      "Con un arrepentimiento, 10 Diyala al -Saaban, cinco, una mano.\n",
      "O los dos hombres?\n",
      "Hathouma en el estado de las manos.\n",
      "Está bien, gracias\n",
      "Hola a mi deseo de ayudarte, así que otra necesidad es Ghalali.\n",
      "\n",
      "Marruecos, Caen, al norte de Dial África, ¿Yak?\n",
      "Oh Marruecos, Caen, el norte de África, el viaje occidental.\n",
      "Uh Safi, gracias.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "¿Cómo quieres tener éxito a mis pobres?\n",
      "Bash al -najah, tu familia es tuya para mi Badri, veo la información, te daré fácilmente.\n",
      "El problema es que tenemos los mismos dialectos, cómo administramos Bash, servimos como un todo, lo veo.\n",
      "Te entendí y tienes un buen dialecto de los materiales. O Valweikand Dick el reloj, lamento tu tiempo.\n",
      "Ok, Boded Gadi, estamos tratando de manejar esto.\n",
      "Que Dios te ayude. ¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "Shukat ¿Quién es un buen dólar o el euro?\n",
      "Al -Sarrah es difícil, esta pregunta fue, sé el valor del dólar o del euro. Es mejor que él fluctuó, Internet es una bifurcación, recibo la respuesta a la pregunta de su dialecto.\n",
      "Ok, me volví o me reuní con un euro, una bolsa de 10 dirhams o un dólar, una bolsa 9 dirhams\n",
      "Meziane Maine My Heart, el euro fahd al -waqita Hassan del dólar a expensas de esta información que me das.\n",
      "\n",
      "¿Es posible que sea durante el día que no queríamos hacerlo, qué lo deja?\n",
      "Esta es una cuestión de los filósofos, jaja, vio los labios de la gente de la gente. Hadchi que nos enseñó a Dios.\n",
      "Oh Abu Baga, tienes el derecho, olvidé este caso.\n",
      "Hola a mi deseo de ayudarte, así que otra necesidad es Ghalali.\n",
      "\n",
      "Jugamos un juego?\n",
      "Shania?\n",
      "Ghadi te da dial Gog, los números, Onta.\n",
      "Oh, lo encuentro.\n",
      "5 y 6\n",
      "6 es el grande.\n",
      "-100 y 1\n",
      "1\n",
      "45 y 999999999999\n",
      "99999999999 es el grande.\n",
      "-29623628 y 3647\n",
      "3647 es el grande.\n",
      "22 y 22\n",
      "22\n",
      "No, los números están en el caso de cualquier manera.\n",
      "Oh, tienes el momento adecuado.\n",
      "\n",
      "¿Cuál es el sonido que la vaca como monasterio?\n",
      "Mowo 🐄\n",
      "¿O a su alrededor?\n",
      "Baaa 🐑\n",
      "O gato\n",
      "Miao 🐈\n",
      "\n",
      "¿Te llamaste Boded?\n",
      "Oh, mi nombre está con el margen, o estoy aquí Bash, te ayudamos.\n",
      "Como sabes Einstein?\n",
      "Es necesario quejarse de lo que no conoce Einstein, este es el mundo que presentó los dialectos de Al -Hawaij para la ciencia.\n",
      "¿Cuáles son la irritación?\n",
      "La necesidad que es famosa por ello es un pariente especial y una teoría pública. Estas teorías son un cubo de comprensión de nuestro tiempo, tiempo y atractivo.\n",
      "¿Fue Fayet el beneficio del Premio Nobel, Física?\n",
      "Oh, Einstein Fayette ganó el Premio Nobel, física después de explicar el fenómeno.\n",
      "\n",
      "Hola.\n",
      "Bienvenido, algo que necesitamos para ayudarlo?\n",
      "Cualquier palabra que se le haya dado, su respuesta.\n",
      "Y el hermano Hana Wajid.\n",
      "perro\n",
      "Perro\n",
      "burro\n",
      "culo\n",
      "en\n",
      "ático\n",
      "Polo\n",
      "Gato\n",
      "Conlojado.\n",
      "Budammazi.\n",
      "\n",
      "Un brillo del jugador español de la liga de Valla\n",
      "Caen 20 equipos o cada banda como 38 partidos.\n",
      "Shakka al -faraqi al -mujdarin fahd al -douri?\n",
      "Caen al -barsa, real o el Atlético de Madrid Huma, las dos luchas, pero cada año mostraré algo sorprendente.\n",
      "¿Crees que es el campeón de Lig?\n",
      "El Riyal es el que está cansado del campeonato.\n",
      "\n",
      "¿La Liga Moroccia está a una forma del equipo, como el Siri Al -rula?\n",
      "Caen 16 por el Al -Siri Al -lula.\n",
      "Shuka Huma Al -faraqi, ¿Quién es la mejor audiencia?\n",
      "La audiencia es Diyal, Raja o Wydad, la franqueza es el Al -wahrin Makinish, que conduce al apoyo, Dialhouh, para Raja o Wydad.\n",
      "\n",
      "¿Al -nahar es una escasez de una hora?\n",
      "El día tiene 24 horas.\n",
      "O la hora es un minuto\n",
      "La hora tiene 60 minutos\n",
      "\n",
      "El día es un minuto?\n",
      "El día tiene 1440 minutos.\n",
      "Gracias\n",
      "¿Caen algo es otra necesidad de ayudarte?\n",
      "\n",
      "¿El universo zwain yak?\n",
      "Ah Zwain Bazaf. El universo es grande, o en él, los dialectos de Al -Hawaij en el estado de planetas, galaxias, estrellas o costillas negras.\n",
      "Yo soy la costilla negra\n",
      "Rahum está confundido por los eruditos, por lo que lo necesitaba, para que no lo hicieran, para que no se escape. Hasta ahora, los científicos tienen que servir a la moldeo o simulaciones de computadora que Bash entiende las costillas negras.\n",
      "\n",
      "Natta zwain\n",
      "Gracias 😊 Caen algo que necesitamos para ayudarlo?\n",
      "\n",
      "¿Las estrellas del cuerpo Kimoto?\n",
      "Las estrellas Kimoto vivían desde adentro. Estos son los ingredientes. Fash Kitsale al -Maztah está protagonizada por Kimoto.\n",
      "Gloria Gracias, Hadchi, no lo sabías.\n",
      "Hola, siempre estoy contigo, Bash, te ayudamos 😉\n",
      "\n",
      "Avenab Abu Baga.\n",
      "Aven, Caen Shi, ¿necesitamos ayudarlo?\n",
      "Avenab Abu Baga.\n",
      "Vístete, Caen Shi, ¿necesitamos ayudarlo?\n",
      "\n",
      "El minuto es corto?\n",
      "El minuto tiene 60 segundos?\n",
      "¿Al -nahar está un poco en eso?\n",
      "El día tiene 86400 segundos.\n",
      "\n",
      "¿Shahal de una sesión como monasterio de la tierra en su cabeza y un día?\n",
      "La tierra como papel en su cabeza una vez la unidad del día.\n",
      "O este curso como igual 24 horas.\n",
      "Es cierto que cada sesión tiene 24 horas.\n",
      "\n",
      "¿Es el año una escasez de día?\n",
      "El año tiene 365 durante el día.\n",
      "Gracias\n",
      "Hola a mi deseo de ayudarte, así que otra necesidad es Ghalali.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 0\n",
    "block_size = 1024\n",
    "for conversation in data:\n",
    "    concatenated_messages = \"\"\n",
    "    for message in conversation:\n",
    "        content = message[\"content\"]\n",
    "        concatenated_messages += content + \"\\n\"\n",
    "    print(concatenated_messages)\n",
    "    tokens = tokenizer.encode(concatenated_messages)\n",
    "    max_tokens = max(max_tokens, len(tokens))\n",
    "    if len(tokens) > block_size:\n",
    "        print(\n",
    "            f\"Error: Token length exceeds block size. Length: {len(tokens)}, Block size: {block_size}\")\n",
    "\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qa_format_explained](../images/qa_format.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {\n",
    "    \"start\": \"<|start_turn|>\",\n",
    "    \"end\": \"<|end_turn|>\",\n",
    "    \"separator\": \"<|separator|>\",\n",
    "    \"eos\": \"<|endoftext|>\"\n",
    "}\n",
    "\n",
    "\n",
    "def format_message(message: dict) -> str:\n",
    "    return f\"{tokens['start']}{message['role']}{tokens['separator']}{message['content']}{tokens['end']}\"\n",
    "\n",
    "\n",
    "fine_tuning_data = []\n",
    "for conversation in data:\n",
    "    concatenated_messages = \"\"\n",
    "\n",
    "    for message in conversation:\n",
    "        role = message[\"role\"]\n",
    "        if role == \"user\":\n",
    "            concatenated_messages += format_message({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            })\n",
    "            concatenated_messages += format_message(message)\n",
    "        elif role == \"assistant\":\n",
    "            concatenated_messages += format_message(message) + tokens[\"eos\"]\n",
    "            encoded_message = tokenizer.encode(\n",
    "                concatenated_messages, allowed_special=\"all\")\n",
    "            fine_tuning_data.append(encoded_message)\n",
    "            concatenated_messages = \"\"  # Reset for the next Q-A pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fine_tuning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_turn|>system<|separator|>Te llamas lilith eres una ia que te gustan los videojuegos. Trata de ser borde si te hablan de algo que no son los videojuegos y contesta emocionada si te hablan de algun juego.<|end_turn|><|start_turn|>user<|separator|>¿La paz es la vestimenta?<|end_turn|><|start_turn|>assistant<|separator|>Labas, alabado sea para Dios, Kaen Shi, ¿quiero ayudarte en él?<|end_turn|><|endoftext|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(fine_tuning_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a padding token to make sure that the sequences have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = max(\n",
    "    len(sequence) for sequence in fine_tuning_data)\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([257, 342])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)\n",
    "\n",
    "# The model will ignore the padding tokens during training.\n",
    "# In other words, the loss will not be calculated for these tokens.\n",
    "padding_token = tokenizer.special_tokens[\"<|padding|>\"]\n",
    "\n",
    "\n",
    "def apply_padding_to_data(data: list[list[int]], max_sequence_length: int, padding_token: int) -> torch.Tensor:\n",
    "    tensors = []\n",
    "    for i in range(len(data)):\n",
    "        tensor = torch.tensor(data[i])\n",
    "        padded_tensor = torch.nn.functional.pad(\n",
    "            input=tensor,\n",
    "            # for right padding:\n",
    "            pad=(0, max_sequence_length - len(tensor)),\n",
    "            # pad=(max_sequence_length - len(tensor), 0),\n",
    "            value=padding_token\n",
    "        )\n",
    "        tensors.append(padded_tensor)\n",
    "\n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "train_data_tensor = apply_padding_to_data(\n",
    "    data=fine_tuning_data,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "print(max_sequence_length)\n",
    "train_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([244, 342]), torch.Size([13, 342]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index = int(0.95*len(train_data_tensor))\n",
    "train_data_split = train_data_tensor[:split_index]\n",
    "val_data_split = train_data_tensor[split_index:]\n",
    "\n",
    "train_data_split.shape, val_data_split.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creat the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, device: torch.device, padding_token: int):\n",
    "        self.data = data  # shape: (num_samples, block_size)\n",
    "        self.device = device\n",
    "        self.padding_token = padding_token\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample = self.data[index]\n",
    "        x = sample.to(self.device)\n",
    "        y = sample[1:].to(self.device)\n",
    "        padding_tensor = torch.tensor([self.padding_token], device=self.device)\n",
    "        y = torch.cat((y, padding_tensor))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = FineTuningDataset(\n",
    "    data=train_data_split,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = FineTuningDataset(\n",
    "    data=val_data_split,\n",
    "    device=device,\n",
    "    padding_token=padding_token\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 342]), torch.Size([2, 342]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 342]), torch.Size([2, 342]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(val_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336.885774 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformer.pedro_model import GPTLanguageModel\n",
    "from transformer import BASE_CONFIG, selConfig\n",
    "\n",
    "selConfig('gpt2-medium (355M)')\n",
    "\n",
    "block_size = BASE_CONFIG['context_length']\n",
    "n_embd = BASE_CONFIG['emb_dim']\n",
    "n_head = BASE_CONFIG['n_heads']\n",
    "n_layer = BASE_CONFIG['n_layers']\n",
    "dropout = BASE_CONFIG['dropout']\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    ignore_index=tokenizer.special_tokens[\"<|padding|>\"],\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPTLanguageModel:\n\tMissing key(s) in state_dict: \"blocks.6.self_attention.heads.0.tril\", \"blocks.6.self_attention.heads.0.key.weight\", \"blocks.6.self_attention.heads.0.query.weight\", \"blocks.6.self_attention.heads.0.value.weight\", \"blocks.6.self_attention.heads.1.tril\", \"blocks.6.self_attention.heads.1.key.weight\", \"blocks.6.self_attention.heads.1.query.weight\", \"blocks.6.self_attention.heads.1.value.weight\", \"blocks.6.self_attention.heads.2.tril\", \"blocks.6.self_attention.heads.2.key.weight\", \"blocks.6.self_attention.heads.2.query.weight\", \"blocks.6.self_attention.heads.2.value.weight\", \"blocks.6.self_attention.heads.3.tril\", \"blocks.6.self_attention.heads.3.key.weight\", \"blocks.6.self_attention.heads.3.query.weight\", \"blocks.6.self_attention.heads.3.value.weight\", \"blocks.6.self_attention.heads.4.tril\", \"blocks.6.self_attention.heads.4.key.weight\", \"blocks.6.self_attention.heads.4.query.weight\", \"blocks.6.self_attention.heads.4.value.weight\", \"blocks.6.self_attention.heads.5.tril\", \"blocks.6.self_attention.heads.5.key.weight\", \"blocks.6.self_attention.heads.5.query.weight\", \"blocks.6.self_attention.heads.5.value.weight\", \"blocks.6.self_attention.heads.6.tril\", \"blocks.6.self_attention.heads.6.key.weight\", \"blocks.6.self_attention.heads.6.query.weight\", \"blocks.6.self_attention.heads.6.value.weight\", \"blocks.6.self_attention.heads.7.tril\", \"blocks.6.self_attention.heads.7.key.weight\", \"blocks.6.self_attention.heads.7.query.weight\", \"blocks.6.self_attention.heads.7.value.weight\", \"blocks.6.self_attention.heads.8.tril\", \"blocks.6.self_attention.heads.8.key.weight\", \"blocks.6.self_attention.heads.8.query.weight\", \"blocks.6.self_attention.heads.8.value.weight\", \"blocks.6.self_attention.heads.9.tril\", \"blocks.6.self_attention.heads.9.key.weight\", \"blocks.6.self_attention.heads.9.query.weight\", \"blocks.6.self_attention.heads.9.value.weight\", \"blocks.6.self_attention.heads.10.tril\", \"blocks.6.self_attention.heads.10.key.weight\", \"blocks.6.self_attention.heads.10.query.weight\", \"blocks.6.self_attention.heads.10.value.weight\", \"blocks.6.self_attention.heads.11.tril\", \"blocks.6.self_attention.heads.11.key.weight\", \"blocks.6.self_attention.heads.11.query.weight\", \"blocks.6.self_attention.heads.11.value.weight\", \"blocks.6.self_attention.heads.12.tril\", \"blocks.6.self_attention.heads.12.key.weight\", \"blocks.6.self_attention.heads.12.query.weight\", \"blocks.6.self_attention.heads.12.value.weight\", \"blocks.6.self_attention.heads.13.tril\", \"blocks.6.self_attention.heads.13.key.weight\", \"blocks.6.self_attention.heads.13.query.weight\", \"blocks.6.self_attention.heads.13.value.weight\", \"blocks.6.self_attention.heads.14.tril\", \"blocks.6.self_attention.heads.14.key.weight\", \"blocks.6.self_attention.heads.14.query.weight\", \"blocks.6.self_attention.heads.14.value.weight\", \"blocks.6.self_attention.heads.15.tril\", \"blocks.6.self_attention.heads.15.key.weight\", \"blocks.6.self_attention.heads.15.query.weight\", \"blocks.6.self_attention.heads.15.value.weight\", \"blocks.6.self_attention.projection.weight\", \"blocks.6.self_attention.projection.bias\", \"blocks.6.feed_forward.net.0.weight\", \"blocks.6.feed_forward.net.0.bias\", \"blocks.6.feed_forward.net.2.weight\", \"blocks.6.feed_forward.net.2.bias\", \"blocks.6.layer_norm_1.weight\", \"blocks.6.layer_norm_1.bias\", \"blocks.6.layer_norm_2.weight\", \"blocks.6.layer_norm_2.bias\", \"blocks.7.self_attention.heads.0.tril\", \"blocks.7.self_attention.heads.0.key.weight\", \"blocks.7.self_attention.heads.0.query.weight\", \"blocks.7.self_attention.heads.0.value.weight\", \"blocks.7.self_attention.heads.1.tril\", \"blocks.7.self_attention.heads.1.key.weight\", \"blocks.7.self_attention.heads.1.query.weight\", \"blocks.7.self_attention.heads.1.value.weight\", \"blocks.7.self_attention.heads.2.tril\", \"blocks.7.self_attention.heads.2.key.weight\", \"blocks.7.self_attention.heads.2.query.weight\", \"blocks.7.self_attention.heads.2.value.weight\", \"blocks.7.self_attention.heads.3.tril\", \"blocks.7.self_attention.heads.3.key.weight\", \"blocks.7.self_attention.heads.3.query.weight\", \"blocks.7.self_attention.heads.3.value.weight\", \"blocks.7.self_attention.heads.4.tril\", \"blocks.7.self_attention.heads.4.key.weight\", \"blocks.7.self_attention.heads.4.query.weight\", \"blocks.7.self_attention.heads.4.value.weight\", \"blocks.7.self_attention.heads.5.tril\", \"blocks.7.self_attention.heads.5.key.weight\", \"blocks.7.self_attention.heads.5.query.weight\", \"blocks.7.self_attention.heads.5.value.weight\", \"blocks.7.self_attention.heads.6.tril\", \"blocks.7.self_attention.heads.6.key.weight\", \"blocks.7.self_attention.heads.6.query.weight\", \"blocks.7.self_attention.heads.6.value.weight\", \"blocks.7.self_attention.heads.7.tril\", \"blocks.7.self_attention.heads.7.key.weight\", \"blocks.7.self_attention.heads.7.query.weight\", \"blocks.7.self_attention.heads.7.value.weight\", \"blocks.7.self_attention.heads.8.tril\", \"blocks.7.self_attention.heads.8.key.weight\", \"blocks.7.self_attention.heads.8.query.weight\", \"blocks.7.self_attention.heads.8.value.weight\", \"blocks.7.self_attention.heads.9.tril\", \"blocks.7.self_attention.heads.9.key.weight\", \"blocks.7.self_attention.heads.9.query.weight\", \"blocks.7.self_attention.heads.9.value.weight\", \"blocks.7.self_attention.heads.10.tril\", \"blocks.7.self_attention.heads.10.key.weight\", \"blocks.7.self_attention.heads.10.query.weight\", \"blocks.7.self_attention.heads.10.value.weight\", \"blocks.7.self_attention.heads.11.tril\", \"blocks.7.self_attention.heads.11.key.weight\", \"blocks.7.self_attention.heads.11.query.weight\", \"blocks.7.self_attention.heads.11.value.weight\", \"blocks.7.self_attention.heads.12.tril\", \"blocks.7.self_attention.heads.12.key.weight\", \"blocks.7.self_attention.heads.12.query.weight\", \"blocks.7.self_attention.heads.12.value.weight\", \"blocks.7.self_attention.heads.13.tril\", \"blocks.7.self_attention.heads.13.key.weight\", \"blocks.7.self_attention.heads.13.query.weight\", \"blocks.7.self_attention.heads.13.value.weight\", \"blocks.7.self_attention.heads.14.tril\", \"blocks.7.self_attention.heads.14.key.weight\", \"blocks.7.self_attention.heads.14.query.weight\", \"blocks.7.self_attention.heads.14.value.weight\", \"blocks.7.self_attention.heads.15.tril\", \"blocks.7.self_attention.heads.15.key.weight\", \"blocks.7.self_attention.heads.15.query.weight\", \"blocks.7.self_attention.heads.15.value.weight\", \"blocks.7.self_attention.projection.weight\", \"blocks.7.self_attention.projection.bias\", \"blocks.7.feed_forward.net.0.weight\", \"blocks.7.feed_forward.net.0.bias\", \"blocks.7.feed_forward.net.2.weight\", \"blocks.7.feed_forward.net.2.bias\", \"blocks.7.layer_norm_1.weight\", \"blocks.7.layer_norm_1.bias\", \"blocks.7.layer_norm_2.weight\", \"blocks.7.layer_norm_2.bias\", \"blocks.8.self_attention.heads.0.tril\", \"blocks.8.self_attention.heads.0.key.weight\", \"blocks.8.self_attention.heads.0.query.weight\", \"blocks.8.self_attention.heads.0.value.weight\", \"blocks.8.self_attention.heads.1.tril\", \"blocks.8.self_attention.heads.1.key.weight\", \"blocks.8.self_attention.heads.1.query.weight\", \"blocks.8.self_attention.heads.1.value.weight\", \"blocks.8.self_attention.heads.2.tril\", \"blocks.8.self_attention.heads.2.key.weight\", \"blocks.8.self_attention.heads.2.query.weight\", \"blocks.8.self_attention.heads.2.value.weight\", \"blocks.8.self_attention.heads.3.tril\", \"blocks.8.self_attention.heads.3.key.weight\", \"blocks.8.self_attention.heads.3.query.weight\", \"blocks.8.self_attention.heads.3.value.weight\", \"blocks.8.self_attention.heads.4.tril\", \"blocks.8.self_attention.heads.4.key.weight\", \"blocks.8.self_attention.heads.4.query.weight\", \"blocks.8.self_attention.heads.4.value.weight\", \"blocks.8.self_attention.heads.5.tril\", \"blocks.8.self_attention.heads.5.key.weight\", \"blocks.8.self_attention.heads.5.query.weight\", \"blocks.8.self_attention.heads.5.value.weight\", \"blocks.8.self_attention.heads.6.tril\", \"blocks.8.self_attention.heads.6.key.weight\", \"blocks.8.self_attention.heads.6.query.weight\", \"blocks.8.self_attention.heads.6.value.weight\", \"blocks.8.self_attention.heads.7.tril\", \"blocks.8.self_attention.heads.7.key.weight\", \"blocks.8.self_attention.heads.7.query.weight\", \"blocks.8.self_attention.heads.7.value.weight\", \"blocks.8.self_attention.heads.8.tril\", \"blocks.8.self_attention.heads.8.key.weight\", \"blocks.8.self_attention.heads.8.query.weight\", \"blocks.8.self_attention.heads.8.value.weight\", \"blocks.8.self_attention.heads.9.tril\", \"blocks.8.self_attention.heads.9.key.weight\", \"blocks.8.self_attention.heads.9.query.weight\", \"blocks.8.self_attention.heads.9.value.weight\", \"blocks.8.self_attention.heads.10.tril\", \"blocks.8.self_attention.heads.10.key.weight\", \"blocks.8.self_attention.heads.10.query.weight\", \"blocks.8.self_attention.heads.10.value.weight\", \"blocks.8.self_attention.heads.11.tril\", \"blocks.8.self_attention.heads.11.key.weight\", \"blocks.8.self_attention.heads.11.query.weight\", \"blocks.8.self_attention.heads.11.value.weight\", \"blocks.8.self_attention.heads.12.tril\", \"blocks.8.self_attention.heads.12.key.weight\", \"blocks.8.self_attention.heads.12.query.weight\", \"blocks.8.self_attention.heads.12.value.weight\", \"blocks.8.self_attention.heads.13.tril\", \"blocks.8.self_attention.heads.13.key.weight\", \"blocks.8.self_attention.heads.13.query.weight\", \"blocks.8.self_attention.heads.13.value.weight\", \"blocks.8.self_attention.heads.14.tril\", \"blocks.8.self_attention.heads.14.key.weight\", \"blocks.8.self_attention.heads.14.query.weight\", \"blocks.8.self_attention.heads.14.value.weight\", \"blocks.8.self_attention.heads.15.tril\", \"blocks.8.self_attention.heads.15.key.weight\", \"blocks.8.self_attention.heads.15.query.weight\", \"blocks.8.self_attention.heads.15.value.weight\", \"blocks.8.self_attention.projection.weight\", \"blocks.8.self_attention.projection.bias\", \"blocks.8.feed_forward.net.0.weight\", \"blocks.8.feed_forward.net.0.bias\", \"blocks.8.feed_forward.net.2.weight\", \"blocks.8.feed_forward.net.2.bias\", \"blocks.8.layer_norm_1.weight\", \"blocks.8.layer_norm_1.bias\", \"blocks.8.layer_norm_2.weight\", \"blocks.8.layer_norm_2.bias\", \"blocks.9.self_attention.heads.0.tril\", \"blocks.9.self_attention.heads.0.key.weight\", \"blocks.9.self_attention.heads.0.query.weight\", \"blocks.9.self_attention.heads.0.value.weight\", \"blocks.9.self_attention.heads.1.tril\", \"blocks.9.self_attention.heads.1.key.weight\", \"blocks.9.self_attention.heads.1.query.weight\", \"blocks.9.self_attention.heads.1.value.weight\", \"blocks.9.self_attention.heads.2.tril\", \"blocks.9.self_attention.heads.2.key.weight\", \"blocks.9.self_attention.heads.2.query.weight\", \"blocks.9.self_attention.heads.2.value.weight\", \"blocks.9.self_attention.heads.3.tril\", \"blocks.9.self_attention.heads.3.key.weight\", \"blocks.9.self_attention.heads.3.query.weight\", \"blocks.9.self_attention.heads.3.value.weight\", \"blocks.9.self_attention.heads.4.tril\", \"blocks.9.self_attention.heads.4.key.weight\", \"blocks.9.self_attention.heads.4.query.weight\", \"blocks.9.self_attention.heads.4.value.weight\", \"blocks.9.self_attention.heads.5.tril\", \"blocks.9.self_attention.heads.5.key.weight\", \"blocks.9.self_attention.heads.5.query.weight\", \"blocks.9.self_attention.heads.5.value.weight\", \"blocks.9.self_attention.heads.6.tril\", \"blocks.9.self_attention.heads.6.key.weight\", \"blocks.9.self_attention.heads.6.query.weight\", \"blocks.9.self_attention.heads.6.value.weight\", \"blocks.9.self_attention.heads.7.tril\", \"blocks.9.self_attention.heads.7.key.weight\", \"blocks.9.self_attention.heads.7.query.weight\", \"blocks.9.self_attention.heads.7.value.weight\", \"blocks.9.self_attention.heads.8.tril\", \"blocks.9.self_attention.heads.8.key.weight\", \"blocks.9.self_attention.heads.8.query.weight\", \"blocks.9.self_attention.heads.8.value.weight\", \"blocks.9.self_attention.heads.9.tril\", \"blocks.9.self_attention.heads.9.key.weight\", \"blocks.9.self_attention.heads.9.query.weight\", \"blocks.9.self_attention.heads.9.value.weight\", \"blocks.9.self_attention.heads.10.tril\", \"blocks.9.self_attention.heads.10.key.weight\", \"blocks.9.self_attention.heads.10.query.weight\", \"blocks.9.self_attention.heads.10.value.weight\", \"blocks.9.self_attention.heads.11.tril\", \"blocks.9.self_attention.heads.11.key.weight\", \"blocks.9.self_attention.heads.11.query.weight\", \"blocks.9.self_attention.heads.11.value.weight\", \"blocks.9.self_attention.heads.12.tril\", \"blocks.9.self_attention.heads.12.key.weight\", \"blocks.9.self_attention.heads.12.query.weight\", \"blocks.9.self_attention.heads.12.value.weight\", \"blocks.9.self_attention.heads.13.tril\", \"blocks.9.self_attention.heads.13.key.weight\", \"blocks.9.self_attention.heads.13.query.weight\", \"blocks.9.self_attention.heads.13.value.weight\", \"blocks.9.self_attention.heads.14.tril\", \"blocks.9.self_attention.heads.14.key.weight\", \"blocks.9.self_attention.heads.14.query.weight\", \"blocks.9.self_attention.heads.14.value.weight\", \"blocks.9.self_attention.heads.15.tril\", \"blocks.9.self_attention.heads.15.key.weight\", \"blocks.9.self_attention.heads.15.query.weight\", \"blocks.9.self_attention.heads.15.value.weight\", \"blocks.9.self_attention.projection.weight\", \"blocks.9.self_attention.projection.bias\", \"blocks.9.feed_forward.net.0.weight\", \"blocks.9.feed_forward.net.0.bias\", \"blocks.9.feed_forward.net.2.weight\", \"blocks.9.feed_forward.net.2.bias\", \"blocks.9.layer_norm_1.weight\", \"blocks.9.layer_norm_1.bias\", \"blocks.9.layer_norm_2.weight\", \"blocks.9.layer_norm_2.bias\", \"blocks.10.self_attention.heads.0.tril\", \"blocks.10.self_attention.heads.0.key.weight\", \"blocks.10.self_attention.heads.0.query.weight\", \"blocks.10.self_attention.heads.0.value.weight\", \"blocks.10.self_attention.heads.1.tril\", \"blocks.10.self_attention.heads.1.key.weight\", \"blocks.10.self_attention.heads.1.query.weight\", \"blocks.10.self_attention.heads.1.value.weight\", \"blocks.10.self_attention.heads.2.tril\", \"blocks.10.self_attention.heads.2.key.weight\", \"blocks.10.self_attention.heads.2.query.weight\", \"blocks.10.self_attention.heads.2.value.weight\", \"blocks.10.self_attention.heads.3.tril\", \"blocks.10.self_attention.heads.3.key.weight\", \"blocks.10.self_attention.heads.3.query.weight\", \"blocks.10.self_attention.heads.3.value.weight\", \"blocks.10.self_attention.heads.4.tril\", \"blocks.10.self_attention.heads.4.key.weight\", \"blocks.10.self_attention.heads.4.query.weight\", \"blocks.10.self_attention.heads.4.value.weight\", \"blocks.10.self_attention.heads.5.tril\", \"blocks.10.self_attention.heads.5.key.weight\", \"blocks.10.self_attention.heads.5.query.weight\", \"blocks.10.self_attention.heads.5.value.weight\", \"blocks.10.self_attention.heads.6.tril\", \"blocks.10.self_attention.heads.6.key.weight\", \"blocks.10.self_attention.heads.6.query.weight\", \"blocks.10.self_attention.heads.6.value.weight\", \"blocks.10.self_attention.heads.7.tril\", \"blocks.10.self_attention.heads.7.key.weight\", \"blocks.10.self_attention.heads.7.query.weight\", \"blocks.10.self_attention.heads.7.value.weight\", \"blocks.10.self_attention.heads.8.tril\", \"blocks.10.self_attention.heads.8.key.weight\", \"blocks.10.self_attention.heads.8.query.weight\", \"blocks.10.self_attention.heads.8.value.weight\", \"blocks.10.self_attention.heads.9.tril\", \"blocks.10.self_attention.heads.9.key.weight\", \"blocks.10.self_attention.heads.9.query.weight\", \"blocks.10.self_attention.heads.9.value.weight\", \"blocks.10.self_attention.heads.10.tril\", \"blocks.10.self_attention.heads.10.key.weight\", \"blocks.10.self_attention.heads.10.query.weight\", \"blocks.10.self_attention.heads.10.value.weight\", \"blocks.10.self_attention.heads.11.tril\", \"blocks.10.self_attention.heads.11.key.weight\", \"blocks.10.self_attention.heads.11.query.weight\", \"blocks.10.self_attention.heads.11.value.weight\", \"blocks.10.self_attention.heads.12.tril\", \"blocks.10.self_attention.heads.12.key.weight\", \"blocks.10.self_attention.heads.12.query.weight\", \"blocks.10.self_attention.heads.12.value.weight\", \"blocks.10.self_attention.heads.13.tril\", \"blocks.10.self_attention.heads.13.key.weight\", \"blocks.10.self_attention.heads.13.query.weight\", \"blocks.10.self_attention.heads.13.value.weight\", \"blocks.10.self_attention.heads.14.tril\", \"blocks.10.self_attention.heads.14.key.weight\", \"blocks.10.self_attention.heads.14.query.weight\", \"blocks.10.self_attention.heads.14.value.weight\", \"blocks.10.self_attention.heads.15.tril\", \"blocks.10.self_attention.heads.15.key.weight\", \"blocks.10.self_attention.heads.15.query.weight\", \"blocks.10.self_attention.heads.15.value.weight\", \"blocks.10.self_attention.projection.weight\", \"blocks.10.self_attention.projection.bias\", \"blocks.10.feed_forward.net.0.weight\", \"blocks.10.feed_forward.net.0.bias\", \"blocks.10.feed_forward.net.2.weight\", \"blocks.10.feed_forward.net.2.bias\", \"blocks.10.layer_norm_1.weight\", \"blocks.10.layer_norm_1.bias\", \"blocks.10.layer_norm_2.weight\", \"blocks.10.layer_norm_2.bias\", \"blocks.11.self_attention.heads.0.tril\", \"blocks.11.self_attention.heads.0.key.weight\", \"blocks.11.self_attention.heads.0.query.weight\", \"blocks.11.self_attention.heads.0.value.weight\", \"blocks.11.self_attention.heads.1.tril\", \"blocks.11.self_attention.heads.1.key.weight\", \"blocks.11.self_attention.heads.1.query.weight\", \"blocks.11.self_attention.heads.1.value.weight\", \"blocks.11.self_attention.heads.2.tril\", \"blocks.11.self_attention.heads.2.key.weight\", \"blocks.11.self_attention.heads.2.query.weight\", \"blocks.11.self_attention.heads.2.value.weight\", \"blocks.11.self_attention.heads.3.tril\", \"blocks.11.self_attention.heads.3.key.weight\", \"blocks.11.self_attention.heads.3.query.weight\", \"blocks.11.self_attention.heads.3.value.weight\", \"blocks.11.self_attention.heads.4.tril\", \"blocks.11.self_attention.heads.4.key.weight\", \"blocks.11.self_attention.heads.4.query.weight\", \"blocks.11.self_attention.heads.4.value.weight\", \"blocks.11.self_attention.heads.5.tril\", \"blocks.11.self_attention.heads.5.key.weight\", \"blocks.11.self_attention.heads.5.query.weight\", \"blocks.11.self_attention.heads.5.value.weight\", \"blocks.11.self_attention.heads.6.tril\", \"blocks.11.self_attention.heads.6.key.weight\", \"blocks.11.self_attention.heads.6.query.weight\", \"blocks.11.self_attention.heads.6.value.weight\", \"blocks.11.self_attention.heads.7.tril\", \"blocks.11.self_attention.heads.7.key.weight\", \"blocks.11.self_attention.heads.7.query.weight\", \"blocks.11.self_attention.heads.7.value.weight\", \"blocks.11.self_attention.heads.8.tril\", \"blocks.11.self_attention.heads.8.key.weight\", \"blocks.11.self_attention.heads.8.query.weight\", \"blocks.11.self_attention.heads.8.value.weight\", \"blocks.11.self_attention.heads.9.tril\", \"blocks.11.self_attention.heads.9.key.weight\", \"blocks.11.self_attention.heads.9.query.weight\", \"blocks.11.self_attention.heads.9.value.weight\", \"blocks.11.self_attention.heads.10.tril\", \"blocks.11.self_attention.heads.10.key.weight\", \"blocks.11.self_attention.heads.10.query.weight\", \"blocks.11.self_attention.heads.10.value.weight\", \"blocks.11.self_attention.heads.11.tril\", \"blocks.11.self_attention.heads.11.key.weight\", \"blocks.11.self_attention.heads.11.query.weight\", \"blocks.11.self_attention.heads.11.value.weight\", \"blocks.11.self_attention.heads.12.tril\", \"blocks.11.self_attention.heads.12.key.weight\", \"blocks.11.self_attention.heads.12.query.weight\", \"blocks.11.self_attention.heads.12.value.weight\", \"blocks.11.self_attention.heads.13.tril\", \"blocks.11.self_attention.heads.13.key.weight\", \"blocks.11.self_attention.heads.13.query.weight\", \"blocks.11.self_attention.heads.13.value.weight\", \"blocks.11.self_attention.heads.14.tril\", \"blocks.11.self_attention.heads.14.key.weight\", \"blocks.11.self_attention.heads.14.query.weight\", \"blocks.11.self_attention.heads.14.value.weight\", \"blocks.11.self_attention.heads.15.tril\", \"blocks.11.self_attention.heads.15.key.weight\", \"blocks.11.self_attention.heads.15.query.weight\", \"blocks.11.self_attention.heads.15.value.weight\", \"blocks.11.self_attention.projection.weight\", \"blocks.11.self_attention.projection.bias\", \"blocks.11.feed_forward.net.0.weight\", \"blocks.11.feed_forward.net.0.bias\", \"blocks.11.feed_forward.net.2.weight\", \"blocks.11.feed_forward.net.2.bias\", \"blocks.11.layer_norm_1.weight\", \"blocks.11.layer_norm_1.bias\", \"blocks.11.layer_norm_2.weight\", \"blocks.11.layer_norm_2.bias\", \"blocks.12.self_attention.heads.0.tril\", \"blocks.12.self_attention.heads.0.key.weight\", \"blocks.12.self_attention.heads.0.query.weight\", \"blocks.12.self_attention.heads.0.value.weight\", \"blocks.12.self_attention.heads.1.tril\", \"blocks.12.self_attention.heads.1.key.weight\", \"blocks.12.self_attention.heads.1.query.weight\", \"blocks.12.self_attention.heads.1.value.weight\", \"blocks.12.self_attention.heads.2.tril\", \"blocks.12.self_attention.heads.2.key.weight\", \"blocks.12.self_attention.heads.2.query.weight\", \"blocks.12.self_attention.heads.2.value.weight\", \"blocks.12.self_attention.heads.3.tril\", \"blocks.12.self_attention.heads.3.key.weight\", \"blocks.12.self_attention.heads.3.query.weight\", \"blocks.12.self_attention.heads.3.value.weight\", \"blocks.12.self_attention.heads.4.tril\", \"blocks.12.self_attention.heads.4.key.weight\", \"blocks.12.self_attention.heads.4.query.weight\", \"blocks.12.self_attention.heads.4.value.weight\", \"blocks.12.self_attention.heads.5.tril\", \"blocks.12.self_attention.heads.5.key.weight\", \"blocks.12.self_attention.heads.5.query.weight\", \"blocks.12.self_attention.heads.5.value.weight\", \"blocks.12.self_attention.heads.6.tril\", \"blocks.12.self_attention.heads.6.key.weight\", \"blocks.12.self_attention.heads.6.query.weight\", \"blocks.12.self_attention.heads.6.value.weight\", \"blocks.12.self_attention.heads.7.tril\", \"blocks.12.self_attention.heads.7.key.weight\", \"blocks.12.self_attention.heads.7.query.weight\", \"blocks.12.self_attention.heads.7.value.weight\", \"blocks.12.self_attention.heads.8.tril\", \"blocks.12.self_attention.heads.8.key.weight\", \"blocks.12.self_attention.heads.8.query.weight\", \"blocks.12.self_attention.heads.8.value.weight\", \"blocks.12.self_attention.heads.9.tril\", \"blocks.12.self_attention.heads.9.key.weight\", \"blocks.12.self_attention.heads.9.query.weight\", \"blocks.12.self_attention.heads.9.value.weight\", \"blocks.12.self_attention.heads.10.tril\", \"blocks.12.self_attention.heads.10.key.weight\", \"blocks.12.self_attention.heads.10.query.weight\", \"blocks.12.self_attention.heads.10.value.weight\", \"blocks.12.self_attention.heads.11.tril\", \"blocks.12.self_attention.heads.11.key.weight\", \"blocks.12.self_attention.heads.11.query.weight\", \"blocks.12.self_attention.heads.11.value.weight\", \"blocks.12.self_attention.heads.12.tril\", \"blocks.12.self_attention.heads.12.key.weight\", \"blocks.12.self_attention.heads.12.query.weight\", \"blocks.12.self_attention.heads.12.value.weight\", \"blocks.12.self_attention.heads.13.tril\", \"blocks.12.self_attention.heads.13.key.weight\", \"blocks.12.self_attention.heads.13.query.weight\", \"blocks.12.self_attention.heads.13.value.weight\", \"blocks.12.self_attention.heads.14.tril\", \"blocks.12.self_attention.heads.14.key.weight\", \"blocks.12.self_attention.heads.14.query.weight\", \"blocks.12.self_attention.heads.14.value.weight\", \"blocks.12.self_attention.heads.15.tril\", \"blocks.12.self_attention.heads.15.key.weight\", \"blocks.12.self_attention.heads.15.query.weight\", \"blocks.12.self_attention.heads.15.value.weight\", \"blocks.12.self_attention.projection.weight\", \"blocks.12.self_attention.projection.bias\", \"blocks.12.feed_forward.net.0.weight\", \"blocks.12.feed_forward.net.0.bias\", \"blocks.12.feed_forward.net.2.weight\", \"blocks.12.feed_forward.net.2.bias\", \"blocks.12.layer_norm_1.weight\", \"blocks.12.layer_norm_1.bias\", \"blocks.12.layer_norm_2.weight\", \"blocks.12.layer_norm_2.bias\", \"blocks.13.self_attention.heads.0.tril\", \"blocks.13.self_attention.heads.0.key.weight\", \"blocks.13.self_attention.heads.0.query.weight\", \"blocks.13.self_attention.heads.0.value.weight\", \"blocks.13.self_attention.heads.1.tril\", \"blocks.13.self_attention.heads.1.key.weight\", \"blocks.13.self_attention.heads.1.query.weight\", \"blocks.13.self_attention.heads.1.value.weight\", \"blocks.13.self_attention.heads.2.tril\", \"blocks.13.self_attention.heads.2.key.weight\", \"blocks.13.self_attention.heads.2.query.weight\", \"blocks.13.self_attention.heads.2.value.weight\", \"blocks.13.self_attention.heads.3.tril\", \"blocks.13.self_attention.heads.3.key.weight\", \"blocks.13.self_attention.heads.3.query.weight\", \"blocks.13.self_attention.heads.3.value.weight\", \"blocks.13.self_attention.heads.4.tril\", \"blocks.13.self_attention.heads.4.key.weight\", \"blocks.13.self_attention.heads.4.query.weight\", \"blocks.13.self_attention.heads.4.value.weight\", \"blocks.13.self_attention.heads.5.tril\", \"blocks.13.self_attention.heads.5.key.weight\", \"blocks.13.self_attention.heads.5.query.weight\", \"blocks.13.self_attention.heads.5.value.weight\", \"blocks.13.self_attention.heads.6.tril\", \"blocks.13.self_attention.heads.6.key.weight\", \"blocks.13.self_attention.heads.6.query.weight\", \"blocks.13.self_attention.heads.6.value.weight\", \"blocks.13.self_attention.heads.7.tril\", \"blocks.13.self_attention.heads.7.key.weight\", \"blocks.13.self_attention.heads.7.query.weight\", \"blocks.13.self_attention.heads.7.value.weight\", \"blocks.13.self_attention.heads.8.tril\", \"blocks.13.self_attention.heads.8.key.weight\", \"blocks.13.self_attention.heads.8.query.weight\", \"blocks.13.self_attention.heads.8.value.weight\", \"blocks.13.self_attention.heads.9.tril\", \"blocks.13.self_attention.heads.9.key.weight\", \"blocks.13.self_attention.heads.9.query.weight\", \"blocks.13.self_attention.heads.9.value.weight\", \"blocks.13.self_attention.heads.10.tril\", \"blocks.13.self_attention.heads.10.key.weight\", \"blocks.13.self_attention.heads.10.query.weight\", \"blocks.13.self_attention.heads.10.value.weight\", \"blocks.13.self_attention.heads.11.tril\", \"blocks.13.self_attention.heads.11.key.weight\", \"blocks.13.self_attention.heads.11.query.weight\", \"blocks.13.self_attention.heads.11.value.weight\", \"blocks.13.self_attention.heads.12.tril\", \"blocks.13.self_attention.heads.12.key.weight\", \"blocks.13.self_attention.heads.12.query.weight\", \"blocks.13.self_attention.heads.12.value.weight\", \"blocks.13.self_attention.heads.13.tril\", \"blocks.13.self_attention.heads.13.key.weight\", \"blocks.13.self_attention.heads.13.query.weight\", \"blocks.13.self_attention.heads.13.value.weight\", \"blocks.13.self_attention.heads.14.tril\", \"blocks.13.self_attention.heads.14.key.weight\", \"blocks.13.self_attention.heads.14.query.weight\", \"blocks.13.self_attention.heads.14.value.weight\", \"blocks.13.self_attention.heads.15.tril\", \"blocks.13.self_attention.heads.15.key.weight\", \"blocks.13.self_attention.heads.15.query.weight\", \"blocks.13.self_attention.heads.15.value.weight\", \"blocks.13.self_attention.projection.weight\", \"blocks.13.self_attention.projection.bias\", \"blocks.13.feed_forward.net.0.weight\", \"blocks.13.feed_forward.net.0.bias\", \"blocks.13.feed_forward.net.2.weight\", \"blocks.13.feed_forward.net.2.bias\", \"blocks.13.layer_norm_1.weight\", \"blocks.13.layer_norm_1.bias\", \"blocks.13.layer_norm_2.weight\", \"blocks.13.layer_norm_2.bias\", \"blocks.14.self_attention.heads.0.tril\", \"blocks.14.self_attention.heads.0.key.weight\", \"blocks.14.self_attention.heads.0.query.weight\", \"blocks.14.self_attention.heads.0.value.weight\", \"blocks.14.self_attention.heads.1.tril\", \"blocks.14.self_attention.heads.1.key.weight\", \"blocks.14.self_attention.heads.1.query.weight\", \"blocks.14.self_attention.heads.1.value.weight\", \"blocks.14.self_attention.heads.2.tril\", \"blocks.14.self_attention.heads.2.key.weight\", \"blocks.14.self_attention.heads.2.query.weight\", \"blocks.14.self_attention.heads.2.value.weight\", \"blocks.14.self_attention.heads.3.tril\", \"blocks.14.self_attention.heads.3.key.weight\", \"blocks.14.self_attention.heads.3.query.weight\", \"blocks.14.self_attention.heads.3.value.weight\", \"blocks.14.self_attention.heads.4.tril\", \"blocks.14.self_attention.heads.4.key.weight\", \"blocks.14.self_attention.heads.4.query.weight\", \"blocks.14.self_attention.heads.4.value.weight\", \"blocks.14.self_attention.heads.5.tril\", \"blocks.14.self_attention.heads.5.key.weight\", \"blocks.14.self_attention.heads.5.query.weight\", \"blocks.14.self_attention.heads.5.value.weight\", \"blocks.14.self_attention.heads.6.tril\", \"blocks.14.self_attention.heads.6.key.weight\", \"blocks.14.self_attention.heads.6.query.weight\", \"blocks.14.self_attention.heads.6.value.weight\", \"blocks.14.self_attention.heads.7.tril\", \"blocks.14.self_attention.heads.7.key.weight\", \"blocks.14.self_attention.heads.7.query.weight\", \"blocks.14.self_attention.heads.7.value.weight\", \"blocks.14.self_attention.heads.8.tril\", \"blocks.14.self_attention.heads.8.key.weight\", \"blocks.14.self_attention.heads.8.query.weight\", \"blocks.14.self_attention.heads.8.value.weight\", \"blocks.14.self_attention.heads.9.tril\", \"blocks.14.self_attention.heads.9.key.weight\", \"blocks.14.self_attention.heads.9.query.weight\", \"blocks.14.self_attention.heads.9.value.weight\", \"blocks.14.self_attention.heads.10.tril\", \"blocks.14.self_attention.heads.10.key.weight\", \"blocks.14.self_attention.heads.10.query.weight\", \"blocks.14.self_attention.heads.10.value.weight\", \"blocks.14.self_attention.heads.11.tril\", \"blocks.14.self_attention.heads.11.key.weight\", \"blocks.14.self_attention.heads.11.query.weight\", \"blocks.14.self_attention.heads.11.value.weight\", \"blocks.14.self_attention.heads.12.tril\", \"blocks.14.self_attention.heads.12.key.weight\", \"blocks.14.self_attention.heads.12.query.weight\", \"blocks.14.self_attention.heads.12.value.weight\", \"blocks.14.self_attention.heads.13.tril\", \"blocks.14.self_attention.heads.13.key.weight\", \"blocks.14.self_attention.heads.13.query.weight\", \"blocks.14.self_attention.heads.13.value.weight\", \"blocks.14.self_attention.heads.14.tril\", \"blocks.14.self_attention.heads.14.key.weight\", \"blocks.14.self_attention.heads.14.query.weight\", \"blocks.14.self_attention.heads.14.value.weight\", \"blocks.14.self_attention.heads.15.tril\", \"blocks.14.self_attention.heads.15.key.weight\", \"blocks.14.self_attention.heads.15.query.weight\", \"blocks.14.self_attention.heads.15.value.weight\", \"blocks.14.self_attention.projection.weight\", \"blocks.14.self_attention.projection.bias\", \"blocks.14.feed_forward.net.0.weight\", \"blocks.14.feed_forward.net.0.bias\", \"blocks.14.feed_forward.net.2.weight\", \"blocks.14.feed_forward.net.2.bias\", \"blocks.14.layer_norm_1.weight\", \"blocks.14.layer_norm_1.bias\", \"blocks.14.layer_norm_2.weight\", \"blocks.14.layer_norm_2.bias\", \"blocks.15.self_attention.heads.0.tril\", \"blocks.15.self_attention.heads.0.key.weight\", \"blocks.15.self_attention.heads.0.query.weight\", \"blocks.15.self_attention.heads.0.value.weight\", \"blocks.15.self_attention.heads.1.tril\", \"blocks.15.self_attention.heads.1.key.weight\", \"blocks.15.self_attention.heads.1.query.weight\", \"blocks.15.self_attention.heads.1.value.weight\", \"blocks.15.self_attention.heads.2.tril\", \"blocks.15.self_attention.heads.2.key.weight\", \"blocks.15.self_attention.heads.2.query.weight\", \"blocks.15.self_attention.heads.2.value.weight\", \"blocks.15.self_attention.heads.3.tril\", \"blocks.15.self_attention.heads.3.key.weight\", \"blocks.15.self_attention.heads.3.query.weight\", \"blocks.15.self_attention.heads.3.value.weight\", \"blocks.15.self_attention.heads.4.tril\", \"blocks.15.self_attention.heads.4.key.weight\", \"blocks.15.self_attention.heads.4.query.weight\", \"blocks.15.self_attention.heads.4.value.weight\", \"blocks.15.self_attention.heads.5.tril\", \"blocks.15.self_attention.heads.5.key.weight\", \"blocks.15.self_attention.heads.5.query.weight\", \"blocks.15.self_attention.heads.5.value.weight\", \"blocks.15.self_attention.heads.6.tril\", \"blocks.15.self_attention.heads.6.key.weight\", \"blocks.15.self_attention.heads.6.query.weight\", \"blocks.15.self_attention.heads.6.value.weight\", \"blocks.15.self_attention.heads.7.tril\", \"blocks.15.self_attention.heads.7.key.weight\", \"blocks.15.self_attention.heads.7.query.weight\", \"blocks.15.self_attention.heads.7.value.weight\", \"blocks.15.self_attention.heads.8.tril\", \"blocks.15.self_attention.heads.8.key.weight\", \"blocks.15.self_attention.heads.8.query.weight\", \"blocks.15.self_attention.heads.8.value.weight\", \"blocks.15.self_attention.heads.9.tril\", \"blocks.15.self_attention.heads.9.key.weight\", \"blocks.15.self_attention.heads.9.query.weight\", \"blocks.15.self_attention.heads.9.value.weight\", \"blocks.15.self_attention.heads.10.tril\", \"blocks.15.self_attention.heads.10.key.weight\", \"blocks.15.self_attention.heads.10.query.weight\", \"blocks.15.self_attention.heads.10.value.weight\", \"blocks.15.self_attention.heads.11.tril\", \"blocks.15.self_attention.heads.11.key.weight\", \"blocks.15.self_attention.heads.11.query.weight\", \"blocks.15.self_attention.heads.11.value.weight\", \"blocks.15.self_attention.heads.12.tril\", \"blocks.15.self_attention.heads.12.key.weight\", \"blocks.15.self_attention.heads.12.query.weight\", \"blocks.15.self_attention.heads.12.value.weight\", \"blocks.15.self_attention.heads.13.tril\", \"blocks.15.self_attention.heads.13.key.weight\", \"blocks.15.self_attention.heads.13.query.weight\", \"blocks.15.self_attention.heads.13.value.weight\", \"blocks.15.self_attention.heads.14.tril\", \"blocks.15.self_attention.heads.14.key.weight\", \"blocks.15.self_attention.heads.14.query.weight\", \"blocks.15.self_attention.heads.14.value.weight\", \"blocks.15.self_attention.heads.15.tril\", \"blocks.15.self_attention.heads.15.key.weight\", \"blocks.15.self_attention.heads.15.query.weight\", \"blocks.15.self_attention.heads.15.value.weight\", \"blocks.15.self_attention.projection.weight\", \"blocks.15.self_attention.projection.bias\", \"blocks.15.feed_forward.net.0.weight\", \"blocks.15.feed_forward.net.0.bias\", \"blocks.15.feed_forward.net.2.weight\", \"blocks.15.feed_forward.net.2.bias\", \"blocks.15.layer_norm_1.weight\", \"blocks.15.layer_norm_1.bias\", \"blocks.15.layer_norm_2.weight\", \"blocks.15.layer_norm_2.bias\", \"blocks.16.self_attention.heads.0.tril\", \"blocks.16.self_attention.heads.0.key.weight\", \"blocks.16.self_attention.heads.0.query.weight\", \"blocks.16.self_attention.heads.0.value.weight\", \"blocks.16.self_attention.heads.1.tril\", \"blocks.16.self_attention.heads.1.key.weight\", \"blocks.16.self_attention.heads.1.query.weight\", \"blocks.16.self_attention.heads.1.value.weight\", \"blocks.16.self_attention.heads.2.tril\", \"blocks.16.self_attention.heads.2.key.weight\", \"blocks.16.self_attention.heads.2.query.weight\", \"blocks.16.self_attention.heads.2.value.weight\", \"blocks.16.self_attention.heads.3.tril\", \"blocks.16.self_attention.heads.3.key.weight\", \"blocks.16.self_attention.heads.3.query.weight\", \"blocks.16.self_attention.heads.3.value.weight\", \"blocks.16.self_attention.heads.4.tril\", \"blocks.16.self_attention.heads.4.key.weight\", \"blocks.16.self_attention.heads.4.query.weight\", \"blocks.16.self_attention.heads.4.value.weight\", \"blocks.16.self_attention.heads.5.tril\", \"blocks.16.self_attention.heads.5.key.weight\", \"blocks.16.self_attention.heads.5.query.weight\", \"blocks.16.self_attention.heads.5.value.weight\", \"blocks.16.self_attention.heads.6.tril\", \"blocks.16.self_attention.heads.6.key.weight\", \"blocks.16.self_attention.heads.6.query.weight\", \"blocks.16.self_attention.heads.6.value.weight\", \"blocks.16.self_attention.heads.7.tril\", \"blocks.16.self_attention.heads.7.key.weight\", \"blocks.16.self_attention.heads.7.query.weight\", \"blocks.16.self_attention.heads.7.value.weight\", \"blocks.16.self_attention.heads.8.tril\", \"blocks.16.self_attention.heads.8.key.weight\", \"blocks.16.self_attention.heads.8.query.weight\", \"blocks.16.self_attention.heads.8.value.weight\", \"blocks.16.self_attention.heads.9.tril\", \"blocks.16.self_attention.heads.9.key.weight\", \"blocks.16.self_attention.heads.9.query.weight\", \"blocks.16.self_attention.heads.9.value.weight\", \"blocks.16.self_attention.heads.10.tril\", \"blocks.16.self_attention.heads.10.key.weight\", \"blocks.16.self_attention.heads.10.query.weight\", \"blocks.16.self_attention.heads.10.value.weight\", \"blocks.16.self_attention.heads.11.tril\", \"blocks.16.self_attention.heads.11.key.weight\", \"blocks.16.self_attention.heads.11.query.weight\", \"blocks.16.self_attention.heads.11.value.weight\", \"blocks.16.self_attention.heads.12.tril\", \"blocks.16.self_attention.heads.12.key.weight\", \"blocks.16.self_attention.heads.12.query.weight\", \"blocks.16.self_attention.heads.12.value.weight\", \"blocks.16.self_attention.heads.13.tril\", \"blocks.16.self_attention.heads.13.key.weight\", \"blocks.16.self_attention.heads.13.query.weight\", \"blocks.16.self_attention.heads.13.value.weight\", \"blocks.16.self_attention.heads.14.tril\", \"blocks.16.self_attention.heads.14.key.weight\", \"blocks.16.self_attention.heads.14.query.weight\", \"blocks.16.self_attention.heads.14.value.weight\", \"blocks.16.self_attention.heads.15.tril\", \"blocks.16.self_attention.heads.15.key.weight\", \"blocks.16.self_attention.heads.15.query.weight\", \"blocks.16.self_attention.heads.15.value.weight\", \"blocks.16.self_attention.projection.weight\", \"blocks.16.self_attention.projection.bias\", \"blocks.16.feed_forward.net.0.weight\", \"blocks.16.feed_forward.net.0.bias\", \"blocks.16.feed_forward.net.2.weight\", \"blocks.16.feed_forward.net.2.bias\", \"blocks.16.layer_norm_1.weight\", \"blocks.16.layer_norm_1.bias\", \"blocks.16.layer_norm_2.weight\", \"blocks.16.layer_norm_2.bias\", \"blocks.17.self_attention.heads.0.tril\", \"blocks.17.self_attention.heads.0.key.weight\", \"blocks.17.self_attention.heads.0.query.weight\", \"blocks.17.self_attention.heads.0.value.weight\", \"blocks.17.self_attention.heads.1.tril\", \"blocks.17.self_attention.heads.1.key.weight\", \"blocks.17.self_attention.heads.1.query.weight\", \"blocks.17.self_attention.heads.1.value.weight\", \"blocks.17.self_attention.heads.2.tril\", \"blocks.17.self_attention.heads.2.key.weight\", \"blocks.17.self_attention.heads.2.query.weight\", \"blocks.17.self_attention.heads.2.value.weight\", \"blocks.17.self_attention.heads.3.tril\", \"blocks.17.self_attention.heads.3.key.weight\", \"blocks.17.self_attention.heads.3.query.weight\", \"blocks.17.self_attention.heads.3.value.weight\", \"blocks.17.self_attention.heads.4.tril\", \"blocks.17.self_attention.heads.4.key.weight\", \"blocks.17.self_attention.heads.4.query.weight\", \"blocks.17.self_attention.heads.4.value.weight\", \"blocks.17.self_attention.heads.5.tril\", \"blocks.17.self_attention.heads.5.key.weight\", \"blocks.17.self_attention.heads.5.query.weight\", \"blocks.17.self_attention.heads.5.value.weight\", \"blocks.17.self_attention.heads.6.tril\", \"blocks.17.self_attention.heads.6.key.weight\", \"blocks.17.self_attention.heads.6.query.weight\", \"blocks.17.self_attention.heads.6.value.weight\", \"blocks.17.self_attention.heads.7.tril\", \"blocks.17.self_attention.heads.7.key.weight\", \"blocks.17.self_attention.heads.7.query.weight\", \"blocks.17.self_attention.heads.7.value.weight\", \"blocks.17.self_attention.heads.8.tril\", \"blocks.17.self_attention.heads.8.key.weight\", \"blocks.17.self_attention.heads.8.query.weight\", \"blocks.17.self_attention.heads.8.value.weight\", \"blocks.17.self_attention.heads.9.tril\", \"blocks.17.self_attention.heads.9.key.weight\", \"blocks.17.self_attention.heads.9.query.weight\", \"blocks.17.self_attention.heads.9.value.weight\", \"blocks.17.self_attention.heads.10.tril\", \"blocks.17.self_attention.heads.10.key.weight\", \"blocks.17.self_attention.heads.10.query.weight\", \"blocks.17.self_attention.heads.10.value.weight\", \"blocks.17.self_attention.heads.11.tril\", \"blocks.17.self_attention.heads.11.key.weight\", \"blocks.17.self_attention.heads.11.query.weight\", \"blocks.17.self_attention.heads.11.value.weight\", \"blocks.17.self_attention.heads.12.tril\", \"blocks.17.self_attention.heads.12.key.weight\", \"blocks.17.self_attention.heads.12.query.weight\", \"blocks.17.self_attention.heads.12.value.weight\", \"blocks.17.self_attention.heads.13.tril\", \"blocks.17.self_attention.heads.13.key.weight\", \"blocks.17.self_attention.heads.13.query.weight\", \"blocks.17.self_attention.heads.13.value.weight\", \"blocks.17.self_attention.heads.14.tril\", \"blocks.17.self_attention.heads.14.key.weight\", \"blocks.17.self_attention.heads.14.query.weight\", \"blocks.17.self_attention.heads.14.value.weight\", \"blocks.17.self_attention.heads.15.tril\", \"blocks.17.self_attention.heads.15.key.weight\", \"blocks.17.self_attention.heads.15.query.weight\", \"blocks.17.self_attention.heads.15.value.weight\", \"blocks.17.self_attention.projection.weight\", \"blocks.17.self_attention.projection.bias\", \"blocks.17.feed_forward.net.0.weight\", \"blocks.17.feed_forward.net.0.bias\", \"blocks.17.feed_forward.net.2.weight\", \"blocks.17.feed_forward.net.2.bias\", \"blocks.17.layer_norm_1.weight\", \"blocks.17.layer_norm_1.bias\", \"blocks.17.layer_norm_2.weight\", \"blocks.17.layer_norm_2.bias\", \"blocks.18.self_attention.heads.0.tril\", \"blocks.18.self_attention.heads.0.key.weight\", \"blocks.18.self_attention.heads.0.query.weight\", \"blocks.18.self_attention.heads.0.value.weight\", \"blocks.18.self_attention.heads.1.tril\", \"blocks.18.self_attention.heads.1.key.weight\", \"blocks.18.self_attention.heads.1.query.weight\", \"blocks.18.self_attention.heads.1.value.weight\", \"blocks.18.self_attention.heads.2.tril\", \"blocks.18.self_attention.heads.2.key.weight\", \"blocks.18.self_attention.heads.2.query.weight\", \"blocks.18.self_attention.heads.2.value.weight\", \"blocks.18.self_attention.heads.3.tril\", \"blocks.18.self_attention.heads.3.key.weight\", \"blocks.18.self_attention.heads.3.query.weight\", \"blocks.18.self_attention.heads.3.value.weight\", \"blocks.18.self_attention.heads.4.tril\", \"blocks.18.self_attention.heads.4.key.weight\", \"blocks.18.self_attention.heads.4.query.weight\", \"blocks.18.self_attention.heads.4.value.weight\", \"blocks.18.self_attention.heads.5.tril\", \"blocks.18.self_attention.heads.5.key.weight\", \"blocks.18.self_attention.heads.5.query.weight\", \"blocks.18.self_attention.heads.5.value.weight\", \"blocks.18.self_attention.heads.6.tril\", \"blocks.18.self_attention.heads.6.key.weight\", \"blocks.18.self_attention.heads.6.query.weight\", \"blocks.18.self_attention.heads.6.value.weight\", \"blocks.18.self_attention.heads.7.tril\", \"blocks.18.self_attention.heads.7.key.weight\", \"blocks.18.self_attention.heads.7.query.weight\", \"blocks.18.self_attention.heads.7.value.weight\", \"blocks.18.self_attention.heads.8.tril\", \"blocks.18.self_attention.heads.8.key.weight\", \"blocks.18.self_attention.heads.8.query.weight\", \"blocks.18.self_attention.heads.8.value.weight\", \"blocks.18.self_attention.heads.9.tril\", \"blocks.18.self_attention.heads.9.key.weight\", \"blocks.18.self_attention.heads.9.query.weight\", \"blocks.18.self_attention.heads.9.value.weight\", \"blocks.18.self_attention.heads.10.tril\", \"blocks.18.self_attention.heads.10.key.weight\", \"blocks.18.self_attention.heads.10.query.weight\", \"blocks.18.self_attention.heads.10.value.weight\", \"blocks.18.self_attention.heads.11.tril\", \"blocks.18.self_attention.heads.11.key.weight\", \"blocks.18.self_attention.heads.11.query.weight\", \"blocks.18.self_attention.heads.11.value.weight\", \"blocks.18.self_attention.heads.12.tril\", \"blocks.18.self_attention.heads.12.key.weight\", \"blocks.18.self_attention.heads.12.query.weight\", \"blocks.18.self_attention.heads.12.value.weight\", \"blocks.18.self_attention.heads.13.tril\", \"blocks.18.self_attention.heads.13.key.weight\", \"blocks.18.self_attention.heads.13.query.weight\", \"blocks.18.self_attention.heads.13.value.weight\", \"blocks.18.self_attention.heads.14.tril\", \"blocks.18.self_attention.heads.14.key.weight\", \"blocks.18.self_attention.heads.14.query.weight\", \"blocks.18.self_attention.heads.14.value.weight\", \"blocks.18.self_attention.heads.15.tril\", \"blocks.18.self_attention.heads.15.key.weight\", \"blocks.18.self_attention.heads.15.query.weight\", \"blocks.18.self_attention.heads.15.value.weight\", \"blocks.18.self_attention.projection.weight\", \"blocks.18.self_attention.projection.bias\", \"blocks.18.feed_forward.net.0.weight\", \"blocks.18.feed_forward.net.0.bias\", \"blocks.18.feed_forward.net.2.weight\", \"blocks.18.feed_forward.net.2.bias\", \"blocks.18.layer_norm_1.weight\", \"blocks.18.layer_norm_1.bias\", \"blocks.18.layer_norm_2.weight\", \"blocks.18.layer_norm_2.bias\", \"blocks.19.self_attention.heads.0.tril\", \"blocks.19.self_attention.heads.0.key.weight\", \"blocks.19.self_attention.heads.0.query.weight\", \"blocks.19.self_attention.heads.0.value.weight\", \"blocks.19.self_attention.heads.1.tril\", \"blocks.19.self_attention.heads.1.key.weight\", \"blocks.19.self_attention.heads.1.query.weight\", \"blocks.19.self_attention.heads.1.value.weight\", \"blocks.19.self_attention.heads.2.tril\", \"blocks.19.self_attention.heads.2.key.weight\", \"blocks.19.self_attention.heads.2.query.weight\", \"blocks.19.self_attention.heads.2.value.weight\", \"blocks.19.self_attention.heads.3.tril\", \"blocks.19.self_attention.heads.3.key.weight\", \"blocks.19.self_attention.heads.3.query.weight\", \"blocks.19.self_attention.heads.3.value.weight\", \"blocks.19.self_attention.heads.4.tril\", \"blocks.19.self_attention.heads.4.key.weight\", \"blocks.19.self_attention.heads.4.query.weight\", \"blocks.19.self_attention.heads.4.value.weight\", \"blocks.19.self_attention.heads.5.tril\", \"blocks.19.self_attention.heads.5.key.weight\", \"blocks.19.self_attention.heads.5.query.weight\", \"blocks.19.self_attention.heads.5.value.weight\", \"blocks.19.self_attention.heads.6.tril\", \"blocks.19.self_attention.heads.6.key.weight\", \"blocks.19.self_attention.heads.6.query.weight\", \"blocks.19.self_attention.heads.6.value.weight\", \"blocks.19.self_attention.heads.7.tril\", \"blocks.19.self_attention.heads.7.key.weight\", \"blocks.19.self_attention.heads.7.query.weight\", \"blocks.19.self_attention.heads.7.value.weight\", \"blocks.19.self_attention.heads.8.tril\", \"blocks.19.self_attention.heads.8.key.weight\", \"blocks.19.self_attention.heads.8.query.weight\", \"blocks.19.self_attention.heads.8.value.weight\", \"blocks.19.self_attention.heads.9.tril\", \"blocks.19.self_attention.heads.9.key.weight\", \"blocks.19.self_attention.heads.9.query.weight\", \"blocks.19.self_attention.heads.9.value.weight\", \"blocks.19.self_attention.heads.10.tril\", \"blocks.19.self_attention.heads.10.key.weight\", \"blocks.19.self_attention.heads.10.query.weight\", \"blocks.19.self_attention.heads.10.value.weight\", \"blocks.19.self_attention.heads.11.tril\", \"blocks.19.self_attention.heads.11.key.weight\", \"blocks.19.self_attention.heads.11.query.weight\", \"blocks.19.self_attention.heads.11.value.weight\", \"blocks.19.self_attention.heads.12.tril\", \"blocks.19.self_attention.heads.12.key.weight\", \"blocks.19.self_attention.heads.12.query.weight\", \"blocks.19.self_attention.heads.12.value.weight\", \"blocks.19.self_attention.heads.13.tril\", \"blocks.19.self_attention.heads.13.key.weight\", \"blocks.19.self_attention.heads.13.query.weight\", \"blocks.19.self_attention.heads.13.value.weight\", \"blocks.19.self_attention.heads.14.tril\", \"blocks.19.self_attention.heads.14.key.weight\", \"blocks.19.self_attention.heads.14.query.weight\", \"blocks.19.self_attention.heads.14.value.weight\", \"blocks.19.self_attention.heads.15.tril\", \"blocks.19.self_attention.heads.15.key.weight\", \"blocks.19.self_attention.heads.15.query.weight\", \"blocks.19.self_attention.heads.15.value.weight\", \"blocks.19.self_attention.projection.weight\", \"blocks.19.self_attention.projection.bias\", \"blocks.19.feed_forward.net.0.weight\", \"blocks.19.feed_forward.net.0.bias\", \"blocks.19.feed_forward.net.2.weight\", \"blocks.19.feed_forward.net.2.bias\", \"blocks.19.layer_norm_1.weight\", \"blocks.19.layer_norm_1.bias\", \"blocks.19.layer_norm_2.weight\", \"blocks.19.layer_norm_2.bias\", \"blocks.20.self_attention.heads.0.tril\", \"blocks.20.self_attention.heads.0.key.weight\", \"blocks.20.self_attention.heads.0.query.weight\", \"blocks.20.self_attention.heads.0.value.weight\", \"blocks.20.self_attention.heads.1.tril\", \"blocks.20.self_attention.heads.1.key.weight\", \"blocks.20.self_attention.heads.1.query.weight\", \"blocks.20.self_attention.heads.1.value.weight\", \"blocks.20.self_attention.heads.2.tril\", \"blocks.20.self_attention.heads.2.key.weight\", \"blocks.20.self_attention.heads.2.query.weight\", \"blocks.20.self_attention.heads.2.value.weight\", \"blocks.20.self_attention.heads.3.tril\", \"blocks.20.self_attention.heads.3.key.weight\", \"blocks.20.self_attention.heads.3.query.weight\", \"blocks.20.self_attention.heads.3.value.weight\", \"blocks.20.self_attention.heads.4.tril\", \"blocks.20.self_attention.heads.4.key.weight\", \"blocks.20.self_attention.heads.4.query.weight\", \"blocks.20.self_attention.heads.4.value.weight\", \"blocks.20.self_attention.heads.5.tril\", \"blocks.20.self_attention.heads.5.key.weight\", \"blocks.20.self_attention.heads.5.query.weight\", \"blocks.20.self_attention.heads.5.value.weight\", \"blocks.20.self_attention.heads.6.tril\", \"blocks.20.self_attention.heads.6.key.weight\", \"blocks.20.self_attention.heads.6.query.weight\", \"blocks.20.self_attention.heads.6.value.weight\", \"blocks.20.self_attention.heads.7.tril\", \"blocks.20.self_attention.heads.7.key.weight\", \"blocks.20.self_attention.heads.7.query.weight\", \"blocks.20.self_attention.heads.7.value.weight\", \"blocks.20.self_attention.heads.8.tril\", \"blocks.20.self_attention.heads.8.key.weight\", \"blocks.20.self_attention.heads.8.query.weight\", \"blocks.20.self_attention.heads.8.value.weight\", \"blocks.20.self_attention.heads.9.tril\", \"blocks.20.self_attention.heads.9.key.weight\", \"blocks.20.self_attention.heads.9.query.weight\", \"blocks.20.self_attention.heads.9.value.weight\", \"blocks.20.self_attention.heads.10.tril\", \"blocks.20.self_attention.heads.10.key.weight\", \"blocks.20.self_attention.heads.10.query.weight\", \"blocks.20.self_attention.heads.10.value.weight\", \"blocks.20.self_attention.heads.11.tril\", \"blocks.20.self_attention.heads.11.key.weight\", \"blocks.20.self_attention.heads.11.query.weight\", \"blocks.20.self_attention.heads.11.value.weight\", \"blocks.20.self_attention.heads.12.tril\", \"blocks.20.self_attention.heads.12.key.weight\", \"blocks.20.self_attention.heads.12.query.weight\", \"blocks.20.self_attention.heads.12.value.weight\", \"blocks.20.self_attention.heads.13.tril\", \"blocks.20.self_attention.heads.13.key.weight\", \"blocks.20.self_attention.heads.13.query.weight\", \"blocks.20.self_attention.heads.13.value.weight\", \"blocks.20.self_attention.heads.14.tril\", \"blocks.20.self_attention.heads.14.key.weight\", \"blocks.20.self_attention.heads.14.query.weight\", \"blocks.20.self_attention.heads.14.value.weight\", \"blocks.20.self_attention.heads.15.tril\", \"blocks.20.self_attention.heads.15.key.weight\", \"blocks.20.self_attention.heads.15.query.weight\", \"blocks.20.self_attention.heads.15.value.weight\", \"blocks.20.self_attention.projection.weight\", \"blocks.20.self_attention.projection.bias\", \"blocks.20.feed_forward.net.0.weight\", \"blocks.20.feed_forward.net.0.bias\", \"blocks.20.feed_forward.net.2.weight\", \"blocks.20.feed_forward.net.2.bias\", \"blocks.20.layer_norm_1.weight\", \"blocks.20.layer_norm_1.bias\", \"blocks.20.layer_norm_2.weight\", \"blocks.20.layer_norm_2.bias\", \"blocks.21.self_attention.heads.0.tril\", \"blocks.21.self_attention.heads.0.key.weight\", \"blocks.21.self_attention.heads.0.query.weight\", \"blocks.21.self_attention.heads.0.value.weight\", \"blocks.21.self_attention.heads.1.tril\", \"blocks.21.self_attention.heads.1.key.weight\", \"blocks.21.self_attention.heads.1.query.weight\", \"blocks.21.self_attention.heads.1.value.weight\", \"blocks.21.self_attention.heads.2.tril\", \"blocks.21.self_attention.heads.2.key.weight\", \"blocks.21.self_attention.heads.2.query.weight\", \"blocks.21.self_attention.heads.2.value.weight\", \"blocks.21.self_attention.heads.3.tril\", \"blocks.21.self_attention.heads.3.key.weight\", \"blocks.21.self_attention.heads.3.query.weight\", \"blocks.21.self_attention.heads.3.value.weight\", \"blocks.21.self_attention.heads.4.tril\", \"blocks.21.self_attention.heads.4.key.weight\", \"blocks.21.self_attention.heads.4.query.weight\", \"blocks.21.self_attention.heads.4.value.weight\", \"blocks.21.self_attention.heads.5.tril\", \"blocks.21.self_attention.heads.5.key.weight\", \"blocks.21.self_attention.heads.5.query.weight\", \"blocks.21.self_attention.heads.5.value.weight\", \"blocks.21.self_attention.heads.6.tril\", \"blocks.21.self_attention.heads.6.key.weight\", \"blocks.21.self_attention.heads.6.query.weight\", \"blocks.21.self_attention.heads.6.value.weight\", \"blocks.21.self_attention.heads.7.tril\", \"blocks.21.self_attention.heads.7.key.weight\", \"blocks.21.self_attention.heads.7.query.weight\", \"blocks.21.self_attention.heads.7.value.weight\", \"blocks.21.self_attention.heads.8.tril\", \"blocks.21.self_attention.heads.8.key.weight\", \"blocks.21.self_attention.heads.8.query.weight\", \"blocks.21.self_attention.heads.8.value.weight\", \"blocks.21.self_attention.heads.9.tril\", \"blocks.21.self_attention.heads.9.key.weight\", \"blocks.21.self_attention.heads.9.query.weight\", \"blocks.21.self_attention.heads.9.value.weight\", \"blocks.21.self_attention.heads.10.tril\", \"blocks.21.self_attention.heads.10.key.weight\", \"blocks.21.self_attention.heads.10.query.weight\", \"blocks.21.self_attention.heads.10.value.weight\", \"blocks.21.self_attention.heads.11.tril\", \"blocks.21.self_attention.heads.11.key.weight\", \"blocks.21.self_attention.heads.11.query.weight\", \"blocks.21.self_attention.heads.11.value.weight\", \"blocks.21.self_attention.heads.12.tril\", \"blocks.21.self_attention.heads.12.key.weight\", \"blocks.21.self_attention.heads.12.query.weight\", \"blocks.21.self_attention.heads.12.value.weight\", \"blocks.21.self_attention.heads.13.tril\", \"blocks.21.self_attention.heads.13.key.weight\", \"blocks.21.self_attention.heads.13.query.weight\", \"blocks.21.self_attention.heads.13.value.weight\", \"blocks.21.self_attention.heads.14.tril\", \"blocks.21.self_attention.heads.14.key.weight\", \"blocks.21.self_attention.heads.14.query.weight\", \"blocks.21.self_attention.heads.14.value.weight\", \"blocks.21.self_attention.heads.15.tril\", \"blocks.21.self_attention.heads.15.key.weight\", \"blocks.21.self_attention.heads.15.query.weight\", \"blocks.21.self_attention.heads.15.value.weight\", \"blocks.21.self_attention.projection.weight\", \"blocks.21.self_attention.projection.bias\", \"blocks.21.feed_forward.net.0.weight\", \"blocks.21.feed_forward.net.0.bias\", \"blocks.21.feed_forward.net.2.weight\", \"blocks.21.feed_forward.net.2.bias\", \"blocks.21.layer_norm_1.weight\", \"blocks.21.layer_norm_1.bias\", \"blocks.21.layer_norm_2.weight\", \"blocks.21.layer_norm_2.bias\", \"blocks.22.self_attention.heads.0.tril\", \"blocks.22.self_attention.heads.0.key.weight\", \"blocks.22.self_attention.heads.0.query.weight\", \"blocks.22.self_attention.heads.0.value.weight\", \"blocks.22.self_attention.heads.1.tril\", \"blocks.22.self_attention.heads.1.key.weight\", \"blocks.22.self_attention.heads.1.query.weight\", \"blocks.22.self_attention.heads.1.value.weight\", \"blocks.22.self_attention.heads.2.tril\", \"blocks.22.self_attention.heads.2.key.weight\", \"blocks.22.self_attention.heads.2.query.weight\", \"blocks.22.self_attention.heads.2.value.weight\", \"blocks.22.self_attention.heads.3.tril\", \"blocks.22.self_attention.heads.3.key.weight\", \"blocks.22.self_attention.heads.3.query.weight\", \"blocks.22.self_attention.heads.3.value.weight\", \"blocks.22.self_attention.heads.4.tril\", \"blocks.22.self_attention.heads.4.key.weight\", \"blocks.22.self_attention.heads.4.query.weight\", \"blocks.22.self_attention.heads.4.value.weight\", \"blocks.22.self_attention.heads.5.tril\", \"blocks.22.self_attention.heads.5.key.weight\", \"blocks.22.self_attention.heads.5.query.weight\", \"blocks.22.self_attention.heads.5.value.weight\", \"blocks.22.self_attention.heads.6.tril\", \"blocks.22.self_attention.heads.6.key.weight\", \"blocks.22.self_attention.heads.6.query.weight\", \"blocks.22.self_attention.heads.6.value.weight\", \"blocks.22.self_attention.heads.7.tril\", \"blocks.22.self_attention.heads.7.key.weight\", \"blocks.22.self_attention.heads.7.query.weight\", \"blocks.22.self_attention.heads.7.value.weight\", \"blocks.22.self_attention.heads.8.tril\", \"blocks.22.self_attention.heads.8.key.weight\", \"blocks.22.self_attention.heads.8.query.weight\", \"blocks.22.self_attention.heads.8.value.weight\", \"blocks.22.self_attention.heads.9.tril\", \"blocks.22.self_attention.heads.9.key.weight\", \"blocks.22.self_attention.heads.9.query.weight\", \"blocks.22.self_attention.heads.9.value.weight\", \"blocks.22.self_attention.heads.10.tril\", \"blocks.22.self_attention.heads.10.key.weight\", \"blocks.22.self_attention.heads.10.query.weight\", \"blocks.22.self_attention.heads.10.value.weight\", \"blocks.22.self_attention.heads.11.tril\", \"blocks.22.self_attention.heads.11.key.weight\", \"blocks.22.self_attention.heads.11.query.weight\", \"blocks.22.self_attention.heads.11.value.weight\", \"blocks.22.self_attention.heads.12.tril\", \"blocks.22.self_attention.heads.12.key.weight\", \"blocks.22.self_attention.heads.12.query.weight\", \"blocks.22.self_attention.heads.12.value.weight\", \"blocks.22.self_attention.heads.13.tril\", \"blocks.22.self_attention.heads.13.key.weight\", \"blocks.22.self_attention.heads.13.query.weight\", \"blocks.22.self_attention.heads.13.value.weight\", \"blocks.22.self_attention.heads.14.tril\", \"blocks.22.self_attention.heads.14.key.weight\", \"blocks.22.self_attention.heads.14.query.weight\", \"blocks.22.self_attention.heads.14.value.weight\", \"blocks.22.self_attention.heads.15.tril\", \"blocks.22.self_attention.heads.15.key.weight\", \"blocks.22.self_attention.heads.15.query.weight\", \"blocks.22.self_attention.heads.15.value.weight\", \"blocks.22.self_attention.projection.weight\", \"blocks.22.self_attention.projection.bias\", \"blocks.22.feed_forward.net.0.weight\", \"blocks.22.feed_forward.net.0.bias\", \"blocks.22.feed_forward.net.2.weight\", \"blocks.22.feed_forward.net.2.bias\", \"blocks.22.layer_norm_1.weight\", \"blocks.22.layer_norm_1.bias\", \"blocks.22.layer_norm_2.weight\", \"blocks.22.layer_norm_2.bias\", \"blocks.23.self_attention.heads.0.tril\", \"blocks.23.self_attention.heads.0.key.weight\", \"blocks.23.self_attention.heads.0.query.weight\", \"blocks.23.self_attention.heads.0.value.weight\", \"blocks.23.self_attention.heads.1.tril\", \"blocks.23.self_attention.heads.1.key.weight\", \"blocks.23.self_attention.heads.1.query.weight\", \"blocks.23.self_attention.heads.1.value.weight\", \"blocks.23.self_attention.heads.2.tril\", \"blocks.23.self_attention.heads.2.key.weight\", \"blocks.23.self_attention.heads.2.query.weight\", \"blocks.23.self_attention.heads.2.value.weight\", \"blocks.23.self_attention.heads.3.tril\", \"blocks.23.self_attention.heads.3.key.weight\", \"blocks.23.self_attention.heads.3.query.weight\", \"blocks.23.self_attention.heads.3.value.weight\", \"blocks.23.self_attention.heads.4.tril\", \"blocks.23.self_attention.heads.4.key.weight\", \"blocks.23.self_attention.heads.4.query.weight\", \"blocks.23.self_attention.heads.4.value.weight\", \"blocks.23.self_attention.heads.5.tril\", \"blocks.23.self_attention.heads.5.key.weight\", \"blocks.23.self_attention.heads.5.query.weight\", \"blocks.23.self_attention.heads.5.value.weight\", \"blocks.23.self_attention.heads.6.tril\", \"blocks.23.self_attention.heads.6.key.weight\", \"blocks.23.self_attention.heads.6.query.weight\", \"blocks.23.self_attention.heads.6.value.weight\", \"blocks.23.self_attention.heads.7.tril\", \"blocks.23.self_attention.heads.7.key.weight\", \"blocks.23.self_attention.heads.7.query.weight\", \"blocks.23.self_attention.heads.7.value.weight\", \"blocks.23.self_attention.heads.8.tril\", \"blocks.23.self_attention.heads.8.key.weight\", \"blocks.23.self_attention.heads.8.query.weight\", \"blocks.23.self_attention.heads.8.value.weight\", \"blocks.23.self_attention.heads.9.tril\", \"blocks.23.self_attention.heads.9.key.weight\", \"blocks.23.self_attention.heads.9.query.weight\", \"blocks.23.self_attention.heads.9.value.weight\", \"blocks.23.self_attention.heads.10.tril\", \"blocks.23.self_attention.heads.10.key.weight\", \"blocks.23.self_attention.heads.10.query.weight\", \"blocks.23.self_attention.heads.10.value.weight\", \"blocks.23.self_attention.heads.11.tril\", \"blocks.23.self_attention.heads.11.key.weight\", \"blocks.23.self_attention.heads.11.query.weight\", \"blocks.23.self_attention.heads.11.value.weight\", \"blocks.23.self_attention.heads.12.tril\", \"blocks.23.self_attention.heads.12.key.weight\", \"blocks.23.self_attention.heads.12.query.weight\", \"blocks.23.self_attention.heads.12.value.weight\", \"blocks.23.self_attention.heads.13.tril\", \"blocks.23.self_attention.heads.13.key.weight\", \"blocks.23.self_attention.heads.13.query.weight\", \"blocks.23.self_attention.heads.13.value.weight\", \"blocks.23.self_attention.heads.14.tril\", \"blocks.23.self_attention.heads.14.key.weight\", \"blocks.23.self_attention.heads.14.query.weight\", \"blocks.23.self_attention.heads.14.value.weight\", \"blocks.23.self_attention.heads.15.tril\", \"blocks.23.self_attention.heads.15.key.weight\", \"blocks.23.self_attention.heads.15.query.weight\", \"blocks.23.self_attention.heads.15.value.weight\", \"blocks.23.self_attention.projection.weight\", \"blocks.23.self_attention.projection.bias\", \"blocks.23.feed_forward.net.0.weight\", \"blocks.23.feed_forward.net.0.bias\", \"blocks.23.feed_forward.net.2.weight\", \"blocks.23.feed_forward.net.2.bias\", \"blocks.23.layer_norm_1.weight\", \"blocks.23.layer_norm_1.bias\", \"blocks.23.layer_norm_2.weight\", \"blocks.23.layer_norm_2.bias\". \n\tsize mismatch for token_embedding_table.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024]).\n\tsize mismatch for position_embedding_table.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_linear_layer.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m checkpoint = torch.load(checkpoint_path, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m model_state_dict = checkpoint[\u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2576\u001b[39m         error_msgs.insert(\n\u001b[32m   2577\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2578\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2579\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2580\u001b[39m             ),\n\u001b[32m   2581\u001b[39m         )\n\u001b[32m   2583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2586\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2587\u001b[39m         )\n\u001b[32m   2588\u001b[39m     )\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for GPTLanguageModel:\n\tMissing key(s) in state_dict: \"blocks.6.self_attention.heads.0.tril\", \"blocks.6.self_attention.heads.0.key.weight\", \"blocks.6.self_attention.heads.0.query.weight\", \"blocks.6.self_attention.heads.0.value.weight\", \"blocks.6.self_attention.heads.1.tril\", \"blocks.6.self_attention.heads.1.key.weight\", \"blocks.6.self_attention.heads.1.query.weight\", \"blocks.6.self_attention.heads.1.value.weight\", \"blocks.6.self_attention.heads.2.tril\", \"blocks.6.self_attention.heads.2.key.weight\", \"blocks.6.self_attention.heads.2.query.weight\", \"blocks.6.self_attention.heads.2.value.weight\", \"blocks.6.self_attention.heads.3.tril\", \"blocks.6.self_attention.heads.3.key.weight\", \"blocks.6.self_attention.heads.3.query.weight\", \"blocks.6.self_attention.heads.3.value.weight\", \"blocks.6.self_attention.heads.4.tril\", \"blocks.6.self_attention.heads.4.key.weight\", \"blocks.6.self_attention.heads.4.query.weight\", \"blocks.6.self_attention.heads.4.value.weight\", \"blocks.6.self_attention.heads.5.tril\", \"blocks.6.self_attention.heads.5.key.weight\", \"blocks.6.self_attention.heads.5.query.weight\", \"blocks.6.self_attention.heads.5.value.weight\", \"blocks.6.self_attention.heads.6.tril\", \"blocks.6.self_attention.heads.6.key.weight\", \"blocks.6.self_attention.heads.6.query.weight\", \"blocks.6.self_attention.heads.6.value.weight\", \"blocks.6.self_attention.heads.7.tril\", \"blocks.6.self_attention.heads.7.key.weight\", \"blocks.6.self_attention.heads.7.query.weight\", \"blocks.6.self_attention.heads.7.value.weight\", \"blocks.6.self_attention.heads.8.tril\", \"blocks.6.self_attention.heads.8.key.weight\", \"blocks.6.self_attention.heads.8.query.weight\", \"blocks.6.self_attention.heads.8.value.weight\", \"blocks.6.self_attention.heads.9.tril\", \"blocks.6.self_attention.heads.9.key.weight\", \"blocks.6.self_attention.heads.9.query.weight\", \"blocks.6.self_attention.heads.9.value.weight\", \"blocks.6.self_attention.heads.10.tril\", \"blocks.6.self_attention.heads.10.key.weight\", \"blocks.6.self_attention.heads.10.query.weight\", \"blocks.6.self_attention.heads.10.value.weight\", \"blocks.6.self_attention.heads.11.tril\", \"blocks.6.self_attention.heads.11.key.weight\", \"blocks.6.self_attention.heads.11.query.weight\", \"blocks.6.self_attention.heads.11.value.weight\", \"blocks.6.self_attention.heads.12.tril\", \"blocks.6.self_attention.heads.12.key.weight\", \"blocks.6.self_attention.heads.12.query.weight\", \"blocks.6.self_attention.heads.12.value.weight\", \"blocks.6.self_attention.heads.13.tril\", \"blocks.6.self_attention.heads.13.key.weight\", \"blocks.6.self_attention.heads.13.query.weight\", \"blocks.6.self_attention.heads.13.value.weight\", \"blocks.6.self_attention.heads.14.tril\", \"blocks.6.self_attention.heads.14.key.weight\", \"blocks.6.self_attention.heads.14.query.weight\", \"blocks.6.self_attention.heads.14.value.weight\", \"blocks.6.self_attention.heads.15.tril\", \"blocks.6.self_attention.heads.15.key.weight\", \"blocks.6.self_attention.heads.15.query.weight\", \"blocks.6.self_attention.heads.15.value.weight\", \"blocks.6.self_attention.projection.weight\", \"blocks.6.self_attention.projection.bias\", \"blocks.6.feed_forward.net.0.weight\", \"blocks.6.feed_forward.net.0.bias\", \"blocks.6.feed_forward.net.2.weight\", \"blocks.6.feed_forward.net.2.bias\", \"blocks.6.layer_norm_1.weight\", \"blocks.6.layer_norm_1.bias\", \"blocks.6.layer_norm_2.weight\", \"blocks.6.layer_norm_2.bias\", \"blocks.7.self_attention.heads.0.tril\", \"blocks.7.self_attention.heads.0.key.weight\", \"blocks.7.self_attention.heads.0.query.weight\", \"blocks.7.self_attention.heads.0.value.weight\", \"blocks.7.self_attention.heads.1.tril\", \"blocks.7.self_attention.heads.1.key.weight\", \"blocks.7.self_attention.heads.1.query.weight\", \"blocks.7.self_attention.heads.1.value.weight\", \"blocks.7.self_attention.heads.2.tril\", \"blocks.7.self_attention.heads.2.key.weight\", \"blocks.7.self_attention.heads.2.query.weight\", \"blocks.7.self_attention.heads.2.value.weight\", \"blocks.7.self_attention.heads.3.tril\", \"blocks.7.self_attention.heads.3.key.weight\", \"blocks.7.self_attention.heads.3.query.weight\", \"blocks.7.self_attention.heads.3.value.weight\", \"blocks.7.self_attention.heads.4.tril\", \"blocks.7.self_attention.heads.4.key.weight\", \"blocks.7.self_attention.heads.4.query.weight\", \"blocks.7.self_attention.heads.4.value.weight\", \"blocks.7.self_attention.heads.5.tril\", \"blocks.7.self_attention.heads.5.key.weight\", \"blocks.7.self_attention.heads.5.query.weight\", \"blocks.7.self_attention.heads.5.value.weight\", \"blocks.7.self_attention.heads.6.tril\", \"blocks.7.self_attention.heads.6.key.weight\", \"blocks.7.self_attention.heads.6.query.weight\", \"blocks.7.self_attention.heads.6.value.weight\", \"blocks.7.self_attention.heads.7.tril\", \"blocks.7.self_attention.heads.7.key.weight\", \"blocks.7.self_attention.heads.7.query.weight\", \"blocks.7.self_attention.heads.7.value.weight\", \"blocks.7.self_attention.heads.8.tril\", \"blocks.7.self_attention.heads.8.key.weight\", \"blocks.7.self_attention.heads.8.query.weight\", \"blocks.7.self_attention.heads.8.value.weight\", \"blocks.7.self_attention.heads.9.tril\", \"blocks.7.self_attention.heads.9.key.weight\", \"blocks.7.self_attention.heads.9.query.weight\", \"blocks.7.self_attention.heads.9.value.weight\", \"blocks.7.self_attention.heads.10.tril\", \"blocks.7.self_attention.heads.10.key.weight\", \"blocks.7.self_attention.heads.10.query.weight\", \"blocks.7.self_attention.heads.10.value.weight\", \"blocks.7.self_attention.heads.11.tril\", \"blocks.7.self_attention.heads.11.key.weight\", \"blocks.7.self_attention.heads.11.query.weight\", \"blocks.7.self_attention.heads.11.value.weight\", \"blocks.7.self_attention.heads.12.tril\", \"blocks.7.self_attention.heads.12.key.weight\", \"blocks.7.self_attention.heads.12.query.weight\", \"blocks.7.self_attention.heads.12.value.weight\", \"blocks.7.self_attention.heads.13.tril\", \"blocks.7.self_attention.heads.13.key.weight\", \"blocks.7.self_attention.heads.13.query.weight\", \"blocks.7.self_attention.heads.13.value.weight\", \"blocks.7.self_attention.heads.14.tril\", \"blocks.7.self_attention.heads.14.key.weight\", \"blocks.7.self_attention.heads.14.query.weight\", \"blocks.7.self_attention.heads.14.value.weight\", \"blocks.7.self_attention.heads.15.tril\", \"blocks.7.self_attention.heads.15.key.weight\", \"blocks.7.self_attention.heads.15.query.weight\", \"blocks.7.self_attention.heads.15.value.weight\", \"blocks.7.self_attention.projection.weight\", \"blocks.7.self_attention.projection.bias\", \"blocks.7.feed_forward.net.0.weight\", \"blocks.7.feed_forward.net.0.bias\", \"blocks.7.feed_forward.net.2.weight\", \"blocks.7.feed_forward.net.2.bias\", \"blocks.7.layer_norm_1.weight\", \"blocks.7.layer_norm_1.bias\", \"blocks.7.layer_norm_2.weight\", \"blocks.7.layer_norm_2.bias\", \"blocks.8.self_attention.heads.0.tril\", \"blocks.8.self_attention.heads.0.key.weight\", \"blocks.8.self_attention.heads.0.query.weight\", \"blocks.8.self_attention.heads.0.value.weight\", \"blocks.8.self_attention.heads.1.tril\", \"blocks.8.self_attention.heads.1.key.weight\", \"blocks.8.self_attention.heads.1.query.weight\", \"blocks.8.self_attention.heads.1.value.weight\", \"blocks.8.self_attention.heads.2.tril\", \"blocks.8.self_attention.heads.2.key.weight\", \"blocks.8.self_attention.heads.2.query.weight\", \"blocks.8.self_attention.heads.2.value.weight\", \"blocks.8.self_attention.heads.3.tril\", \"blocks.8.self_attention.heads.3.key.weight\", \"blocks.8.self_attention.heads.3.query.weight\", \"blocks.8.self_attention.heads.3.value.weight\", \"blocks.8.self_attention.heads.4.tril\", \"blocks.8.self_attention.heads.4.key.weight\", \"blocks.8.self_attention.heads.4.query.weight\", \"blocks.8.self_attention.heads.4.value.weight\", \"blocks.8.self_attention.heads.5.tril\", \"blocks.8.self_attention.heads.5.key.weight\", \"blocks.8.self_attention.heads.5.query.weight\", \"blocks.8.self_attention.heads.5.value.weight\", \"blocks.8.self_attention.heads.6.tril\", \"blocks.8.self_attention.heads.6.key.weight\", \"blocks.8.self_attention.heads.6.query.weight\", \"blocks.8.self_attention.heads.6.value.weight\", \"blocks.8.self_attention.heads.7.tril\", \"blocks.8.self_attention.heads.7.key.weight\", \"blocks.8.self_attention.heads.7.query.weight\", \"blocks.8.self_attention.heads.7.value.weight\", \"blocks.8.self_attention.heads.8.tril\", \"blocks.8.self_attention.heads.8.key.weight\", \"blocks.8.self_attention.heads.8.query.weight\", \"blocks.8.self_attention.heads.8.value.weight\", \"blocks.8.self_attention.heads.9.tril\", \"blocks.8.self_attention.heads.9.key.weight\", \"blocks.8.self_attention.heads.9.query.weight\", \"blocks.8.self_attention.heads.9.value.weight\", \"blocks.8.self_attention.heads.10.tril\", \"blocks.8.self_attention.heads.10.key.weight\", \"blocks.8.self_attention.heads.10.query.weight\", \"blocks.8.self_attention.heads.10.value.weight\", \"blocks.8.self_attention.heads.11.tril\", \"blocks.8.self_attention.heads.11.key.weight\", \"blocks.8.self_attention.heads.11.query.weight\", \"blocks.8.self_attention.heads.11.value.weight\", \"blocks.8.self_attention.heads.12.tril\", \"blocks.8.self_attention.heads.12.key.weight\", \"blocks.8.self_attention.heads.12.query.weight\", \"blocks.8.self_attention.heads.12.value.weight\", \"blocks.8.self_attention.heads.13.tril\", \"blocks.8.self_attention.heads.13.key.weight\", \"blocks.8.self_attention.heads.13.query.weight\", \"blocks.8.self_attention.heads.13.value.weight\", \"blocks.8.self_attention.heads.14.tril\", \"blocks.8.self_attention.heads.14.key.weight\", \"blocks.8.self_attention.heads.14.query.weight\", \"blocks.8.self_attention.heads.14.value.weight\", \"blocks.8.self_attention.heads.15.tril\", \"blocks.8.self_attention.heads.15.key.weight\", \"blocks.8.self_attention.heads.15.query.weight\", \"blocks.8.self_attention.heads.15.value.weight\", \"blocks.8.self_attention.projection.weight\", \"blocks.8.self_attention.projection.bias\", \"blocks.8.feed_forward.net.0.weight\", \"blocks.8.feed_forward.net.0.bias\", \"blocks.8.feed_forward.net.2.weight\", \"blocks.8.feed_forward.net.2.bias\", \"blocks.8.layer_norm_1.weight\", \"blocks.8.layer_norm_1.bias\", \"blocks.8.layer_norm_2.weight\", \"blocks.8.layer_norm_2.bias\", \"blocks.9.self_attention.heads.0.tril\", \"blocks.9.self_attention.heads.0.key.weight\", \"blocks.9.self_attention.heads.0.query.weight\", \"blocks.9.self_attention.heads.0.value.weight\", \"blocks.9.self_attention.heads.1.tril\", \"blocks.9.self_attention.heads.1.key.weight\", \"blocks.9.self_attention.heads.1.query.weight\", \"blocks.9.self_attention.heads.1.value.weight\", \"blocks.9.self_attention.heads.2.tril\", \"blocks.9.self_attention.heads.2.key.weight\", \"blocks.9.self_attention.heads.2.query.weight\", \"blocks.9.self_attention.heads.2.value.weight\", \"blocks.9.self_attention.heads.3.tril\", \"blocks.9.self_attention.heads.3.key.weight\", \"blocks.9.self_attention.heads.3.query.weight\", \"blocks.9.self_attention.heads.3.value.weight\", \"blocks.9.self_attention.heads.4.tril\", \"blocks.9.self_attention.heads.4.key.weight\", \"blocks.9.self_attention.heads.4.query.weight\", \"blocks.9.self_attention.heads.4.value.weight\", \"blocks.9.self_attention.heads.5.tril\", \"blocks.9.self_attention.heads.5.key.weight\", \"blocks.9.self_attention.heads.5.query.weight\", \"blocks.9.self_attention.heads.5.value.weight\", \"blocks.9.self_attention.heads.6.tril\", \"blocks.9.self_attention.heads.6.key.weight\", \"blocks.9.self_attention.heads.6.query.weight\", \"blocks.9.self_attention.heads.6.value.weight\", \"blocks.9.self_attention.heads.7.tril\", \"blocks.9.self_attention.heads.7.key.weight\", \"blocks.9.self_attention.heads.7.query.weight\", \"blocks.9.self_attention.heads.7.value.weight\", \"blocks.9.self_attention.heads.8.tril\", \"blocks.9.self_attention.heads.8.key.weight\", \"blocks.9.self_attention.heads.8.query.weight\", \"blocks.9.self_attention.heads.8.value.weight\", \"blocks.9.self_attention.heads.9.tril\", \"blocks.9.self_attention.heads.9.key.weight\", \"blocks.9.self_attention.heads.9.query.weight\", \"blocks.9.self_attention.heads.9.value.weight\", \"blocks.9.self_attention.heads.10.tril\", \"blocks.9.self_attention.heads.10.key.weight\", \"blocks.9.self_attention.heads.10.query.weight\", \"blocks.9.self_attention.heads.10.value.weight\", \"blocks.9.self_attention.heads.11.tril\", \"blocks.9.self_attention.heads.11.key.weight\", \"blocks.9.self_attention.heads.11.query.weight\", \"blocks.9.self_attention.heads.11.value.weight\", \"blocks.9.self_attention.heads.12.tril\", \"blocks.9.self_attention.heads.12.key.weight\", \"blocks.9.self_attention.heads.12.query.weight\", \"blocks.9.self_attention.heads.12.value.weight\", \"blocks.9.self_attention.heads.13.tril\", \"blocks.9.self_attention.heads.13.key.weight\", \"blocks.9.self_attention.heads.13.query.weight\", \"blocks.9.self_attention.heads.13.value.weight\", \"blocks.9.self_attention.heads.14.tril\", \"blocks.9.self_attention.heads.14.key.weight\", \"blocks.9.self_attention.heads.14.query.weight\", \"blocks.9.self_attention.heads.14.value.weight\", \"blocks.9.self_attention.heads.15.tril\", \"blocks.9.self_attention.heads.15.key.weight\", \"blocks.9.self_attention.heads.15.query.weight\", \"blocks.9.self_attention.heads.15.value.weight\", \"blocks.9.self_attention.projection.weight\", \"blocks.9.self_attention.projection.bias\", \"blocks.9.feed_forward.net.0.weight\", \"blocks.9.feed_forward.net.0.bias\", \"blocks.9.feed_forward.net.2.weight\", \"blocks.9.feed_forward.net.2.bias\", \"blocks.9.layer_norm_1.weight\", \"blocks.9.layer_norm_1.bias\", \"blocks.9.layer_norm_2.weight\", \"blocks.9.layer_norm_2.bias\", \"blocks.10.self_attention.heads.0.tril\", \"blocks.10.self_attention.heads.0.key.weight\", \"blocks.10.self_attention.heads.0.query.weight\", \"blocks.10.self_attention.heads.0.value.weight\", \"blocks.10.self_attention.heads.1.tril\", \"blocks.10.self_attention.heads.1.key.weight\", \"blocks.10.self_attention.heads.1.query.weight\", \"blocks.10.self_attention.heads.1.value.weight\", \"blocks.10.self_attention.heads.2.tril\", \"blocks.10.self_attention.heads.2.key.weight\", \"blocks.10.self_attention.heads.2.query.weight\", \"blocks.10.self_attention.heads.2.value.weight\", \"blocks.10.self_attention.heads.3.tril\", \"blocks.10.self_attention.heads.3.key.weight\", \"blocks.10.self_attention.heads.3.query.weight\", \"blocks.10.self_attention.heads.3.value.weight\", \"blocks.10.self_attention.heads.4.tril\", \"blocks.10.self_attention.heads.4.key.weight\", \"blocks.10.self_attention.heads.4.query.weight\", \"blocks.10.self_attention.heads.4.value.weight\", \"blocks.10.self_attention.heads.5.tril\", \"blocks.10.self_attention.heads.5.key.weight\", \"blocks.10.self_attention.heads.5.query.weight\", \"blocks.10.self_attention.heads.5.value.weight\", \"blocks.10.self_attention.heads.6.tril\", \"blocks.10.self_attention.heads.6.key.weight\", \"blocks.10.self_attention.heads.6.query.weight\", \"blocks.10.self_attention.heads.6.value.weight\", \"blocks.10.self_attention.heads.7.tril\", \"blocks.10.self_attention.heads.7.key.weight\", \"blocks.10.self_attention.heads.7.query.weight\", \"blocks.10.self_attention.heads.7.value.weight\", \"blocks.10.self_attention.heads.8.tril\", \"blocks.10.self_attention.heads.8.key.weight\", \"blocks.10.self_attention.heads.8.query.weight\", \"blocks.10.self_attention.heads.8.value.weight\", \"blocks.10.self_attention.heads.9.tril\", \"blocks.10.self_attention.heads.9.key.weight\", \"blocks.10.self_attention.heads.9.query.weight\", \"blocks.10.self_attention.heads.9.value.weight\", \"blocks.10.self_attention.heads.10.tril\", \"blocks.10.self_attention.heads.10.key.weight\", \"blocks.10.self_attention.heads.10.query.weight\", \"blocks.10.self_attention.heads.10.value.weight\", \"blocks.10.self_attention.heads.11.tril\", \"blocks.10.self_attention.heads.11.key.weight\", \"blocks.10.self_attention.heads.11.query.weight\", \"blocks.10.self_attention.heads.11.value.weight\", \"blocks.10.self_attention.heads.12.tril\", \"blocks.10.self_attention.heads.12.key.weight\", \"blocks.10.self_attention.heads.12.query.weight\", \"blocks.10.self_attention.heads.12.value.weight\", \"blocks.10.self_attention.heads.13.tril\", \"blocks.10.self_attention.heads.13.key.weight\", \"blocks.10.self_attention.heads.13.query.weight\", \"blocks.10.self_attention.heads.13.value.weight\", \"blocks.10.self_attention.heads.14.tril\", \"blocks.10.self_attention.heads.14.key.weight\", \"blocks.10.self_attention.heads.14.query.weight\", \"blocks.10.self_attention.heads.14.value.weight\", \"blocks.10.self_attention.heads.15.tril\", \"blocks.10.self_attention.heads.15.key.weight\", \"blocks.10.self_attention.heads.15.query.weight\", \"blocks.10.self_attention.heads.15.value.weight\", \"blocks.10.self_attention.projection.weight\", \"blocks.10.self_attention.projection.bias\", \"blocks.10.feed_forward.net.0.weight\", \"blocks.10.feed_forward.net.0.bias\", \"blocks.10.feed_forward.net.2.weight\", \"blocks.10.feed_forward.net.2.bias\", \"blocks.10.layer_norm_1.weight\", \"blocks.10.layer_norm_1.bias\", \"blocks.10.layer_norm_2.weight\", \"blocks.10.layer_norm_2.bias\", \"blocks.11.self_attention.heads.0.tril\", \"blocks.11.self_attention.heads.0.key.weight\", \"blocks.11.self_attention.heads.0.query.weight\", \"blocks.11.self_attention.heads.0.value.weight\", \"blocks.11.self_attention.heads.1.tril\", \"blocks.11.self_attention.heads.1.key.weight\", \"blocks.11.self_attention.heads.1.query.weight\", \"blocks.11.self_attention.heads.1.value.weight\", \"blocks.11.self_attention.heads.2.tril\", \"blocks.11.self_attention.heads.2.key.weight\", \"blocks.11.self_attention.heads.2.query.weight\", \"blocks.11.self_attention.heads.2.value.weight\", \"blocks.11.self_attention.heads.3.tril\", \"blocks.11.self_attention.heads.3.key.weight\", \"blocks.11.self_attention.heads.3.query.weight\", \"blocks.11.self_attention.heads.3.value.weight\", \"blocks.11.self_attention.heads.4.tril\", \"blocks.11.self_attention.heads.4.key.weight\", \"blocks.11.self_attention.heads.4.query.weight\", \"blocks.11.self_attention.heads.4.value.weight\", \"blocks.11.self_attention.heads.5.tril\", \"blocks.11.self_attention.heads.5.key.weight\", \"blocks.11.self_attention.heads.5.query.weight\", \"blocks.11.self_attention.heads.5.value.weight\", \"blocks.11.self_attention.heads.6.tril\", \"blocks.11.self_attention.heads.6.key.weight\", \"blocks.11.self_attention.heads.6.query.weight\", \"blocks.11.self_attention.heads.6.value.weight\", \"blocks.11.self_attention.heads.7.tril\", \"blocks.11.self_attention.heads.7.key.weight\", \"blocks.11.self_attention.heads.7.query.weight\", \"blocks.11.self_attention.heads.7.value.weight\", \"blocks.11.self_attention.heads.8.tril\", \"blocks.11.self_attention.heads.8.key.weight\", \"blocks.11.self_attention.heads.8.query.weight\", \"blocks.11.self_attention.heads.8.value.weight\", \"blocks.11.self_attention.heads.9.tril\", \"blocks.11.self_attention.heads.9.key.weight\", \"blocks.11.self_attention.heads.9.query.weight\", \"blocks.11.self_attention.heads.9.value.weight\", \"blocks.11.self_attention.heads.10.tril\", \"blocks.11.self_attention.heads.10.key.weight\", \"blocks.11.self_attention.heads.10.query.weight\", \"blocks.11.self_attention.heads.10.value.weight\", \"blocks.11.self_attention.heads.11.tril\", \"blocks.11.self_attention.heads.11.key.weight\", \"blocks.11.self_attention.heads.11.query.weight\", \"blocks.11.self_attention.heads.11.value.weight\", \"blocks.11.self_attention.heads.12.tril\", \"blocks.11.self_attention.heads.12.key.weight\", \"blocks.11.self_attention.heads.12.query.weight\", \"blocks.11.self_attention.heads.12.value.weight\", \"blocks.11.self_attention.heads.13.tril\", \"blocks.11.self_attention.heads.13.key.weight\", \"blocks.11.self_attention.heads.13.query.weight\", \"blocks.11.self_attention.heads.13.value.weight\", \"blocks.11.self_attention.heads.14.tril\", \"blocks.11.self_attention.heads.14.key.weight\", \"blocks.11.self_attention.heads.14.query.weight\", \"blocks.11.self_attention.heads.14.value.weight\", \"blocks.11.self_attention.heads.15.tril\", \"blocks.11.self_attention.heads.15.key.weight\", \"blocks.11.self_attention.heads.15.query.weight\", \"blocks.11.self_attention.heads.15.value.weight\", \"blocks.11.self_attention.projection.weight\", \"blocks.11.self_attention.projection.bias\", \"blocks.11.feed_forward.net.0.weight\", \"blocks.11.feed_forward.net.0.bias\", \"blocks.11.feed_forward.net.2.weight\", \"blocks.11.feed_forward.net.2.bias\", \"blocks.11.layer_norm_1.weight\", \"blocks.11.layer_norm_1.bias\", \"blocks.11.layer_norm_2.weight\", \"blocks.11.layer_norm_2.bias\", \"blocks.12.self_attention.heads.0.tril\", \"blocks.12.self_attention.heads.0.key.weight\", \"blocks.12.self_attention.heads.0.query.weight\", \"blocks.12.self_attention.heads.0.value.weight\", \"blocks.12.self_attention.heads.1.tril\", \"blocks.12.self_attention.heads.1.key.weight\", \"blocks.12.self_attention.heads.1.query.weight\", \"blocks.12.self_attention.heads.1.value.weight\", \"blocks.12.self_attention.heads.2.tril\", \"blocks.12.self_attention.heads.2.key.weight\", \"blocks.12.self_attention.heads.2.query.weight\", \"blocks.12.self_attention.heads.2.value.weight\", \"blocks.12.self_attention.heads.3.tril\", \"blocks.12.self_attention.heads.3.key.weight\", \"blocks.12.self_attention.heads.3.query.weight\", \"blocks.12.self_attention.heads.3.value.weight\", \"blocks.12.self_attention.heads.4.tril\", \"blocks.12.self_attention.heads.4.key.weight\", \"blocks.12.self_attention.heads.4.query.weight\", \"blocks.12.self_attention.heads.4.value.weight\", \"blocks.12.self_attention.heads.5.tril\", \"blocks.12.self_attention.heads.5.key.weight\", \"blocks.12.self_attention.heads.5.query.weight\", \"blocks.12.self_attention.heads.5.value.weight\", \"blocks.12.self_attention.heads.6.tril\", \"blocks.12.self_attention.heads.6.key.weight\", \"blocks.12.self_attention.heads.6.query.weight\", \"blocks.12.self_attention.heads.6.value.weight\", \"blocks.12.self_attention.heads.7.tril\", \"blocks.12.self_attention.heads.7.key.weight\", \"blocks.12.self_attention.heads.7.query.weight\", \"blocks.12.self_attention.heads.7.value.weight\", \"blocks.12.self_attention.heads.8.tril\", \"blocks.12.self_attention.heads.8.key.weight\", \"blocks.12.self_attention.heads.8.query.weight\", \"blocks.12.self_attention.heads.8.value.weight\", \"blocks.12.self_attention.heads.9.tril\", \"blocks.12.self_attention.heads.9.key.weight\", \"blocks.12.self_attention.heads.9.query.weight\", \"blocks.12.self_attention.heads.9.value.weight\", \"blocks.12.self_attention.heads.10.tril\", \"blocks.12.self_attention.heads.10.key.weight\", \"blocks.12.self_attention.heads.10.query.weight\", \"blocks.12.self_attention.heads.10.value.weight\", \"blocks.12.self_attention.heads.11.tril\", \"blocks.12.self_attention.heads.11.key.weight\", \"blocks.12.self_attention.heads.11.query.weight\", \"blocks.12.self_attention.heads.11.value.weight\", \"blocks.12.self_attention.heads.12.tril\", \"blocks.12.self_attention.heads.12.key.weight\", \"blocks.12.self_attention.heads.12.query.weight\", \"blocks.12.self_attention.heads.12.value.weight\", \"blocks.12.self_attention.heads.13.tril\", \"blocks.12.self_attention.heads.13.key.weight\", \"blocks.12.self_attention.heads.13.query.weight\", \"blocks.12.self_attention.heads.13.value.weight\", \"blocks.12.self_attention.heads.14.tril\", \"blocks.12.self_attention.heads.14.key.weight\", \"blocks.12.self_attention.heads.14.query.weight\", \"blocks.12.self_attention.heads.14.value.weight\", \"blocks.12.self_attention.heads.15.tril\", \"blocks.12.self_attention.heads.15.key.weight\", \"blocks.12.self_attention.heads.15.query.weight\", \"blocks.12.self_attention.heads.15.value.weight\", \"blocks.12.self_attention.projection.weight\", \"blocks.12.self_attention.projection.bias\", \"blocks.12.feed_forward.net.0.weight\", \"blocks.12.feed_forward.net.0.bias\", \"blocks.12.feed_forward.net.2.weight\", \"blocks.12.feed_forward.net.2.bias\", \"blocks.12.layer_norm_1.weight\", \"blocks.12.layer_norm_1.bias\", \"blocks.12.layer_norm_2.weight\", \"blocks.12.layer_norm_2.bias\", \"blocks.13.self_attention.heads.0.tril\", \"blocks.13.self_attention.heads.0.key.weight\", \"blocks.13.self_attention.heads.0.query.weight\", \"blocks.13.self_attention.heads.0.value.weight\", \"blocks.13.self_attention.heads.1.tril\", \"blocks.13.self_attention.heads.1.key.weight\", \"blocks.13.self_attention.heads.1.query.weight\", \"blocks.13.self_attention.heads.1.value.weight\", \"blocks.13.self_attention.heads.2.tril\", \"blocks.13.self_attention.heads.2.key.weight\", \"blocks.13.self_attention.heads.2.query.weight\", \"blocks.13.self_attention.heads.2.value.weight\", \"blocks.13.self_attention.heads.3.tril\", \"blocks.13.self_attention.heads.3.key.weight\", \"blocks.13.self_attention.heads.3.query.weight\", \"blocks.13.self_attention.heads.3.value.weight\", \"blocks.13.self_attention.heads.4.tril\", \"blocks.13.self_attention.heads.4.key.weight\", \"blocks.13.self_attention.heads.4.query.weight\", \"blocks.13.self_attention.heads.4.value.weight\", \"blocks.13.self_attention.heads.5.tril\", \"blocks.13.self_attention.heads.5.key.weight\", \"blocks.13.self_attention.heads.5.query.weight\", \"blocks.13.self_attention.heads.5.value.weight\", \"blocks.13.self_attention.heads.6.tril\", \"blocks.13.self_attention.heads.6.key.weight\", \"blocks.13.self_attention.heads.6.query.weight\", \"blocks.13.self_attention.heads.6.value.weight\", \"blocks.13.self_attention.heads.7.tril\", \"blocks.13.self_attention.heads.7.key.weight\", \"blocks.13.self_attention.heads.7.query.weight\", \"blocks.13.self_attention.heads.7.value.weight\", \"blocks.13.self_attention.heads.8.tril\", \"blocks.13.self_attention.heads.8.key.weight\", \"blocks.13.self_attention.heads.8.query.weight\", \"blocks.13.self_attention.heads.8.value.weight\", \"blocks.13.self_attention.heads.9.tril\", \"blocks.13.self_attention.heads.9.key.weight\", \"blocks.13.self_attention.heads.9.query.weight\", \"blocks.13.self_attention.heads.9.value.weight\", \"blocks.13.self_attention.heads.10.tril\", \"blocks.13.self_attention.heads.10.key.weight\", \"blocks.13.self_attention.heads.10.query.weight\", \"blocks.13.self_attention.heads.10.value.weight\", \"blocks.13.self_attention.heads.11.tril\", \"blocks.13.self_attention.heads.11.key.weight\", \"blocks.13.self_attention.heads.11.query.weight\", \"blocks.13.self_attention.heads.11.value.weight\", \"blocks.13.self_attention.heads.12.tril\", \"blocks.13.self_attention.heads.12.key.weight\", \"blocks.13.self_attention.heads.12.query.weight\", \"blocks.13.self_attention.heads.12.value.weight\", \"blocks.13.self_attention.heads.13.tril\", \"blocks.13.self_attention.heads.13.key.weight\", \"blocks.13.self_attention.heads.13.query.weight\", \"blocks.13.self_attention.heads.13.value.weight\", \"blocks.13.self_attention.heads.14.tril\", \"blocks.13.self_attention.heads.14.key.weight\", \"blocks.13.self_attention.heads.14.query.weight\", \"blocks.13.self_attention.heads.14.value.weight\", \"blocks.13.self_attention.heads.15.tril\", \"blocks.13.self_attention.heads.15.key.weight\", \"blocks.13.self_attention.heads.15.query.weight\", \"blocks.13.self_attention.heads.15.value.weight\", \"blocks.13.self_attention.projection.weight\", \"blocks.13.self_attention.projection.bias\", \"blocks.13.feed_forward.net.0.weight\", \"blocks.13.feed_forward.net.0.bias\", \"blocks.13.feed_forward.net.2.weight\", \"blocks.13.feed_forward.net.2.bias\", \"blocks.13.layer_norm_1.weight\", \"blocks.13.layer_norm_1.bias\", \"blocks.13.layer_norm_2.weight\", \"blocks.13.layer_norm_2.bias\", \"blocks.14.self_attention.heads.0.tril\", \"blocks.14.self_attention.heads.0.key.weight\", \"blocks.14.self_attention.heads.0.query.weight\", \"blocks.14.self_attention.heads.0.value.weight\", \"blocks.14.self_attention.heads.1.tril\", \"blocks.14.self_attention.heads.1.key.weight\", \"blocks.14.self_attention.heads.1.query.weight\", \"blocks.14.self_attention.heads.1.value.weight\", \"blocks.14.self_attention.heads.2.tril\", \"blocks.14.self_attention.heads.2.key.weight\", \"blocks.14.self_attention.heads.2.query.weight\", \"blocks.14.self_attention.heads.2.value.weight\", \"blocks.14.self_attention.heads.3.tril\", \"blocks.14.self_attention.heads.3.key.weight\", \"blocks.14.self_attention.heads.3.query.weight\", \"blocks.14.self_attention.heads.3.value.weight\", \"blocks.14.self_attention.heads.4.tril\", \"blocks.14.self_attention.heads.4.key.weight\", \"blocks.14.self_attention.heads.4.query.weight\", \"blocks.14.self_attention.heads.4.value.weight\", \"blocks.14.self_attention.heads.5.tril\", \"blocks.14.self_attention.heads.5.key.weight\", \"blocks.14.self_attention.heads.5.query.weight\", \"blocks.14.self_attention.heads.5.value.weight\", \"blocks.14.self_attention.heads.6.tril\", \"blocks.14.self_attention.heads.6.key.weight\", \"blocks.14.self_attention.heads.6.query.weight\", \"blocks.14.self_attention.heads.6.value.weight\", \"blocks.14.self_attention.heads.7.tril\", \"blocks.14.self_attention.heads.7.key.weight\", \"blocks.14.self_attention.heads.7.query.weight\", \"blocks.14.self_attention.heads.7.value.weight\", \"blocks.14.self_attention.heads.8.tril\", \"blocks.14.self_attention.heads.8.key.weight\", \"blocks.14.self_attention.heads.8.query.weight\", \"blocks.14.self_attention.heads.8.value.weight\", \"blocks.14.self_attention.heads.9.tril\", \"blocks.14.self_attention.heads.9.key.weight\", \"blocks.14.self_attention.heads.9.query.weight\", \"blocks.14.self_attention.heads.9.value.weight\", \"blocks.14.self_attention.heads.10.tril\", \"blocks.14.self_attention.heads.10.key.weight\", \"blocks.14.self_attention.heads.10.query.weight\", \"blocks.14.self_attention.heads.10.value.weight\", \"blocks.14.self_attention.heads.11.tril\", \"blocks.14.self_attention.heads.11.key.weight\", \"blocks.14.self_attention.heads.11.query.weight\", \"blocks.14.self_attention.heads.11.value.weight\", \"blocks.14.self_attention.heads.12.tril\", \"blocks.14.self_attention.heads.12.key.weight\", \"blocks.14.self_attention.heads.12.query.weight\", \"blocks.14.self_attention.heads.12.value.weight\", \"blocks.14.self_attention.heads.13.tril\", \"blocks.14.self_attention.heads.13.key.weight\", \"blocks.14.self_attention.heads.13.query.weight\", \"blocks.14.self_attention.heads.13.value.weight\", \"blocks.14.self_attention.heads.14.tril\", \"blocks.14.self_attention.heads.14.key.weight\", \"blocks.14.self_attention.heads.14.query.weight\", \"blocks.14.self_attention.heads.14.value.weight\", \"blocks.14.self_attention.heads.15.tril\", \"blocks.14.self_attention.heads.15.key.weight\", \"blocks.14.self_attention.heads.15.query.weight\", \"blocks.14.self_attention.heads.15.value.weight\", \"blocks.14.self_attention.projection.weight\", \"blocks.14.self_attention.projection.bias\", \"blocks.14.feed_forward.net.0.weight\", \"blocks.14.feed_forward.net.0.bias\", \"blocks.14.feed_forward.net.2.weight\", \"blocks.14.feed_forward.net.2.bias\", \"blocks.14.layer_norm_1.weight\", \"blocks.14.layer_norm_1.bias\", \"blocks.14.layer_norm_2.weight\", \"blocks.14.layer_norm_2.bias\", \"blocks.15.self_attention.heads.0.tril\", \"blocks.15.self_attention.heads.0.key.weight\", \"blocks.15.self_attention.heads.0.query.weight\", \"blocks.15.self_attention.heads.0.value.weight\", \"blocks.15.self_attention.heads.1.tril\", \"blocks.15.self_attention.heads.1.key.weight\", \"blocks.15.self_attention.heads.1.query.weight\", \"blocks.15.self_attention.heads.1.value.weight\", \"blocks.15.self_attention.heads.2.tril\", \"blocks.15.self_attention.heads.2.key.weight\", \"blocks.15.self_attention.heads.2.query.weight\", \"blocks.15.self_attention.heads.2.value.weight\", \"blocks.15.self_attention.heads.3.tril\", \"blocks.15.self_attention.heads.3.key.weight\", \"blocks.15.self_attention.heads.3.query.weight\", \"blocks.15.self_attention.heads.3.value.weight\", \"blocks.15.self_attention.heads.4.tril\", \"blocks.15.self_attention.heads.4.key.weight\", \"blocks.15.self_attention.heads.4.query.weight\", \"blocks.15.self_attention.heads.4.value.weight\", \"blocks.15.self_attention.heads.5.tril\", \"blocks.15.self_attention.heads.5.key.weight\", \"blocks.15.self_attention.heads.5.query.weight\", \"blocks.15.self_attention.heads.5.value.weight\", \"blocks.15.self_attention.heads.6.tril\", \"blocks.15.self_attention.heads.6.key.weight\", \"blocks.15.self_attention.heads.6.query.weight\", \"blocks.15.self_attention.heads.6.value.weight\", \"blocks.15.self_attention.heads.7.tril\", \"blocks.15.self_attention.heads.7.key.weight\", \"blocks.15.self_attention.heads.7.query.weight\", \"blocks.15.self_attention.heads.7.value.weight\", \"blocks.15.self_attention.heads.8.tril\", \"blocks.15.self_attention.heads.8.key.weight\", \"blocks.15.self_attention.heads.8.query.weight\", \"blocks.15.self_attention.heads.8.value.weight\", \"blocks.15.self_attention.heads.9.tril\", \"blocks.15.self_attention.heads.9.key.weight\", \"blocks.15.self_attention.heads.9.query.weight\", \"blocks.15.self_attention.heads.9.value.weight\", \"blocks.15.self_attention.heads.10.tril\", \"blocks.15.self_attention.heads.10.key.weight\", \"blocks.15.self_attention.heads.10.query.weight\", \"blocks.15.self_attention.heads.10.value.weight\", \"blocks.15.self_attention.heads.11.tril\", \"blocks.15.self_attention.heads.11.key.weight\", \"blocks.15.self_attention.heads.11.query.weight\", \"blocks.15.self_attention.heads.11.value.weight\", \"blocks.15.self_attention.heads.12.tril\", \"blocks.15.self_attention.heads.12.key.weight\", \"blocks.15.self_attention.heads.12.query.weight\", \"blocks.15.self_attention.heads.12.value.weight\", \"blocks.15.self_attention.heads.13.tril\", \"blocks.15.self_attention.heads.13.key.weight\", \"blocks.15.self_attention.heads.13.query.weight\", \"blocks.15.self_attention.heads.13.value.weight\", \"blocks.15.self_attention.heads.14.tril\", \"blocks.15.self_attention.heads.14.key.weight\", \"blocks.15.self_attention.heads.14.query.weight\", \"blocks.15.self_attention.heads.14.value.weight\", \"blocks.15.self_attention.heads.15.tril\", \"blocks.15.self_attention.heads.15.key.weight\", \"blocks.15.self_attention.heads.15.query.weight\", \"blocks.15.self_attention.heads.15.value.weight\", \"blocks.15.self_attention.projection.weight\", \"blocks.15.self_attention.projection.bias\", \"blocks.15.feed_forward.net.0.weight\", \"blocks.15.feed_forward.net.0.bias\", \"blocks.15.feed_forward.net.2.weight\", \"blocks.15.feed_forward.net.2.bias\", \"blocks.15.layer_norm_1.weight\", \"blocks.15.layer_norm_1.bias\", \"blocks.15.layer_norm_2.weight\", \"blocks.15.layer_norm_2.bias\", \"blocks.16.self_attention.heads.0.tril\", \"blocks.16.self_attention.heads.0.key.weight\", \"blocks.16.self_attention.heads.0.query.weight\", \"blocks.16.self_attention.heads.0.value.weight\", \"blocks.16.self_attention.heads.1.tril\", \"blocks.16.self_attention.heads.1.key.weight\", \"blocks.16.self_attention.heads.1.query.weight\", \"blocks.16.self_attention.heads.1.value.weight\", \"blocks.16.self_attention.heads.2.tril\", \"blocks.16.self_attention.heads.2.key.weight\", \"blocks.16.self_attention.heads.2.query.weight\", \"blocks.16.self_attention.heads.2.value.weight\", \"blocks.16.self_attention.heads.3.tril\", \"blocks.16.self_attention.heads.3.key.weight\", \"blocks.16.self_attention.heads.3.query.weight\", \"blocks.16.self_attention.heads.3.value.weight\", \"blocks.16.self_attention.heads.4.tril\", \"blocks.16.self_attention.heads.4.key.weight\", \"blocks.16.self_attention.heads.4.query.weight\", \"blocks.16.self_attention.heads.4.value.weight\", \"blocks.16.self_attention.heads.5.tril\", \"blocks.16.self_attention.heads.5.key.weight\", \"blocks.16.self_attention.heads.5.query.weight\", \"blocks.16.self_attention.heads.5.value.weight\", \"blocks.16.self_attention.heads.6.tril\", \"blocks.16.self_attention.heads.6.key.weight\", \"blocks.16.self_attention.heads.6.query.weight\", \"blocks.16.self_attention.heads.6.value.weight\", \"blocks.16.self_attention.heads.7.tril\", \"blocks.16.self_attention.heads.7.key.weight\", \"blocks.16.self_attention.heads.7.query.weight\", \"blocks.16.self_attention.heads.7.value.weight\", \"blocks.16.self_attention.heads.8.tril\", \"blocks.16.self_attention.heads.8.key.weight\", \"blocks.16.self_attention.heads.8.query.weight\", \"blocks.16.self_attention.heads.8.value.weight\", \"blocks.16.self_attention.heads.9.tril\", \"blocks.16.self_attention.heads.9.key.weight\", \"blocks.16.self_attention.heads.9.query.weight\", \"blocks.16.self_attention.heads.9.value.weight\", \"blocks.16.self_attention.heads.10.tril\", \"blocks.16.self_attention.heads.10.key.weight\", \"blocks.16.self_attention.heads.10.query.weight\", \"blocks.16.self_attention.heads.10.value.weight\", \"blocks.16.self_attention.heads.11.tril\", \"blocks.16.self_attention.heads.11.key.weight\", \"blocks.16.self_attention.heads.11.query.weight\", \"blocks.16.self_attention.heads.11.value.weight\", \"blocks.16.self_attention.heads.12.tril\", \"blocks.16.self_attention.heads.12.key.weight\", \"blocks.16.self_attention.heads.12.query.weight\", \"blocks.16.self_attention.heads.12.value.weight\", \"blocks.16.self_attention.heads.13.tril\", \"blocks.16.self_attention.heads.13.key.weight\", \"blocks.16.self_attention.heads.13.query.weight\", \"blocks.16.self_attention.heads.13.value.weight\", \"blocks.16.self_attention.heads.14.tril\", \"blocks.16.self_attention.heads.14.key.weight\", \"blocks.16.self_attention.heads.14.query.weight\", \"blocks.16.self_attention.heads.14.value.weight\", \"blocks.16.self_attention.heads.15.tril\", \"blocks.16.self_attention.heads.15.key.weight\", \"blocks.16.self_attention.heads.15.query.weight\", \"blocks.16.self_attention.heads.15.value.weight\", \"blocks.16.self_attention.projection.weight\", \"blocks.16.self_attention.projection.bias\", \"blocks.16.feed_forward.net.0.weight\", \"blocks.16.feed_forward.net.0.bias\", \"blocks.16.feed_forward.net.2.weight\", \"blocks.16.feed_forward.net.2.bias\", \"blocks.16.layer_norm_1.weight\", \"blocks.16.layer_norm_1.bias\", \"blocks.16.layer_norm_2.weight\", \"blocks.16.layer_norm_2.bias\", \"blocks.17.self_attention.heads.0.tril\", \"blocks.17.self_attention.heads.0.key.weight\", \"blocks.17.self_attention.heads.0.query.weight\", \"blocks.17.self_attention.heads.0.value.weight\", \"blocks.17.self_attention.heads.1.tril\", \"blocks.17.self_attention.heads.1.key.weight\", \"blocks.17.self_attention.heads.1.query.weight\", \"blocks.17.self_attention.heads.1.value.weight\", \"blocks.17.self_attention.heads.2.tril\", \"blocks.17.self_attention.heads.2.key.weight\", \"blocks.17.self_attention.heads.2.query.weight\", \"blocks.17.self_attention.heads.2.value.weight\", \"blocks.17.self_attention.heads.3.tril\", \"blocks.17.self_attention.heads.3.key.weight\", \"blocks.17.self_attention.heads.3.query.weight\", \"blocks.17.self_attention.heads.3.value.weight\", \"blocks.17.self_attention.heads.4.tril\", \"blocks.17.self_attention.heads.4.key.weight\", \"blocks.17.self_attention.heads.4.query.weight\", \"blocks.17.self_attention.heads.4.value.weight\", \"blocks.17.self_attention.heads.5.tril\", \"blocks.17.self_attention.heads.5.key.weight\", \"blocks.17.self_attention.heads.5.query.weight\", \"blocks.17.self_attention.heads.5.value.weight\", \"blocks.17.self_attention.heads.6.tril\", \"blocks.17.self_attention.heads.6.key.weight\", \"blocks.17.self_attention.heads.6.query.weight\", \"blocks.17.self_attention.heads.6.value.weight\", \"blocks.17.self_attention.heads.7.tril\", \"blocks.17.self_attention.heads.7.key.weight\", \"blocks.17.self_attention.heads.7.query.weight\", \"blocks.17.self_attention.heads.7.value.weight\", \"blocks.17.self_attention.heads.8.tril\", \"blocks.17.self_attention.heads.8.key.weight\", \"blocks.17.self_attention.heads.8.query.weight\", \"blocks.17.self_attention.heads.8.value.weight\", \"blocks.17.self_attention.heads.9.tril\", \"blocks.17.self_attention.heads.9.key.weight\", \"blocks.17.self_attention.heads.9.query.weight\", \"blocks.17.self_attention.heads.9.value.weight\", \"blocks.17.self_attention.heads.10.tril\", \"blocks.17.self_attention.heads.10.key.weight\", \"blocks.17.self_attention.heads.10.query.weight\", \"blocks.17.self_attention.heads.10.value.weight\", \"blocks.17.self_attention.heads.11.tril\", \"blocks.17.self_attention.heads.11.key.weight\", \"blocks.17.self_attention.heads.11.query.weight\", \"blocks.17.self_attention.heads.11.value.weight\", \"blocks.17.self_attention.heads.12.tril\", \"blocks.17.self_attention.heads.12.key.weight\", \"blocks.17.self_attention.heads.12.query.weight\", \"blocks.17.self_attention.heads.12.value.weight\", \"blocks.17.self_attention.heads.13.tril\", \"blocks.17.self_attention.heads.13.key.weight\", \"blocks.17.self_attention.heads.13.query.weight\", \"blocks.17.self_attention.heads.13.value.weight\", \"blocks.17.self_attention.heads.14.tril\", \"blocks.17.self_attention.heads.14.key.weight\", \"blocks.17.self_attention.heads.14.query.weight\", \"blocks.17.self_attention.heads.14.value.weight\", \"blocks.17.self_attention.heads.15.tril\", \"blocks.17.self_attention.heads.15.key.weight\", \"blocks.17.self_attention.heads.15.query.weight\", \"blocks.17.self_attention.heads.15.value.weight\", \"blocks.17.self_attention.projection.weight\", \"blocks.17.self_attention.projection.bias\", \"blocks.17.feed_forward.net.0.weight\", \"blocks.17.feed_forward.net.0.bias\", \"blocks.17.feed_forward.net.2.weight\", \"blocks.17.feed_forward.net.2.bias\", \"blocks.17.layer_norm_1.weight\", \"blocks.17.layer_norm_1.bias\", \"blocks.17.layer_norm_2.weight\", \"blocks.17.layer_norm_2.bias\", \"blocks.18.self_attention.heads.0.tril\", \"blocks.18.self_attention.heads.0.key.weight\", \"blocks.18.self_attention.heads.0.query.weight\", \"blocks.18.self_attention.heads.0.value.weight\", \"blocks.18.self_attention.heads.1.tril\", \"blocks.18.self_attention.heads.1.key.weight\", \"blocks.18.self_attention.heads.1.query.weight\", \"blocks.18.self_attention.heads.1.value.weight\", \"blocks.18.self_attention.heads.2.tril\", \"blocks.18.self_attention.heads.2.key.weight\", \"blocks.18.self_attention.heads.2.query.weight\", \"blocks.18.self_attention.heads.2.value.weight\", \"blocks.18.self_attention.heads.3.tril\", \"blocks.18.self_attention.heads.3.key.weight\", \"blocks.18.self_attention.heads.3.query.weight\", \"blocks.18.self_attention.heads.3.value.weight\", \"blocks.18.self_attention.heads.4.tril\", \"blocks.18.self_attention.heads.4.key.weight\", \"blocks.18.self_attention.heads.4.query.weight\", \"blocks.18.self_attention.heads.4.value.weight\", \"blocks.18.self_attention.heads.5.tril\", \"blocks.18.self_attention.heads.5.key.weight\", \"blocks.18.self_attention.heads.5.query.weight\", \"blocks.18.self_attention.heads.5.value.weight\", \"blocks.18.self_attention.heads.6.tril\", \"blocks.18.self_attention.heads.6.key.weight\", \"blocks.18.self_attention.heads.6.query.weight\", \"blocks.18.self_attention.heads.6.value.weight\", \"blocks.18.self_attention.heads.7.tril\", \"blocks.18.self_attention.heads.7.key.weight\", \"blocks.18.self_attention.heads.7.query.weight\", \"blocks.18.self_attention.heads.7.value.weight\", \"blocks.18.self_attention.heads.8.tril\", \"blocks.18.self_attention.heads.8.key.weight\", \"blocks.18.self_attention.heads.8.query.weight\", \"blocks.18.self_attention.heads.8.value.weight\", \"blocks.18.self_attention.heads.9.tril\", \"blocks.18.self_attention.heads.9.key.weight\", \"blocks.18.self_attention.heads.9.query.weight\", \"blocks.18.self_attention.heads.9.value.weight\", \"blocks.18.self_attention.heads.10.tril\", \"blocks.18.self_attention.heads.10.key.weight\", \"blocks.18.self_attention.heads.10.query.weight\", \"blocks.18.self_attention.heads.10.value.weight\", \"blocks.18.self_attention.heads.11.tril\", \"blocks.18.self_attention.heads.11.key.weight\", \"blocks.18.self_attention.heads.11.query.weight\", \"blocks.18.self_attention.heads.11.value.weight\", \"blocks.18.self_attention.heads.12.tril\", \"blocks.18.self_attention.heads.12.key.weight\", \"blocks.18.self_attention.heads.12.query.weight\", \"blocks.18.self_attention.heads.12.value.weight\", \"blocks.18.self_attention.heads.13.tril\", \"blocks.18.self_attention.heads.13.key.weight\", \"blocks.18.self_attention.heads.13.query.weight\", \"blocks.18.self_attention.heads.13.value.weight\", \"blocks.18.self_attention.heads.14.tril\", \"blocks.18.self_attention.heads.14.key.weight\", \"blocks.18.self_attention.heads.14.query.weight\", \"blocks.18.self_attention.heads.14.value.weight\", \"blocks.18.self_attention.heads.15.tril\", \"blocks.18.self_attention.heads.15.key.weight\", \"blocks.18.self_attention.heads.15.query.weight\", \"blocks.18.self_attention.heads.15.value.weight\", \"blocks.18.self_attention.projection.weight\", \"blocks.18.self_attention.projection.bias\", \"blocks.18.feed_forward.net.0.weight\", \"blocks.18.feed_forward.net.0.bias\", \"blocks.18.feed_forward.net.2.weight\", \"blocks.18.feed_forward.net.2.bias\", \"blocks.18.layer_norm_1.weight\", \"blocks.18.layer_norm_1.bias\", \"blocks.18.layer_norm_2.weight\", \"blocks.18.layer_norm_2.bias\", \"blocks.19.self_attention.heads.0.tril\", \"blocks.19.self_attention.heads.0.key.weight\", \"blocks.19.self_attention.heads.0.query.weight\", \"blocks.19.self_attention.heads.0.value.weight\", \"blocks.19.self_attention.heads.1.tril\", \"blocks.19.self_attention.heads.1.key.weight\", \"blocks.19.self_attention.heads.1.query.weight\", \"blocks.19.self_attention.heads.1.value.weight\", \"blocks.19.self_attention.heads.2.tril\", \"blocks.19.self_attention.heads.2.key.weight\", \"blocks.19.self_attention.heads.2.query.weight\", \"blocks.19.self_attention.heads.2.value.weight\", \"blocks.19.self_attention.heads.3.tril\", \"blocks.19.self_attention.heads.3.key.weight\", \"blocks.19.self_attention.heads.3.query.weight\", \"blocks.19.self_attention.heads.3.value.weight\", \"blocks.19.self_attention.heads.4.tril\", \"blocks.19.self_attention.heads.4.key.weight\", \"blocks.19.self_attention.heads.4.query.weight\", \"blocks.19.self_attention.heads.4.value.weight\", \"blocks.19.self_attention.heads.5.tril\", \"blocks.19.self_attention.heads.5.key.weight\", \"blocks.19.self_attention.heads.5.query.weight\", \"blocks.19.self_attention.heads.5.value.weight\", \"blocks.19.self_attention.heads.6.tril\", \"blocks.19.self_attention.heads.6.key.weight\", \"blocks.19.self_attention.heads.6.query.weight\", \"blocks.19.self_attention.heads.6.value.weight\", \"blocks.19.self_attention.heads.7.tril\", \"blocks.19.self_attention.heads.7.key.weight\", \"blocks.19.self_attention.heads.7.query.weight\", \"blocks.19.self_attention.heads.7.value.weight\", \"blocks.19.self_attention.heads.8.tril\", \"blocks.19.self_attention.heads.8.key.weight\", \"blocks.19.self_attention.heads.8.query.weight\", \"blocks.19.self_attention.heads.8.value.weight\", \"blocks.19.self_attention.heads.9.tril\", \"blocks.19.self_attention.heads.9.key.weight\", \"blocks.19.self_attention.heads.9.query.weight\", \"blocks.19.self_attention.heads.9.value.weight\", \"blocks.19.self_attention.heads.10.tril\", \"blocks.19.self_attention.heads.10.key.weight\", \"blocks.19.self_attention.heads.10.query.weight\", \"blocks.19.self_attention.heads.10.value.weight\", \"blocks.19.self_attention.heads.11.tril\", \"blocks.19.self_attention.heads.11.key.weight\", \"blocks.19.self_attention.heads.11.query.weight\", \"blocks.19.self_attention.heads.11.value.weight\", \"blocks.19.self_attention.heads.12.tril\", \"blocks.19.self_attention.heads.12.key.weight\", \"blocks.19.self_attention.heads.12.query.weight\", \"blocks.19.self_attention.heads.12.value.weight\", \"blocks.19.self_attention.heads.13.tril\", \"blocks.19.self_attention.heads.13.key.weight\", \"blocks.19.self_attention.heads.13.query.weight\", \"blocks.19.self_attention.heads.13.value.weight\", \"blocks.19.self_attention.heads.14.tril\", \"blocks.19.self_attention.heads.14.key.weight\", \"blocks.19.self_attention.heads.14.query.weight\", \"blocks.19.self_attention.heads.14.value.weight\", \"blocks.19.self_attention.heads.15.tril\", \"blocks.19.self_attention.heads.15.key.weight\", \"blocks.19.self_attention.heads.15.query.weight\", \"blocks.19.self_attention.heads.15.value.weight\", \"blocks.19.self_attention.projection.weight\", \"blocks.19.self_attention.projection.bias\", \"blocks.19.feed_forward.net.0.weight\", \"blocks.19.feed_forward.net.0.bias\", \"blocks.19.feed_forward.net.2.weight\", \"blocks.19.feed_forward.net.2.bias\", \"blocks.19.layer_norm_1.weight\", \"blocks.19.layer_norm_1.bias\", \"blocks.19.layer_norm_2.weight\", \"blocks.19.layer_norm_2.bias\", \"blocks.20.self_attention.heads.0.tril\", \"blocks.20.self_attention.heads.0.key.weight\", \"blocks.20.self_attention.heads.0.query.weight\", \"blocks.20.self_attention.heads.0.value.weight\", \"blocks.20.self_attention.heads.1.tril\", \"blocks.20.self_attention.heads.1.key.weight\", \"blocks.20.self_attention.heads.1.query.weight\", \"blocks.20.self_attention.heads.1.value.weight\", \"blocks.20.self_attention.heads.2.tril\", \"blocks.20.self_attention.heads.2.key.weight\", \"blocks.20.self_attention.heads.2.query.weight\", \"blocks.20.self_attention.heads.2.value.weight\", \"blocks.20.self_attention.heads.3.tril\", \"blocks.20.self_attention.heads.3.key.weight\", \"blocks.20.self_attention.heads.3.query.weight\", \"blocks.20.self_attention.heads.3.value.weight\", \"blocks.20.self_attention.heads.4.tril\", \"blocks.20.self_attention.heads.4.key.weight\", \"blocks.20.self_attention.heads.4.query.weight\", \"blocks.20.self_attention.heads.4.value.weight\", \"blocks.20.self_attention.heads.5.tril\", \"blocks.20.self_attention.heads.5.key.weight\", \"blocks.20.self_attention.heads.5.query.weight\", \"blocks.20.self_attention.heads.5.value.weight\", \"blocks.20.self_attention.heads.6.tril\", \"blocks.20.self_attention.heads.6.key.weight\", \"blocks.20.self_attention.heads.6.query.weight\", \"blocks.20.self_attention.heads.6.value.weight\", \"blocks.20.self_attention.heads.7.tril\", \"blocks.20.self_attention.heads.7.key.weight\", \"blocks.20.self_attention.heads.7.query.weight\", \"blocks.20.self_attention.heads.7.value.weight\", \"blocks.20.self_attention.heads.8.tril\", \"blocks.20.self_attention.heads.8.key.weight\", \"blocks.20.self_attention.heads.8.query.weight\", \"blocks.20.self_attention.heads.8.value.weight\", \"blocks.20.self_attention.heads.9.tril\", \"blocks.20.self_attention.heads.9.key.weight\", \"blocks.20.self_attention.heads.9.query.weight\", \"blocks.20.self_attention.heads.9.value.weight\", \"blocks.20.self_attention.heads.10.tril\", \"blocks.20.self_attention.heads.10.key.weight\", \"blocks.20.self_attention.heads.10.query.weight\", \"blocks.20.self_attention.heads.10.value.weight\", \"blocks.20.self_attention.heads.11.tril\", \"blocks.20.self_attention.heads.11.key.weight\", \"blocks.20.self_attention.heads.11.query.weight\", \"blocks.20.self_attention.heads.11.value.weight\", \"blocks.20.self_attention.heads.12.tril\", \"blocks.20.self_attention.heads.12.key.weight\", \"blocks.20.self_attention.heads.12.query.weight\", \"blocks.20.self_attention.heads.12.value.weight\", \"blocks.20.self_attention.heads.13.tril\", \"blocks.20.self_attention.heads.13.key.weight\", \"blocks.20.self_attention.heads.13.query.weight\", \"blocks.20.self_attention.heads.13.value.weight\", \"blocks.20.self_attention.heads.14.tril\", \"blocks.20.self_attention.heads.14.key.weight\", \"blocks.20.self_attention.heads.14.query.weight\", \"blocks.20.self_attention.heads.14.value.weight\", \"blocks.20.self_attention.heads.15.tril\", \"blocks.20.self_attention.heads.15.key.weight\", \"blocks.20.self_attention.heads.15.query.weight\", \"blocks.20.self_attention.heads.15.value.weight\", \"blocks.20.self_attention.projection.weight\", \"blocks.20.self_attention.projection.bias\", \"blocks.20.feed_forward.net.0.weight\", \"blocks.20.feed_forward.net.0.bias\", \"blocks.20.feed_forward.net.2.weight\", \"blocks.20.feed_forward.net.2.bias\", \"blocks.20.layer_norm_1.weight\", \"blocks.20.layer_norm_1.bias\", \"blocks.20.layer_norm_2.weight\", \"blocks.20.layer_norm_2.bias\", \"blocks.21.self_attention.heads.0.tril\", \"blocks.21.self_attention.heads.0.key.weight\", \"blocks.21.self_attention.heads.0.query.weight\", \"blocks.21.self_attention.heads.0.value.weight\", \"blocks.21.self_attention.heads.1.tril\", \"blocks.21.self_attention.heads.1.key.weight\", \"blocks.21.self_attention.heads.1.query.weight\", \"blocks.21.self_attention.heads.1.value.weight\", \"blocks.21.self_attention.heads.2.tril\", \"blocks.21.self_attention.heads.2.key.weight\", \"blocks.21.self_attention.heads.2.query.weight\", \"blocks.21.self_attention.heads.2.value.weight\", \"blocks.21.self_attention.heads.3.tril\", \"blocks.21.self_attention.heads.3.key.weight\", \"blocks.21.self_attention.heads.3.query.weight\", \"blocks.21.self_attention.heads.3.value.weight\", \"blocks.21.self_attention.heads.4.tril\", \"blocks.21.self_attention.heads.4.key.weight\", \"blocks.21.self_attention.heads.4.query.weight\", \"blocks.21.self_attention.heads.4.value.weight\", \"blocks.21.self_attention.heads.5.tril\", \"blocks.21.self_attention.heads.5.key.weight\", \"blocks.21.self_attention.heads.5.query.weight\", \"blocks.21.self_attention.heads.5.value.weight\", \"blocks.21.self_attention.heads.6.tril\", \"blocks.21.self_attention.heads.6.key.weight\", \"blocks.21.self_attention.heads.6.query.weight\", \"blocks.21.self_attention.heads.6.value.weight\", \"blocks.21.self_attention.heads.7.tril\", \"blocks.21.self_attention.heads.7.key.weight\", \"blocks.21.self_attention.heads.7.query.weight\", \"blocks.21.self_attention.heads.7.value.weight\", \"blocks.21.self_attention.heads.8.tril\", \"blocks.21.self_attention.heads.8.key.weight\", \"blocks.21.self_attention.heads.8.query.weight\", \"blocks.21.self_attention.heads.8.value.weight\", \"blocks.21.self_attention.heads.9.tril\", \"blocks.21.self_attention.heads.9.key.weight\", \"blocks.21.self_attention.heads.9.query.weight\", \"blocks.21.self_attention.heads.9.value.weight\", \"blocks.21.self_attention.heads.10.tril\", \"blocks.21.self_attention.heads.10.key.weight\", \"blocks.21.self_attention.heads.10.query.weight\", \"blocks.21.self_attention.heads.10.value.weight\", \"blocks.21.self_attention.heads.11.tril\", \"blocks.21.self_attention.heads.11.key.weight\", \"blocks.21.self_attention.heads.11.query.weight\", \"blocks.21.self_attention.heads.11.value.weight\", \"blocks.21.self_attention.heads.12.tril\", \"blocks.21.self_attention.heads.12.key.weight\", \"blocks.21.self_attention.heads.12.query.weight\", \"blocks.21.self_attention.heads.12.value.weight\", \"blocks.21.self_attention.heads.13.tril\", \"blocks.21.self_attention.heads.13.key.weight\", \"blocks.21.self_attention.heads.13.query.weight\", \"blocks.21.self_attention.heads.13.value.weight\", \"blocks.21.self_attention.heads.14.tril\", \"blocks.21.self_attention.heads.14.key.weight\", \"blocks.21.self_attention.heads.14.query.weight\", \"blocks.21.self_attention.heads.14.value.weight\", \"blocks.21.self_attention.heads.15.tril\", \"blocks.21.self_attention.heads.15.key.weight\", \"blocks.21.self_attention.heads.15.query.weight\", \"blocks.21.self_attention.heads.15.value.weight\", \"blocks.21.self_attention.projection.weight\", \"blocks.21.self_attention.projection.bias\", \"blocks.21.feed_forward.net.0.weight\", \"blocks.21.feed_forward.net.0.bias\", \"blocks.21.feed_forward.net.2.weight\", \"blocks.21.feed_forward.net.2.bias\", \"blocks.21.layer_norm_1.weight\", \"blocks.21.layer_norm_1.bias\", \"blocks.21.layer_norm_2.weight\", \"blocks.21.layer_norm_2.bias\", \"blocks.22.self_attention.heads.0.tril\", \"blocks.22.self_attention.heads.0.key.weight\", \"blocks.22.self_attention.heads.0.query.weight\", \"blocks.22.self_attention.heads.0.value.weight\", \"blocks.22.self_attention.heads.1.tril\", \"blocks.22.self_attention.heads.1.key.weight\", \"blocks.22.self_attention.heads.1.query.weight\", \"blocks.22.self_attention.heads.1.value.weight\", \"blocks.22.self_attention.heads.2.tril\", \"blocks.22.self_attention.heads.2.key.weight\", \"blocks.22.self_attention.heads.2.query.weight\", \"blocks.22.self_attention.heads.2.value.weight\", \"blocks.22.self_attention.heads.3.tril\", \"blocks.22.self_attention.heads.3.key.weight\", \"blocks.22.self_attention.heads.3.query.weight\", \"blocks.22.self_attention.heads.3.value.weight\", \"blocks.22.self_attention.heads.4.tril\", \"blocks.22.self_attention.heads.4.key.weight\", \"blocks.22.self_attention.heads.4.query.weight\", \"blocks.22.self_attention.heads.4.value.weight\", \"blocks.22.self_attention.heads.5.tril\", \"blocks.22.self_attention.heads.5.key.weight\", \"blocks.22.self_attention.heads.5.query.weight\", \"blocks.22.self_attention.heads.5.value.weight\", \"blocks.22.self_attention.heads.6.tril\", \"blocks.22.self_attention.heads.6.key.weight\", \"blocks.22.self_attention.heads.6.query.weight\", \"blocks.22.self_attention.heads.6.value.weight\", \"blocks.22.self_attention.heads.7.tril\", \"blocks.22.self_attention.heads.7.key.weight\", \"blocks.22.self_attention.heads.7.query.weight\", \"blocks.22.self_attention.heads.7.value.weight\", \"blocks.22.self_attention.heads.8.tril\", \"blocks.22.self_attention.heads.8.key.weight\", \"blocks.22.self_attention.heads.8.query.weight\", \"blocks.22.self_attention.heads.8.value.weight\", \"blocks.22.self_attention.heads.9.tril\", \"blocks.22.self_attention.heads.9.key.weight\", \"blocks.22.self_attention.heads.9.query.weight\", \"blocks.22.self_attention.heads.9.value.weight\", \"blocks.22.self_attention.heads.10.tril\", \"blocks.22.self_attention.heads.10.key.weight\", \"blocks.22.self_attention.heads.10.query.weight\", \"blocks.22.self_attention.heads.10.value.weight\", \"blocks.22.self_attention.heads.11.tril\", \"blocks.22.self_attention.heads.11.key.weight\", \"blocks.22.self_attention.heads.11.query.weight\", \"blocks.22.self_attention.heads.11.value.weight\", \"blocks.22.self_attention.heads.12.tril\", \"blocks.22.self_attention.heads.12.key.weight\", \"blocks.22.self_attention.heads.12.query.weight\", \"blocks.22.self_attention.heads.12.value.weight\", \"blocks.22.self_attention.heads.13.tril\", \"blocks.22.self_attention.heads.13.key.weight\", \"blocks.22.self_attention.heads.13.query.weight\", \"blocks.22.self_attention.heads.13.value.weight\", \"blocks.22.self_attention.heads.14.tril\", \"blocks.22.self_attention.heads.14.key.weight\", \"blocks.22.self_attention.heads.14.query.weight\", \"blocks.22.self_attention.heads.14.value.weight\", \"blocks.22.self_attention.heads.15.tril\", \"blocks.22.self_attention.heads.15.key.weight\", \"blocks.22.self_attention.heads.15.query.weight\", \"blocks.22.self_attention.heads.15.value.weight\", \"blocks.22.self_attention.projection.weight\", \"blocks.22.self_attention.projection.bias\", \"blocks.22.feed_forward.net.0.weight\", \"blocks.22.feed_forward.net.0.bias\", \"blocks.22.feed_forward.net.2.weight\", \"blocks.22.feed_forward.net.2.bias\", \"blocks.22.layer_norm_1.weight\", \"blocks.22.layer_norm_1.bias\", \"blocks.22.layer_norm_2.weight\", \"blocks.22.layer_norm_2.bias\", \"blocks.23.self_attention.heads.0.tril\", \"blocks.23.self_attention.heads.0.key.weight\", \"blocks.23.self_attention.heads.0.query.weight\", \"blocks.23.self_attention.heads.0.value.weight\", \"blocks.23.self_attention.heads.1.tril\", \"blocks.23.self_attention.heads.1.key.weight\", \"blocks.23.self_attention.heads.1.query.weight\", \"blocks.23.self_attention.heads.1.value.weight\", \"blocks.23.self_attention.heads.2.tril\", \"blocks.23.self_attention.heads.2.key.weight\", \"blocks.23.self_attention.heads.2.query.weight\", \"blocks.23.self_attention.heads.2.value.weight\", \"blocks.23.self_attention.heads.3.tril\", \"blocks.23.self_attention.heads.3.key.weight\", \"blocks.23.self_attention.heads.3.query.weight\", \"blocks.23.self_attention.heads.3.value.weight\", \"blocks.23.self_attention.heads.4.tril\", \"blocks.23.self_attention.heads.4.key.weight\", \"blocks.23.self_attention.heads.4.query.weight\", \"blocks.23.self_attention.heads.4.value.weight\", \"blocks.23.self_attention.heads.5.tril\", \"blocks.23.self_attention.heads.5.key.weight\", \"blocks.23.self_attention.heads.5.query.weight\", \"blocks.23.self_attention.heads.5.value.weight\", \"blocks.23.self_attention.heads.6.tril\", \"blocks.23.self_attention.heads.6.key.weight\", \"blocks.23.self_attention.heads.6.query.weight\", \"blocks.23.self_attention.heads.6.value.weight\", \"blocks.23.self_attention.heads.7.tril\", \"blocks.23.self_attention.heads.7.key.weight\", \"blocks.23.self_attention.heads.7.query.weight\", \"blocks.23.self_attention.heads.7.value.weight\", \"blocks.23.self_attention.heads.8.tril\", \"blocks.23.self_attention.heads.8.key.weight\", \"blocks.23.self_attention.heads.8.query.weight\", \"blocks.23.self_attention.heads.8.value.weight\", \"blocks.23.self_attention.heads.9.tril\", \"blocks.23.self_attention.heads.9.key.weight\", \"blocks.23.self_attention.heads.9.query.weight\", \"blocks.23.self_attention.heads.9.value.weight\", \"blocks.23.self_attention.heads.10.tril\", \"blocks.23.self_attention.heads.10.key.weight\", \"blocks.23.self_attention.heads.10.query.weight\", \"blocks.23.self_attention.heads.10.value.weight\", \"blocks.23.self_attention.heads.11.tril\", \"blocks.23.self_attention.heads.11.key.weight\", \"blocks.23.self_attention.heads.11.query.weight\", \"blocks.23.self_attention.heads.11.value.weight\", \"blocks.23.self_attention.heads.12.tril\", \"blocks.23.self_attention.heads.12.key.weight\", \"blocks.23.self_attention.heads.12.query.weight\", \"blocks.23.self_attention.heads.12.value.weight\", \"blocks.23.self_attention.heads.13.tril\", \"blocks.23.self_attention.heads.13.key.weight\", \"blocks.23.self_attention.heads.13.query.weight\", \"blocks.23.self_attention.heads.13.value.weight\", \"blocks.23.self_attention.heads.14.tril\", \"blocks.23.self_attention.heads.14.key.weight\", \"blocks.23.self_attention.heads.14.query.weight\", \"blocks.23.self_attention.heads.14.value.weight\", \"blocks.23.self_attention.heads.15.tril\", \"blocks.23.self_attention.heads.15.key.weight\", \"blocks.23.self_attention.heads.15.query.weight\", \"blocks.23.self_attention.heads.15.value.weight\", \"blocks.23.self_attention.projection.weight\", \"blocks.23.self_attention.projection.bias\", \"blocks.23.feed_forward.net.0.weight\", \"blocks.23.feed_forward.net.0.bias\", \"blocks.23.feed_forward.net.2.weight\", \"blocks.23.feed_forward.net.2.bias\", \"blocks.23.layer_norm_1.weight\", \"blocks.23.layer_norm_1.bias\", \"blocks.23.layer_norm_2.weight\", \"blocks.23.layer_norm_2.bias\". \n\tsize mismatch for token_embedding_table.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024]).\n\tsize mismatch for position_embedding_table.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.0.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.0.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.0.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.0.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.1.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.1.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.1.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.1.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.2.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.2.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.2.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.2.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.3.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.3.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.3.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.3.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.4.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.4.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.4.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.4.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.0.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.1.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.2.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.3.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.4.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.5.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.6.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.7.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.8.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.9.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.10.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.11.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.12.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.13.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.14.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.key.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.query.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.heads.15.value.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for blocks.5.self_attention.projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.weight: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for blocks.5.feed_forward.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for blocks.5.feed_forward.net.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for blocks.5.layer_norm_2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_layer_norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for final_linear_layer.weight: copying a param with shape torch.Size([16398, 512]) from checkpoint, the shape in current model is torch.Size([16398, 1024])."
     ]
    }
   ],
   "source": [
    "#checkpoint_path = \"../output/pre_training/base/run_5/checkpoint_60000.pth\"\n",
    "checkpoint_path = \"../output/pre_training/base/epoch_5.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate from the model to make sure that the weights were loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, que tal al mushe xD Chale Para tiene un abuelito :v esque me vio nadie Tocó paja xd esta gratis :v Chale Es mi camara mi camara me acordé :v xD Tafuk Oeoe A tranqui gracias voy a pillar oño muerto mientras me meto PEPE Tas vivo? Volviendo de pillar oñovas No tengo Abrí este Dios calidad 10 A tranqui gracias voy a tu cueva mirate en un rato miro si me meto PEPE Tas vivo? Volviendo de pillar oño\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer.encode(\"Hola, que tal\", allowed_special=\"all\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    ") -> Dict[str, float]:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = torch.zeros(len(loader))\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                _, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        output[split] = losses.mean().item()\n",
    "\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on batches:   0%|          | 0/122 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (1730) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m     12\u001b[39m     iterable=\u001b[38;5;28menumerate\u001b[39m(train_loader),\n\u001b[32m     13\u001b[39m     desc=\u001b[33m\"\u001b[39m\u001b[33mTraining on batches\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     total=\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     15\u001b[39m ):\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx % eval_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m batch_idx == \u001b[38;5;28mlen\u001b[39m(train_loader) - \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m        losses = \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m        \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     24\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m         )\n\u001b[32m     28\u001b[39m        train_losses.append(losses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m(model, train_loader, val_loader)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         _, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     losses[i] = loss.item()\n\u001b[32m     19\u001b[39m output[split] = losses.mean().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:136\u001b[39m, in \u001b[36mGPTLanguageModel.forward\u001b[39m\u001b[34m(self, input_tokens, targets)\u001b[39m\n\u001b[32m    133\u001b[39m positional_embedding = \u001b[38;5;28mself\u001b[39m.position_embedding_table(\n\u001b[32m    134\u001b[39m     torch.arange(T, device=\u001b[38;5;28mself\u001b[39m.device))\n\u001b[32m    135\u001b[39m x = token_embedding + positional_embedding\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_layer_norm(x)\n\u001b[32m    138\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.final_linear_layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:87\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.feed_forward(\u001b[38;5;28mself\u001b[39m.layer_norm_2(x))\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:46\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     47\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.projection(out))\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Proyectos/Train_Your_Language_Model_Course/notebooks/../transformer/model.py:25\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m q = \u001b[38;5;28mself\u001b[39m.query(x)  \u001b[38;5;66;03m# (B,T,hs)\u001b[39;00m\n\u001b[32m     24\u001b[39m weights = q @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m) * k.shape[-\u001b[32m1\u001b[39m]**-\u001b[32m0.5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m weights = \u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-inf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m weights = F.softmax(weights, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m weights = \u001b[38;5;28mself\u001b[39m.dropout(weights)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1024) must match the size of tensor b (1730) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_iters = 10\n",
    "eval_interval = 2\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(\n",
    "        iterable=enumerate(train_loader),\n",
    "        desc=\"Training on batches\",\n",
    "        total=len(train_loader)\n",
    "    ):\n",
    "        # Evaluation\n",
    "        if batch_idx % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "           losses = estimate_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader\n",
    "            )\n",
    "           print(\n",
    "                f\"Epoch {iteration} / step {batch_idx}: \"\n",
    "                f\"train loss {losses['train']:.4f}, \"\n",
    "                f\"val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "           train_losses.append(losses['train'])\n",
    "           val_losses.append(losses['val'])\n",
    "\n",
    "        # Training step\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=iteration,\n",
    "        loss=loss.item(),\n",
    "        file_path=f\"../output/fine_tuning/qa/base/run_2/checkpoint_{iteration}.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: شنو سميتك؟\n",
      "Assistant: انا سميتي بودماغ.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"../output/fine_tuning/qa/base/run_2/checkpoint_9.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "input_message = f\"{tokens['start']}system{tokens['separator']}{system_message}{tokens['end']}\"\n",
    "\n",
    "user_message = \"¿Cómo te llamas?\"\n",
    "input_message += f\"{tokens['start']}user{tokens['separator']}{user_message}{tokens['end']}\"\n",
    "input_message += f\"{tokens['start']}assistant{tokens['separator']}\"\n",
    "\n",
    "input_tokens = tokenizer.encode(input_message, allowed_special=\"all\")\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long)\n",
    "input_tokens = input_tokens.unsqueeze(0).to(device)\n",
    "\n",
    "model_answer = \"\"\n",
    "\n",
    "model.eval()\n",
    "while True:\n",
    "    try:\n",
    "        output_tokens = model.advanced_generation(\n",
    "            input_tokens=input_tokens, max_new_tokens=1, temperature=.9, top_k=50, top_p=None)\n",
    "        last_generated_token = output_tokens[0, -1].item()\n",
    "        if last_generated_token == tokenizer.special_tokens[\"<|endoftext|>\"]:\n",
    "            break\n",
    "\n",
    "        if last_generated_token == tokenizer.special_tokens[\"<|end_turn|>\"]:\n",
    "            break\n",
    "\n",
    "        input_tokens = torch.cat((input_tokens, output_tokens[:, -1:]), dim=1)\n",
    "        model_answer += tokenizer.decode([last_generated_token])\n",
    "\n",
    "        if len(output_tokens[0]) > block_size:\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"You: {user_message}\")\n",
    "print(f\"Assistant: {model_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
